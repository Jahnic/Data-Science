{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping the redflagdeals forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests # Scraping\n",
    "from bs4 import BeautifulSoup # HTML parsing\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data from the \"Hot Deals - All Categories\" sub-forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page format: `url/page#/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL base, current page, and total number of pages. Used to iterate over page URLs.\n",
    "current_page = \"\" # page number used to format base URL\n",
    "total_pages = 0 # total number for endpoint of iteration\n",
    "page_url = \"https://forums.redflagdeals.com/hot-deals-f9/\" # base url for \"Hot Deals\"\n",
    "\n",
    "# URL base to generate links to specific posts\n",
    "base_url = \"https://forums.redflagdeals.com\"\n",
    "\n",
    "# Dataframe to store scraped data\n",
    "table = pd.DataFrame(columns=['title', 'source', 'url', 'votes', 'replies', 'views', 'creation_date', 'last_reply', 'author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(page: str):\n",
    "    \"\"\"Returns list of all HTML post elements found on page\"\"\"\n",
    "    \n",
    "    # Initalize list of posts on page class=\"row topic\"\n",
    "    posts = []\n",
    "    \n",
    "    # Get entire page content\n",
    "    response = requests.get(page)\n",
    "    content = response.content\n",
    "\n",
    "    # URL parser\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Find total number of pages\n",
    "    # Format of text: \" {current page #} of {total page #} \"\n",
    "    # Need to strip white space and extract total page #\n",
    "    pages = parser.select(\".pagination_menu_trigger\")[0].text.strip().split(\"of \")[1]\n",
    "    total_pages = int(pages)\n",
    "    print(\"Total pages: \", total_pages)\n",
    "    \n",
    "    # Find and return list of topics\n",
    "    topics = parser.find_all(\"li\", class_=\"row topic\")\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_info(post: str) -> dict:\n",
    "    \"\"\"Extracts and returns additional information from a specific post:\n",
    "    url-link to the deal, the price and the discount percentage\n",
    "    if available\"\"\"\n",
    "    \n",
    "    # Additional information about deal\n",
    "    add = {}\n",
    "    \n",
    "    # Get content of post\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse URL\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Offer-summary field: may contain deal link, price, saving, and retailer\n",
    "    summary = parser.select(\".post_offer_fields\")[0].text\n",
    "    summary_list = summary.split(\"\\n\")\n",
    "    \n",
    "    # Go through summary elements and save relevant information\n",
    "    for i in range(len(summary_list)):\n",
    "        content = summary_list[i] # content of current list element\n",
    "        if content.startswith(\"Price\") or content.startswith(\"Saving\") or content.startswith(\"Expiry\"):\n",
    "            add[content] = summary_list[i+1] # next elements corrsponds to content\n",
    "    \n",
    "    # If any of the elements is not found add None value to dictionary \n",
    "    if \"Price:\" not in add:\n",
    "        add['Price:'] = None\n",
    "        \n",
    "    if \"Saving:\" not in add:\n",
    "        add['Saving:'] = None\n",
    "        \n",
    "    if \"Expiry:\" not in add:\n",
    "        add['Expiry:'] = None\n",
    "    \n",
    "    return add # Return dictionary containing with information on price, saving and expiry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_table(posts: list) -> None:\n",
    "    '''Fills table with data from elements of the post objects'''\n",
    "    \n",
    "    # For appending data \n",
    "    tmp_table = pd.DataFrame() # temporary DataFrame that holds all column objects. Will be appended to the global `table`. \n",
    "    \n",
    "    # Initializing columns for tmp_table\n",
    "    title_col = pd.Series()\n",
    "    source_col = pd.Series()\n",
    "    url_col = pd.Series()\n",
    "    votes_col = pd.Series()\n",
    "    replies_col = pd.Series()\n",
    "    views_col = pd.Series()\n",
    "    creation_date_col = pd.Series()\n",
    "    last_reply_col = pd.Series()\n",
    "    author_col = pd.Series()\n",
    "    price_col = pd.Series()\n",
    "    saving_col = pd.Series()\n",
    "    expiry_col = pd.Series()\n",
    "    \n",
    "\n",
    "    # Iterate through post elements and extract data for table\n",
    "    for post in posts:\n",
    "        \n",
    "        # Retailer corresponding to deal\n",
    "        try: \n",
    "            source = post.select(\".topictitle_retailer\")[0].text.split(\"\\n\")[0] # split and remove line-break characters\n",
    "            source_series = pd.Series(source) # transform into Series object\n",
    "        except: source_series = pd.Series(None)\n",
    "        source_col = source_col.append(source_series, ignore_index=True) # append to column and ignore index to avoid complications when merging with DataFrame object\n",
    "\n",
    "        # Number of votes\n",
    "        try: \n",
    "            votes = post.select(\".post_voting\")[0].text.split(\"\\n\")[1] # split and remove line-break characters\n",
    "            votes_series = pd.Series(votes) # transform into Series object\n",
    "        except: votes_series = pd.Series(None)\n",
    "        votes_col = votes_col.append(votes_series, ignore_index=True) # append to column\n",
    "            \n",
    "        # Title\n",
    "        try: \n",
    "            topic = post.select(\".topic_title_link\")[0] # contains title and sub-url to post\n",
    "            title = topic.text.split('\\n')[1] # extract text and remove line-break characters\n",
    "            title_series = pd.Series(title)\n",
    "        except: title_series = pd.Series(None)\n",
    "        title_col = title_col.append(title_series, ignore_index=True)\n",
    "\n",
    "        # Date of initial posting\n",
    "        try: \n",
    "            creation = post.select(\".first-post-time\")[0].text.split(\"\\n\")[0] # remove line-breaks\n",
    "            creation_series = pd.Series(creation)\n",
    "        except: creation_series = pd.Series(None)\n",
    "        creation_date_col = creation_date_col.append(creation_series, ignore_index=True) # append to column\n",
    "        \n",
    "        # Date of most recent replie\n",
    "        try: \n",
    "            last_replie = post.select(\".last-post-time\")[0].text.split(\"\\n\")[0] # remove line-breaks\n",
    "            last_replie_series = pd.Series(last_replie)\n",
    "        except: last_replie_series = pd.Series(None)\n",
    "        last_reply_col = last_reply_col.append(last_replie_series, ignore_index=True) # append to column\n",
    "        \n",
    "        # Author user-name\n",
    "        try:\n",
    "            author = post.select(\".thread_meta_author\")[0].text.split(\"\\n\")[0]\n",
    "            author_series = pd.Series(author)\n",
    "        except: author_series = pd.Series(None)\n",
    "        author_col = author_col.append(author_series, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Number of replies\n",
    "        try:\n",
    "            replies = post.select(\".posts\")[0].text.split(\"\\n\")[0]\n",
    "            replies = replies.replace(\",\",\"\") # replace any commas to prepare for data type switch to integer\n",
    "            replies_series = pd.Series(replies)\n",
    "        except: replies_series = pd.Series(None)\n",
    "        replies_col = replies_col.append(replies_series, ignore_index=True)\n",
    "        \n",
    "        # Number of views\n",
    "        try:\n",
    "            views = post.select(\".views\")[0].text.split(\"\\n\")[0]\n",
    "            views = views.replace(\",\",\"\") # replace any commas to prepare for data type switch to integer\n",
    "            views_series = pd.Series(views)\n",
    "        except: replies_series = pd.Series(None)\n",
    "        views_col = views_col.append(views_series, ignore_index=True)\n",
    "        \n",
    "        # Link to current post\n",
    "        try:\n",
    "            link = str(topic).split('href=\"')[1] # split at href to extract link\n",
    "            link_clean = link.split('\">')[0] # remove superfluous characters\n",
    "        except: \n",
    "            link_clean = None\n",
    "        \n",
    "        # Additional information from link to post\n",
    "        if link_clean != None:\n",
    "            url = (base_url + \"{}\").format(link_clean)\n",
    "            add_info = additional_info(url)\n",
    "            \n",
    "            price_col\n",
    "            saving_col\n",
    "            expiry_col\n",
    "        else:\n",
    "            price_col = price_col.append(None)\n",
    "            saving_col = saving_col.append(None)\n",
    "            expiry_col = expiry_col.append(None)\n",
    "        \n",
    "            \n",
    "    # Fill temporary table\n",
    "    tmp_table['title'] = title_col\n",
    "    tmp_table['votes'] = votes_col.astype(int)\n",
    "    tmp_table['source'] = source_col\n",
    "    tmp_table['creation_date'] = creation_date_col\n",
    "    tmp_table['last_reply'] = last_reply_col\n",
    "    tmp_table['author'] = author_col\n",
    "    tmp_table['replies'] = replies_col.astype(int)\n",
    "    tmp_table['views'] = views_col.astype(int)\n",
    "        \n",
    "    # Print result\n",
    "    #print(post_links)\n",
    "    print(tmp_table.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages:  48\n"
     ]
    }
   ],
   "source": [
    "# List of posts\n",
    "posts = get_posts(url)\n",
    "\n",
    "# Test\n",
    "fill_table(posts)\n",
    "\n",
    "# Test\n",
    "# tmp = \"https://forums.redflagdeals.com/bed-bath-and-beyond-zojirushi-ns-tsc10-5-5-cup-micom-rice-cooker-145-59-20-off-coupon-ymmv-no-coupon-181-99-2388699/\"\n",
    "# additional_info(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping the redflagdeals forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests # Scraping\n",
    "from bs4 import BeautifulSoup # HTML parsing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data from the \"Hot Deals - All Categories\" sub-forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page format: `url/page#/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL base, current page, and total number of pages. Used to iterate over page URLs.\n",
    "current_page = \"\" # page number used to format base URL\n",
    "total_pages = 0 # total number for endpoint of iteration\n",
    "page_url = \"https://forums.redflagdeals.com/hot-deals-f9/\" # base url for \"Hot Deals\"\n",
    "\n",
    "# URL base to generate links to specific posts\n",
    "base_url = \"https://forums.redflagdeals.com\"\n",
    "\n",
    "# Dataframe to store scraped data\n",
    "table = pd.DataFrame(columns=['title', 'source', 'url', 'votes', 'replies', 'views', 'creation_date', 'last_reply', 'author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(page: str):\n",
    "    \"\"\"Returns list of all HTML post elements found on page\"\"\"\n",
    "    \n",
    "    # Initalize list of posts on page class=\"row topic\"\n",
    "    posts = []\n",
    "    \n",
    "    # Get entire page content\n",
    "    response = requests.get(page)\n",
    "    content = response.content\n",
    "\n",
    "    # URL parser\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Find total number of pages\n",
    "    # Format of text: \" {current page #} of {total page #} \"\n",
    "    # Need to strip white space and extract total page #\n",
    "    pages = parser.select(\".pagination_menu_trigger\")[0].text.strip().split(\"of \")[1]\n",
    "    total_pages = int(pages)\n",
    "    \n",
    "    # Find and return list of topics\n",
    "    topics = parser.find_all(\"li\", class_=\"row topic\")\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_info(post: str) -> dict:\n",
    "    \"\"\"Extracts and returns additional information from a specific post:\n",
    "    url-link to the deal, the price, the discount percentage, and the expiry date\n",
    "    if available\"\"\"\n",
    "    \n",
    "    # Additional information about deal\n",
    "    add = {}\n",
    "    \n",
    "    # Get content of post\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    \n",
    "    # Parse URL\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Offer-summary field: may contain deal link, price, saving, and retailer\n",
    "    summary = parser.select(\".post_offer_fields\") # format example: \"Price:\\n$200\\nSaving:\\n70%\"\n",
    "    try:\n",
    "        summary_list = summary[0].text.split(\"\\n\") \n",
    "    except: summary_list = []\n",
    "        \n",
    "    # Go through summary elements and save relevant information\n",
    "    for i in range(1, (len(summary_list) -1), 2): # index 0 is empty string\n",
    "        current_element = summary_list[i] # content of current list element\n",
    "        next_element = summary_list[i+1] # next list element\n",
    "        \n",
    "        # Price, saving, and expiry date information contained in the next list element will be saved\n",
    "        if current_element.startswith(\"Price\") or current_element.startswith(\"Saving\") or current_element.startswith(\"Expiry\"):\n",
    "            add[current_element]  = next_element # next elements corrsponds to content\n",
    "            \n",
    "    # URL to link\n",
    "    try: \n",
    "        url = str(summary[0]).split('href=\"')[1].split('\"')[0] # select link between href=\" and \"\n",
    "        add['Link:'] = url\n",
    "    except: add['Link:'] = np.nan\n",
    "        \n",
    "    \n",
    "    # If any of the elements is not found in the field add None value to dictionary \n",
    "    if \"Price:\" not in add:\n",
    "        add['Price:'] = np.nan\n",
    "        \n",
    "    if \"Savings:\" not in add:\n",
    "        add['Savings:'] = np.nan\n",
    "        \n",
    "    if \"Expiry:\" not in add:\n",
    "        add['Expiry:'] = np.nan\n",
    "    \n",
    "    return add # Return dictionary containing with information on price, saving and expiry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_table(posts: list) -> None:\n",
    "    '''Fills table with data from elements of the post objects'''\n",
    "    \n",
    "    # For appending data \n",
    "    tmp_table = pd.DataFrame() # temporary DataFrame that holds all column objects. Will be appended to the global `table`. \n",
    "    \n",
    "    # Initializing columns for tmp_table\n",
    "    title_col = pd.Series()\n",
    "    source_col = pd.Series()\n",
    "    url_col = pd.Series()\n",
    "    votes_col = pd.Series()\n",
    "    replies_col = pd.Series()\n",
    "    views_col = pd.Series()\n",
    "    creation_date_col = pd.Series()\n",
    "    last_reply_col = pd.Series()\n",
    "    author_col = pd.Series()\n",
    "    price_col = pd.Series()\n",
    "    saving_col = pd.Series()\n",
    "    expiry_col = pd.Series()\n",
    "    \n",
    "\n",
    "    # Iterate through post elements and extract data for table\n",
    "    for post in posts:\n",
    "        \n",
    "        # Retailer corresponding to deal\n",
    "        try: \n",
    "            source = post.select(\".topictitle_retailer\")[0].text.split(\"\\n\")[0] # split and remove line-break characters\n",
    "            source_series = pd.Series(source) # transform into Series object\n",
    "        except: source_series = pd.Series(np.nan)\n",
    "        source_col = source_col.append(source_series, ignore_index=True) # append to column and ignore index to avoid complications when merging with DataFrame object\n",
    "\n",
    "        # Number of votes\n",
    "        try: \n",
    "            votes = post.select(\".post_voting\")[0].text.split(\"\\n\")[1] # split and remove line-break characters\n",
    "            votes_series = pd.Series(votes) # transform into Series object\n",
    "        except: votes_series = pd.Series(0)\n",
    "        votes_col = votes_col.append(votes_series, ignore_index=True) # append to column\n",
    "            \n",
    "        # Title \n",
    "        try:\n",
    "            topic = post.select(\".topic_title_link\") # contains title and sub-url to post\n",
    "            title = topic[0].text.split('\\n')[1] # extract text and remove line-break characters\n",
    "            title_series = pd.Series(title)\n",
    "        except: title_series = pd.Series(np.nan)\n",
    "        title_col = title_col.append(title_series, ignore_index=True)\n",
    "\n",
    "        # Date of initial posting\n",
    "        try: \n",
    "            creation = post.select(\".first-post-time\")[0].text.split(\"\\n\")[0] # remove line-breaks\n",
    "            creation_series = pd.Series(creation)\n",
    "        except: creation_series = pd.Series(np.nan)\n",
    "        creation_date_col = creation_date_col.append(creation_series, ignore_index=True) # append to column\n",
    "        \n",
    "        # Date of most recent replie\n",
    "        try: \n",
    "            last_replie = post.select(\".last-post-time\")[0].text.split(\"\\n\")[0] # remove line-breaks\n",
    "            last_replie_series = pd.Series(last_replie)\n",
    "        except: last_replie_series = pd.Series(np.nan)\n",
    "        last_reply_col = last_reply_col.append(last_replie_series, ignore_index=True) # append to column\n",
    "        \n",
    "        # Author user-name\n",
    "        try:\n",
    "            author = post.select(\".thread_meta_author\")[0].text.split(\"\\n\")[0]\n",
    "            author_series = pd.Series(author)\n",
    "        except: author_series = pd.Series(np.nan)\n",
    "        author_col = author_col.append(author_series, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Number of replies\n",
    "        try:\n",
    "            replies = post.select(\".posts\")[0].text.split(\"\\n\")[0]\n",
    "            replies = replies.replace(\",\",\"\") # replace any commas to prepare for data type switch to integer\n",
    "            replies_series = pd.Series(replies)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        replies_col = replies_col.append(replies_series, ignore_index=True)\n",
    "        \n",
    "        # Number of views\n",
    "        try:\n",
    "            views = post.select(\".views\")[0].text.split(\"\\n\")[0]\n",
    "            views = views.replace(\",\",\"\") # replace any commas to prepare for data type switch to integer\n",
    "            views_series = pd.Series(views)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        views_col = views_col.append(views_series, ignore_index=True)\n",
    "        \n",
    "        # Link to current post\n",
    "        try:\n",
    "            link = str(topic).split('href=\"')[1] # split at href to extract link\n",
    "            link_clean = link.split('\">')[0] # remove superfluous characters\n",
    "        except: \n",
    "            link_clean = np.nan\n",
    "        \n",
    "        # Additional information post\n",
    "        if link_clean != None: # retrieve information from post, if it exists\n",
    "            post_url = (base_url + \"{}\").format(link_clean) # merge base-, and sub-url to generate the complete post-link\n",
    "            add_info = additional_info(post_url) # get additonal information on price, saving, and expiry-date\n",
    "            \n",
    "            # Fill columns with additional information from add_info dictionary\n",
    "            price_col = price_col.append(pd.Series(add_info['Price:']), ignore_index=True)\n",
    "            saving_col = saving_col.append(pd.Series(add_info['Savings:']), ignore_index=True)\n",
    "            expiry_col = expiry_col.append(pd.Series(add_info['Expiry:']), ignore_index=True)\n",
    "            url_col = url_col.append(pd.Series(add_info['Link:']), ignore_index=True)\n",
    "        else:\n",
    "            price_col = price_col.append(np.nan)\n",
    "            saving_col = saving_col.append(np.nan)\n",
    "            expiry_col = expiry_col.append(np.nan)\n",
    "            url_col = url_col.append(np.nan)\n",
    "        \n",
    "            \n",
    "    # Fill temporary table\n",
    "    tmp_table['title'] = title_col\n",
    "    tmp_table['votes'] = votes_col.astype(int)\n",
    "    tmp_table['source'] = source_col\n",
    "    tmp_table['creation_date'] = creation_date_col\n",
    "    tmp_table['last_reply'] = last_reply_col\n",
    "    tmp_table['author'] = author_col\n",
    "    tmp_table['replies'] = replies_col.astype(int)\n",
    "    tmp_table['views'] = views_col.astype(int)\n",
    "    tmp_table['price'] = price_col\n",
    "    tmp_table['saving'] = saving_col\n",
    "    tmp_table['expiry'] = expiry_col\n",
    "    tmp_table['url'] = url_col\n",
    "        \n",
    "    # Print result\n",
    "    print(tmp_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  votes  \\\n",
      "0  $499.99 Acer Aspire 3 Ryzen 3 3200U / 8GB RAM ...     20   \n",
      "1                Ryobi Gasoline Backpack Blower YMMV     -2   \n",
      "2                 Quickjack 7000slx $200 off - $1300     25   \n",
      "3  Amex Personal Platinum offer - $250 credits fo...    144   \n",
      "4  Lenovo Q27h QHD 27‚Äù monitor IPS/USB-C/VESA/Spe...     53   \n",
      "\n",
      "               source           creation_date               last_reply  \\\n",
      "0  Shoppers Drug Mart  Jul 10th, 2020 4:40 pm  Jul 11th, 2020 11:17 pm   \n",
      "1          Home Depot  Jul 11th, 2020 3:15 pm  Jul 11th, 2020 11:14 pm   \n",
      "2              Costco   Jul 6th, 2020 9:56 am  Jul 11th, 2020 11:14 pm   \n",
      "3    American Express   Jun 4th, 2020 3:44 am  Jul 11th, 2020 11:14 pm   \n",
      "4       Lenovo Canada  Jun 13th, 2020 9:50 pm  Jul 11th, 2020 11:12 pm   \n",
      "\n",
      "       author  replies  views       price     saving           expiry  \\\n",
      "0    mangoman      157  17998  $499 + 20x        NaN    July 12, 2020   \n",
      "1   HarelD475       15   2415        $125  Reg. $288              NaN   \n",
      "2    hkhorace       60  11719        1300   $200 off    July 12, 2020   \n",
      "3  bigfishman      542  81999         NaN        NaN  August 27, 2020   \n",
      "4  dracolnyte      165  38259         NaN        NaN              NaN   \n",
      "\n",
      "                                                 url  \n",
      "0                                                NaN  \n",
      "1  https://the-home-depot-ca.pxf.io/c/341376/5836...  \n",
      "2  https://www.costco.ca/quickjack-bl-7000slx-318...  \n",
      "3                                                NaN  \n",
      "4  http://lenovo.evyy.net/c/341376/225728/3899?u=...  \n"
     ]
    }
   ],
   "source": [
    "# List of posts\n",
    "posts = get_posts(url)\n",
    "\n",
    "# Test\n",
    "fill_table(posts)\n",
    "\n",
    "# Test\n",
    "# tmp = \"https://forums.redflagdeals.com/bed-bath-and-beyond-zojirushi-ns-tsc10-5-5-cup-micom-rice-cooker-145-59-20-off-coupon-ymmv-no-coupon-181-99-2388699/\"\n",
    "# additional_info(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

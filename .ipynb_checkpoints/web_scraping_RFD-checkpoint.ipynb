{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RedFlagDeals](https://forums.redflagdeals.com/hot-deals-f9/) is a forum where users can post sales or deals that they have come accross. The first part of this project is focused on scraping relevant information from the \"All Hot Deals\" section that includes all product categories. In the second and third part I will clean and visualize the data to extract and summarize useful information.  \n",
    "\n",
    "|Column name|Description|\n",
    "|---|---|\n",
    "|'title'| Title of post|\n",
    "|'votes'| Sum of up-, and down-votes|\n",
    "|'source'| Name of retailer offering the sale|\n",
    "|'creation_date'| Date of initial post|\n",
    "|'last_reply'| Date of most recent reply|\n",
    "|'author'| User name of post author|\n",
    "|'replies'| Number of replies|\n",
    "|'views'| Number of views|\n",
    "|'price'| Price of product on sale|\n",
    "|'saving'| Associated saving|\n",
    "|'expiry'| Expiry date of sale|\n",
    "|'url'| Link to deal|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests # Scraping\n",
    "from bs4 import BeautifulSoup # HTML parsing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Retrieving data from the \"All Hot Deals\" sub-forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL format for different pages: `root-url/page#/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables used to iterate over web-pages.\n",
    "current_page = \"\" # page number; used to format root URL\n",
    "total_pages = 1 # endpoint for iteration; set through get_posts()\n",
    "root_url = \"https://forums.redflagdeals.com/hot-deals-f9/\" # base url for \"All Hot Deals\" sub-forum\n",
    "\n",
    "# URL base to generate links to specific posts\n",
    "base_url = \"https://forums.redflagdeals.com\"\n",
    "\n",
    "# Dataframe to store scraped data\n",
    "table = pd.DataFrame(columns=\n",
    "    ['title',\n",
    "    'votes',\n",
    "    'source',\n",
    "    'creation_date',\n",
    "    'last_reply',\n",
    "    'author',\n",
    "    'replies',\n",
    "    'views',\n",
    "    'price',\n",
    "    'saving',\n",
    "    'expiry',\n",
    "    'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(page: str) -> list:\n",
    "    \"\"\"\n",
    "    Returns list of parsed object containing all post elements from\n",
    "    the current 'page' and sets gloabl variable 'total_pages'\n",
    "    \n",
    "    Args:\n",
    "    page - url string of current page\n",
    "    \n",
    "    Returns:\n",
    "    topics - all parsed elements of class 'row topic'\n",
    "    total_pages - sets global variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initalize list of posts on page class=\"row topic\"\n",
    "    posts = []\n",
    "    \n",
    "    # Get entire page content\n",
    "    response = requests.get(page)\n",
    "    content = response.content\n",
    "    \n",
    "    # Find total number of pages and set global variable accordingly\n",
    "    # Format of text: \" {current page #} of {total page #} \"\n",
    "    # Need to strip white space and extract total page #\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    pages = parser.select(\".pagination_menu_trigger\")[0].text.strip().split(\"of \")[1]\n",
    "    global total_pages\n",
    "    total_pages = int(pages)\n",
    "    \n",
    "    # Find and return topics\n",
    "    topics = parser.find_all(\"li\", class_=\"row topic\")\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_info(post: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts and returns additional information from a RedFlagDeal post:\n",
    "    url-link to the deal, the price of the product, the discount saving, \n",
    "    the expiry date and the parent/thread categories of the product. Returns \n",
    "    NaN values for objects that are not found.\n",
    "    \n",
    "    Args:\n",
    "    post - url string linking to a specific post\n",
    "    \n",
    "    Returns:\n",
    "    additional_info - dictionary containing additional information about the post.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Additional information found in post\n",
    "    additional_info = {}\n",
    "    \n",
    "    # Get content of post\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Thread-header with information on parent and thread category\n",
    "    try: # parent category\n",
    "        parent_category = parser.select(\".thread_parent_category\")[0].text\n",
    "        additional_info['Parent:'] = parent_category\n",
    "    except: additional_info['Parent:'] = np.nan # NaN if category not found\n",
    "    try: # thread category\n",
    "        thread_category = parser.select(\".thread_category\")[0].text\n",
    "        additional_info['Thread:'] = thread_category\n",
    "    except: additional_info['Thread:'] = np.nan # NaN if category not found\n",
    "    \n",
    "    \n",
    "    # Offer-summary field: may contain deal link, price, saving, and retailer\n",
    "    summary = parser.select(\".post_offer_fields\") # format example: \"Price:\\n$200\\nSaving:\\n70%\"\n",
    "    try:\n",
    "        summary_list = summary[0].text.split(\"\\n\") \n",
    "    except: summary_list = []\n",
    "        \n",
    "    # Go through summary elements and save relevant information\n",
    "    for i in range(1, (len(summary_list) -1), 2): # index 0 is empty string\n",
    "        current_element = summary_list[i] # content of current list element\n",
    "        next_element = summary_list[i+1] # next list element\n",
    "        \n",
    "        # Price, saving, and expiry date information contained in the next list element will be saved\n",
    "        if current_element.startswith(\"Price\") or current_element.startswith(\"Saving\") or current_element.startswith(\"Expiry\"):\n",
    "            additional_info[current_element]  = next_element # next elements corrsponds to content\n",
    "            \n",
    "    # URL to link. Full link not available through .text\n",
    "    try: \n",
    "        url = str(summary[0]).split('href=\"')[1].split('\"')[0] # select link between href=\" and \"\n",
    "        additional_info['Link:'] = url\n",
    "    except: additional_info['Link:'] = np.nan\n",
    "        \n",
    "    \n",
    "    # If any of the elements is not found in the summary-field add None value to dictionary \n",
    "    if \"Price:\" not in additional_info:\n",
    "        additional_info['Price:'] = np.nan\n",
    "        \n",
    "    if \"Savings:\" not in additional_info:\n",
    "        additional_info['Savings:'] = np.nan\n",
    "        \n",
    "    if \"Expiry:\" not in additional_info:\n",
    "        additional_info['Expiry:'] = np.nan\n",
    "    \n",
    "    return additional_info # Return dictionary containing with information on price, saving and expiry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_table(posts: list) -> None:\n",
    "    '''\n",
    "    Extracts parsed data from current page and appends to the global table variable.\n",
    "    \n",
    "    Args:\n",
    "    posts - list of parsed post elements obtained through get_posts()\n",
    "    '''\n",
    "    \n",
    "    # Temporary DataFrame object that will be appended to the global 'table' variable\n",
    "    tmp_table = pd.DataFrame() \n",
    "    \n",
    "    # Initializing columns for tmp_table\n",
    "    title_col = pd.Series()\n",
    "    source_col = pd.Series()\n",
    "    url_col = pd.Series()\n",
    "    votes_col = pd.Series()\n",
    "    replies_col = pd.Series()\n",
    "    views_col = pd.Series()\n",
    "    creation_date_col = pd.Series()\n",
    "    last_reply_col = pd.Series()\n",
    "    author_col = pd.Series()\n",
    "    price_col = pd.Series()\n",
    "    saving_col = pd.Series()\n",
    "    expiry_col = pd.Series()\n",
    "    parent_col = pd.Series()\n",
    "    thread_col = pd.Series()\n",
    "    \n",
    "\n",
    "    # Iterate through post elements on current page and extract data for table\n",
    "    for post in posts:\n",
    "        \n",
    "        # Retailer corresponding to deal\n",
    "        try: \n",
    "            source = post.select(\".topictitle_retailer\")[0].text.split(\"\\n\")[0] # split and remove line-break characters\n",
    "            source_series = pd.Series(source) # transforming into Series object allows use of .append method\n",
    "        except: source_series = pd.Series(np.nan)\n",
    "        source_col = source_col.append(source_series, ignore_index=True)\n",
    "\n",
    "        # Number of votes\n",
    "        try: \n",
    "            votes = post.select(\".post_voting\")[0].text.split(\"\\n\")[1] \n",
    "            votes_series = pd.Series(votes) \n",
    "        except: votes_series = pd.Series(0)\n",
    "        votes_col = votes_col.append(votes_series, ignore_index=True)\n",
    "            \n",
    "        # Title \n",
    "        try:\n",
    "            topic = post.select(\".topic_title_link\") \n",
    "            title = topic[0].text.split('\\n')[1] \n",
    "            title_series = pd.Series(title)\n",
    "        except: title_series = pd.Series(np.nan)\n",
    "        title_col = title_col.append(title_series, ignore_index=True)\n",
    "\n",
    "        # Date of initial posting\n",
    "        try: \n",
    "            creation = post.select(\".first-post-time\")[0].text.split(\"\\n\")[0]\n",
    "            creation_series = pd.Series(creation)\n",
    "        except: creation_series = pd.Series(np.nan)\n",
    "        creation_date_col = creation_date_col.append(creation_series, ignore_index=True) \n",
    "        \n",
    "        # Date of most recent replie\n",
    "        try: \n",
    "            last_replie = post.select(\".last-post-time\")[0].text.split(\"\\n\")[0]\n",
    "            last_replie_series = pd.Series(last_replie)\n",
    "        except: last_replie_series = pd.Series(np.nan)\n",
    "        last_reply_col = last_reply_col.append(last_replie_series, ignore_index=True) \n",
    "        \n",
    "        # Author user-name\n",
    "        try:\n",
    "            author = post.select(\".thread_meta_author\")[0].text.split(\"\\n\")[0]\n",
    "            author_series = pd.Series(author)\n",
    "        except: author_series = pd.Series(np.nan)\n",
    "        author_col = author_col.append(author_series, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Number of replies\n",
    "        try:\n",
    "            replies = post.select(\".posts\")[0].text.split(\"\\n\")[0]\n",
    "            replies = replies.replace(\",\",\"\") # remove commas to facilitate data type conversion to integer\n",
    "            replies_series = pd.Series(replies)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        replies_col = replies_col.append(replies_series, ignore_index=True)\n",
    "        \n",
    "        # Number of views\n",
    "        try:\n",
    "            views = post.select(\".views\")[0].text.split(\"\\n\")[0]\n",
    "            views = views.replace(\",\",\"\") # remove commas to facilitate integer conversion\n",
    "            views_series = pd.Series(views)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        views_col = views_col.append(views_series, ignore_index=True)\n",
    "        \n",
    "        # Link to current post\n",
    "        try:\n",
    "            link = str(topic).split('href=\"')[1] # split at href to extract link\n",
    "            link_clean = link.split('\">')[0] # remove superfluous characters\n",
    "        except: \n",
    "            link_clean = np.nan\n",
    "        \n",
    "        # Additional information post\n",
    "        if link_clean != None: # retrieve information from post, if url exists\n",
    "            complete_url = (base_url + \"{}\").format(link_clean) # merge base-, and sub-url to generate the complete post-link\n",
    "            additional_info = get_additional_info(complete_url) # get dictionary of additonal information on price, saving, etc.\n",
    "            \n",
    "            # Fill columns with additional information from additional_info dictionary\n",
    "            price_col = price_col.append(pd.Series(additional_info['Price:']), ignore_index=True)\n",
    "            saving_col = saving_col.append(pd.Series(additional_info['Savings:']), ignore_index=True)\n",
    "            expiry_col = expiry_col.append(pd.Series(additional_info['Expiry:']), ignore_index=True)\n",
    "            url_col = url_col.append(pd.Series(additional_info['Link:']), ignore_index=True)\n",
    "            parent_col = parent_col.append(pd.Series(additional_info['Parent:']), ignore_index=True)\n",
    "            thread_col = thread_col.append(pd.Series(additional_info['Thread:']), ignore_index=True)\n",
    "        else:\n",
    "            price_col = price_col.append(np.nan)\n",
    "            saving_col = saving_col.append(np.nan)\n",
    "            expiry_col = expiry_col.append(np.nan)\n",
    "            url_col = url_col.append(np.nan)\n",
    "        \n",
    "            \n",
    "    # Fill temporary table\n",
    "    tmp_table['title'] = title_col\n",
    "    tmp_table['votes'] = votes_col.astype(int)\n",
    "    tmp_table['source'] = source_col\n",
    "    tmp_table['creation_date'] = creation_date_col\n",
    "    tmp_table['last_reply'] = last_reply_col\n",
    "    tmp_table['author'] = author_col\n",
    "    tmp_table['replies'] = replies_col.astype(int)\n",
    "    tmp_table['views'] = views_col.astype(int)\n",
    "    tmp_table['price'] = price_col\n",
    "    tmp_table['saving'] = saving_col\n",
    "    tmp_table['expiry'] = expiry_col\n",
    "    tmp_table['url'] = url_col\n",
    "    tmp_table['parent_category'] = parent_col\n",
    "    tmp_table['thread_category'] = thread_col\n",
    "        \n",
    "    # Print result\n",
    "    global table # gloabal keyword allows modification inside function\n",
    "    table = table.append(tmp_table)\n",
    "    print(\"Current table length: \", table.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information from page: 1\n",
      "Current table length:  30\n",
      "Extracting information from page:  2  of  51\n",
      "Current table length:  60\n",
      "Extracting information from page:  3  of  51\n",
      "Current table length:  90\n",
      "Extracting information from page:  4  of  51\n",
      "Current table length:  120\n",
      "Extracting information from page:  5  of  51\n",
      "Current table length:  150\n",
      "Extracting information from page:  6  of  51\n",
      "Current table length:  180\n",
      "Extracting information from page:  7  of  51\n",
      "Current table length:  210\n",
      "Extracting information from page:  8  of  51\n",
      "Current table length:  240\n",
      "Extracting information from page:  9  of  51\n",
      "Current table length:  270\n",
      "Extracting information from page:  10  of  51\n",
      "Current table length:  300\n",
      "Extracting information from page:  11  of  51\n",
      "Current table length:  330\n",
      "Extracting information from page:  12  of  51\n",
      "Current table length:  360\n",
      "Extracting information from page:  13  of  51\n",
      "Current table length:  390\n",
      "Extracting information from page:  14  of  51\n",
      "Current table length:  420\n",
      "Extracting information from page:  15  of  51\n",
      "Current table length:  450\n",
      "Extracting information from page:  16  of  51\n",
      "Current table length:  480\n",
      "Extracting information from page:  17  of  51\n",
      "Current table length:  510\n",
      "Extracting information from page:  18  of  51\n",
      "Current table length:  540\n",
      "Extracting information from page:  19  of  51\n",
      "Current table length:  570\n",
      "Extracting information from page:  20  of  51\n",
      "Current table length:  600\n",
      "Extracting information from page:  21  of  51\n",
      "Current table length:  630\n",
      "Extracting information from page:  22  of  51\n",
      "Current table length:  660\n",
      "Extracting information from page:  23  of  51\n",
      "Current table length:  690\n",
      "Extracting information from page:  24  of  51\n",
      "Current table length:  720\n",
      "Extracting information from page:  25  of  51\n",
      "Current table length:  750\n",
      "Extracting information from page:  26  of  51\n",
      "Current table length:  780\n",
      "Extracting information from page:  27  of  51\n",
      "Current table length:  810\n",
      "Extracting information from page:  28  of  51\n",
      "Current table length:  840\n",
      "Extracting information from page:  29  of  51\n",
      "Current table length:  870\n",
      "Extracting information from page:  30  of  51\n",
      "Current table length:  900\n",
      "Extracting information from page:  31  of  51\n",
      "Current table length:  930\n",
      "Extracting information from page:  32  of  51\n",
      "Current table length:  960\n",
      "Extracting information from page:  33  of  51\n",
      "Current table length:  990\n",
      "Extracting information from page:  34  of  51\n",
      "Current table length:  1020\n",
      "Extracting information from page:  35  of  51\n",
      "Current table length:  1050\n",
      "Extracting information from page:  36  of  51\n",
      "Current table length:  1080\n",
      "Extracting information from page:  37  of  51\n",
      "Current table length:  1110\n",
      "Extracting information from page:  38  of  51\n",
      "Current table length:  1140\n",
      "Extracting information from page:  39  of  51\n",
      "Current table length:  1170\n",
      "Extracting information from page:  40  of  51\n",
      "Current table length:  1200\n",
      "Extracting information from page:  41  of  51\n",
      "Current table length:  1230\n",
      "Extracting information from page:  42  of  51\n",
      "Current table length:  1260\n",
      "Extracting information from page:  43  of  51\n",
      "Current table length:  1290\n",
      "Extracting information from page:  44  of  51\n",
      "Current table length:  1320\n",
      "Extracting information from page:  45  of  51\n",
      "Current table length:  1350\n",
      "Extracting information from page:  46  of  51\n",
      "Current table length:  1380\n",
      "Extracting information from page:  47  of  51\n",
      "Current table length:  1410\n",
      "Extracting information from page:  48  of  51\n",
      "Current table length:  1440\n",
      "Extracting information from page:  49  of  51\n",
      "Current table length:  1470\n",
      "Extracting information from page:  50  of  51\n",
      "Current table length:  1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>trystee</td>\n",
       "      <td>Jan 1st, 2020 8:32 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:25 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>[Various Retailers] Gift Card Deals And Discou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365169</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>yellowmp5</td>\n",
       "      <td>Jul 13th, 2020 1:29 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:23 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>RYOBI 20% coupon barcode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2593</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>phoreoneone</td>\n",
       "      <td>Jul 13th, 2020 12:34 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:23 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dell</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...</td>\n",
       "      <td>http://www.jdoqocy.com/click-749547-12105225?u...</td>\n",
       "      <td>2339</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Blackdove77</td>\n",
       "      <td>Jul 6th, 2020 12:49 pm</td>\n",
       "      <td>July 19, 2020</td>\n",
       "      <td>Jul 13th, 2020 3:23 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>413</td>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Watchdogs 2 PC version free for Everyone</td>\n",
       "      <td>https://register.ubisoft.com/ubisoft-forward-r...</td>\n",
       "      <td>57748</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>hkhorace</td>\n",
       "      <td>Jul 6th, 2020 9:56 am</td>\n",
       "      <td>July 12, 2020</td>\n",
       "      <td>Jul 13th, 2020 3:22 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>77</td>\n",
       "      <td>$200 off</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Quickjack 7000slx $200 off - $1300</td>\n",
       "      <td>https://www.costco.ca/quickjack-bl-7000slx-318...</td>\n",
       "      <td>15093</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>jugojugo</td>\n",
       "      <td>Jul 8th, 2020 9:26 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:22 pm</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Home Improvement &amp; Tools</td>\n",
       "      <td>RYOBI 2020 Summer Tour - 20% off 18V + 40V too...</td>\n",
       "      <td>https://bit.ly/RYOBISummerTour</td>\n",
       "      <td>16592</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>idiotcanuck</td>\n",
       "      <td>Jul 13th, 2020 12:06 pm</td>\n",
       "      <td>July 14, 2020</td>\n",
       "      <td>Jul 13th, 2020 3:22 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>517.99</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Televisions</td>\n",
       "      <td>Hisense 55-in. 4K ULED Android Smart TV 55Q7G ...</td>\n",
       "      <td>https://www.costco.ca/hisense-55-in.-4k-uled-a...</td>\n",
       "      <td>1562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>kooltilltheen</td>\n",
       "      <td>Jul 7th, 2020 4:12 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:22 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Upto 75% Off On Everything On Old Navy/Gap + E...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19157</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>tmd2006</td>\n",
       "      <td>Jul 13th, 2020 11:49 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:21 pm</td>\n",
       "      <td>Kids &amp; Babies</td>\n",
       "      <td>199</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Costco.ca Geometric dome climbing structure $199</td>\n",
       "      <td>https://www.costco.ca/lifetime-geometric-dome-...</td>\n",
       "      <td>2499</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>fangdragon2000</td>\n",
       "      <td>Jul 9th, 2020 11:51 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 13th, 2020 3:20 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>139,99</td>\n",
       "      <td>65</td>\n",
       "      <td>30$</td>\n",
       "      <td>Amazon.ca</td>\n",
       "      <td>Peripherals &amp; Accessories</td>\n",
       "      <td>ADATA USA Ultimate Su800 1TB 3D Nand 2.5 Inch ...</td>\n",
       "      <td>http://www.amazon.ca/gp/redirect.html?ie=UTF8&amp;...</td>\n",
       "      <td>8986</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author            creation_date         expiry  \\\n",
       "0         trystee    Jan 1st, 2020 8:32 pm            NaN   \n",
       "1       yellowmp5   Jul 13th, 2020 1:29 pm            NaN   \n",
       "2     phoreoneone  Jul 13th, 2020 12:34 pm            NaN   \n",
       "3     Blackdove77   Jul 6th, 2020 12:49 pm  July 19, 2020   \n",
       "4        hkhorace    Jul 6th, 2020 9:56 am  July 12, 2020   \n",
       "5        jugojugo    Jul 8th, 2020 9:26 pm            NaN   \n",
       "6     idiotcanuck  Jul 13th, 2020 12:06 pm  July 14, 2020   \n",
       "7   kooltilltheen    Jul 7th, 2020 4:12 am            NaN   \n",
       "8         tmd2006  Jul 13th, 2020 11:49 am            NaN   \n",
       "9  fangdragon2000   Jul 9th, 2020 11:51 am            NaN   \n",
       "\n",
       "               last_reply          parent_category   price replies    saving  \\\n",
       "0  Jul 13th, 2020 3:25 pm                      NaN     NaN     672       NaN   \n",
       "1  Jul 13th, 2020 3:23 pm                      NaN     NaN      26       NaN   \n",
       "2  Jul 13th, 2020 3:23 pm                      NaN     NaN      23       NaN   \n",
       "3  Jul 13th, 2020 3:23 pm  Computers & Electronics     NaN     413      100%   \n",
       "4  Jul 13th, 2020 3:22 pm                      NaN    1300      77  $200 off   \n",
       "5  Jul 13th, 2020 3:22 pm            Home & Garden     NaN      87       NaN   \n",
       "6  Jul 13th, 2020 3:22 pm  Computers & Electronics  517.99      20       NaN   \n",
       "7  Jul 13th, 2020 3:22 pm                      NaN     NaN      51       NaN   \n",
       "8  Jul 13th, 2020 3:21 pm            Kids & Babies     199       7       NaN   \n",
       "9  Jul 13th, 2020 3:20 pm  Computers & Electronics  139,99      65       30$   \n",
       "\n",
       "       source            thread_category  \\\n",
       "0         NaN                  Groceries   \n",
       "1  Home Depot              Home & Garden   \n",
       "2        Dell    Computers & Electronics   \n",
       "3         NaN                Video Games   \n",
       "4      Costco                 Automotive   \n",
       "5  Home Depot   Home Improvement & Tools   \n",
       "6      Costco                Televisions   \n",
       "7         NaN                    Apparel   \n",
       "8      Costco               Toys & Games   \n",
       "9   Amazon.ca  Peripherals & Accessories   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Various Retailers] Gift Card Deals And Discou...   \n",
       "1                           RYOBI 20% coupon barcode   \n",
       "2  Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...   \n",
       "3           Watchdogs 2 PC version free for Everyone   \n",
       "4                 Quickjack 7000slx $200 off - $1300   \n",
       "5  RYOBI 2020 Summer Tour - 20% off 18V + 40V too...   \n",
       "6  Hisense 55-in. 4K ULED Android Smart TV 55Q7G ...   \n",
       "7  Upto 75% Off On Everything On Old Navy/Gap + E...   \n",
       "8   Costco.ca Geometric dome climbing structure $199   \n",
       "9  ADATA USA Ultimate Su800 1TB 3D Nand 2.5 Inch ...   \n",
       "\n",
       "                                                 url   views votes  \n",
       "0                                                NaN  365169   317  \n",
       "1                                                NaN    2593    28  \n",
       "2  http://www.jdoqocy.com/click-749547-12105225?u...    2339    15  \n",
       "3  https://register.ubisoft.com/ubisoft-forward-r...   57748   180  \n",
       "4  https://www.costco.ca/quickjack-bl-7000slx-318...   15093    26  \n",
       "5                     https://bit.ly/RYOBISummerTour   16592    28  \n",
       "6  https://www.costco.ca/hisense-55-in.-4k-uled-a...    1562     4  \n",
       "7                                                NaN   19157    16  \n",
       "8  https://www.costco.ca/lifetime-geometric-dome-...    2499     3  \n",
       "9  http://www.amazon.ca/gp/redirect.html?ie=UTF8&...    8986     3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First page information, and set total_pages through get_posts()\n",
    "# Generate list of posts on first page\n",
    "print('Extracting information from page: 1')\n",
    "posts = get_posts(root_url)  \n",
    "# Fill table from information on first page and corresponding posts\n",
    "fill_table(posts)\n",
    "\n",
    "#Loop through pages and fill table\n",
    "for page in range(2, total_pages):\n",
    "    next_url = root_url + str(page) + \"/\" # URL of next page: base-url + number + \"/\"\n",
    "    print('Extracting information from page: ', page, \" of \", total_pages)\n",
    "    # Generate list of posts on current page\n",
    "    posts = get_posts(next_url)\n",
    "\n",
    "    # Fill table from information on current page and posts\n",
    "    fill_table(posts)\n",
    "\n",
    "# Print first 10 rows\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to csv file\n",
    "table.to_csv('C:/Users/User/Documents/GitHub/Data-Science/rfd_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 15 columns):\n",
      "Unnamed: 0         1500 non-null int64\n",
      "author             1500 non-null object\n",
      "creation_date      1500 non-null object\n",
      "expiry             451 non-null object\n",
      "last_reply         1500 non-null object\n",
      "parent_category    920 non-null object\n",
      "price              1017 non-null object\n",
      "replies            1500 non-null int64\n",
      "saving             591 non-null object\n",
      "source             1115 non-null object\n",
      "thread_category    1496 non-null object\n",
      "title              1500 non-null object\n",
      "url                1163 non-null object\n",
      "views              1500 non-null int64\n",
      "votes              1500 non-null int64\n",
      "dtypes: int64(4), object(11)\n",
      "memory usage: 175.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('rfd_scrape.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jun 26th, 2020 10:03 am    3\n",
       "Jul 7th, 2020 11:53 am     2\n",
       "Jul 9th, 2020 11:51 am     2\n",
       "Jul 11th, 2020 12:26 am    2\n",
       "Jul 10th, 2020 5:57 pm     2\n",
       "                          ..\n",
       "Jul 2nd, 2020 10:10 pm     1\n",
       "Jul 6th, 2020 12:49 pm     1\n",
       "Jul 3rd, 2020 7:00 pm      1\n",
       "Jun 11th, 2020 9:26 pm     1\n",
       "Jul 6th, 2020 3:13 pm      1\n",
       "Name: creation_date, Length: 1383, dtype: int64"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

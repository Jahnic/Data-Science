{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](rfd_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RedFlagDeals](https://forums.redflagdeals.com/hot-deals-f9/) is a forum where users can post sales or deals that they have come across. The first part of this project is focused on scraping relevant information from the \"All Hot Deals\" section that includes all product categories. In the second and third part I will clean and visualize the data to extract and summarize useful information. \n",
    "\n",
    "Two tables will be scraped. The main table with a description of all columns is shown below. The second table will store all comments that were made on a post. To insure, that comments can be linked back to the original post, we will also store the corresponding post titles in a second column.\n",
    "\n",
    "|Column name|Description|\n",
    "|---|---|\n",
    "|'title'| Title of post|\n",
    "|'votes'| Sum of up-, and down-votes|\n",
    "|'source'| Name of retailer offering the sale|\n",
    "|'creation_date'| Date of initial post|\n",
    "|'last_reply'| Date of most recent reply|\n",
    "|'author'| User name of post author|\n",
    "|'replies'| Number of replies|\n",
    "|'views'| Number of views|\n",
    "|'price'| Price of product on sale|\n",
    "|'saving'| Associated saving|\n",
    "|'expiry'| Expiry date of sale|\n",
    "|'url'| Link to deal|\n",
    "\n",
    "The comments can be used for natural language processing and analyzing sentiment more robustly. Taking only upvotes and the number of replies into consideration may not be enough to accurately reflect the value of a deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests # Scraping\n",
    "from bs4 import BeautifulSoup # HTML parsing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Retrieving data from the \"All Hot Deals\" sub-forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"All Hot Deals\" section is an overview page of recent posts made on the forum. Information about posts such as title and number of upvotes are summarized and listed. The summaries in the \"All Hot Deals\" section as well as the comments on individual posts are organized into several pages.\n",
    "\n",
    "To scrap as much information as possible, we need to iterate through the different pages on \"All Hot Deals\". Further, we need to access each individual post to obtain additional information not found in the summary. For each post, we will also scrape the comments made by users. For this will need to iterate through the available pages on each post.\n",
    "\n",
    "\n",
    "URL format for different pages: `root-url/page#/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables used to iterate over web-pages.\n",
    "current_page = \"\" # page number; used to format root URL\n",
    "total_pages = 1 # endpoint for iteration; set through get_posts()\n",
    "root_url = \"https://forums.redflagdeals.com/hot-deals-f9/\" # base url for \"All Hot Deals\" sub-forum\n",
    "\n",
    "# URL base to generate links to specific posts\n",
    "base_url = \"https://forums.redflagdeals.com\"\n",
    "\n",
    "# Dataframe to store scraped data\n",
    "main_table = pd.DataFrame(columns=\n",
    "    ['title',\n",
    "    'votes',\n",
    "    'source',\n",
    "    'creation_date',\n",
    "    'last_reply',\n",
    "    'author',\n",
    "    'replies',\n",
    "    'views',\n",
    "    'price',\n",
    "    'saving',\n",
    "    'expiry',\n",
    "    'url'])\n",
    "\n",
    "# Dataframe to to store post comments\n",
    "comment_table = pd.DataFrame(columns=\n",
    "                       ['title',\n",
    "                       'comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(page: str) -> list:\n",
    "    \"\"\"\n",
    "    Returns list of parsed object containing all post elements from\n",
    "    the current 'page' and sets gloabl variable 'total_pages'\n",
    "    \n",
    "    Args:\n",
    "    page - url string of current page\n",
    "    \n",
    "    Returns:\n",
    "    topics - all parsed elements of class 'row topic'\n",
    "    total_pages - sets global variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initalize list of posts on page class=\"row topic\"\n",
    "    posts = []\n",
    "    \n",
    "    # Get entire page content\n",
    "    response = requests.get(page)\n",
    "    content = response.content\n",
    "    \n",
    "    # Find total number of pages and set global variable accordingly\n",
    "    # Format of text: \" {current page #} of {total page #} \"\n",
    "    # Need to strip white space and extract total page #\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    pages = parser.select(\".pagination_menu_trigger\")[0].text.strip().split(\"of \")[1]\n",
    "    global total_pages\n",
    "    total_pages = int(pages)\n",
    "    \n",
    "    # Find and return topics\n",
    "    topics = parser.find_all(\"li\", class_=\"row topic\")\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_info(post: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts and returns additional information from a RedFlagDeals post:\n",
    "    url-link to the deal, the price of the product, the discount saving, \n",
    "    the expiry date, the parent/thread categories of the product and a list of\n",
    "    comments found on all pages. \n",
    "    \n",
    "    Args:\n",
    "    post - url string linking to a specific post\n",
    "    \n",
    "    Returns:\n",
    "    additional_info - dictionary containing additional information about the post.\n",
    "    Stores NaN values in \"additional_info\" for objects that are not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Additional information found in post\n",
    "    additional_info = {}\n",
    "    # Reviews found on all pages of post\n",
    "    \n",
    "    # Get content of post\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Thread-header with information on parent and thread category\n",
    "    try: # parent category\n",
    "        parent_category = parser.select(\".thread_parent_category\")[0].text\n",
    "        additional_info['Parent:'] = parent_category\n",
    "    except: additional_info['Parent:'] = np.nan # NaN if category not found\n",
    "    try: # thread category\n",
    "        thread_category = parser.select(\".thread_category\")[0].text\n",
    "        additional_info['Thread:'] = thread_category\n",
    "    except: additional_info['Thread:'] = np.nan # NaN if category not found\n",
    "    \n",
    "    \n",
    "    # Offer-summary field: may contain deal link, price, saving, and retailer\n",
    "    summary = parser.select(\".post_offer_fields\") # format example: \"Price:\\n$200\\nSaving:\\n70%\"\n",
    "    try:\n",
    "        summary_list = summary[0].text.split(\"\\n\") \n",
    "    except: summary_list = []\n",
    "        \n",
    "    # Example format of summary_list: [\"\", \"Price:\", \"200$\", \"Saving:\", 50%, \"Expiry:\", \"July 23, 2020\"]\n",
    "    for i in range(1, (len(summary_list) -1), 2): # index 0 is empty string\n",
    "        current_element = summary_list[i] # content of current list element\n",
    "        next_element = summary_list[i+1] # next list element containing the information\n",
    "        \n",
    "        # Price, saving, and expiry date information contained in the next list element will be saved\n",
    "        if current_element.startswith(\"Price\") or current_element.startswith(\"Saving\") or current_element.startswith(\"Expiry\"):\n",
    "            additional_info[current_element]  = next_element # next elements corrsponds to content\n",
    "            \n",
    "    # URL to link. Full link not available through .text\n",
    "    try: \n",
    "        url = str(summary[0]).split('href=\"')[1].split('\"')[0] # select link between href=\" and \"\n",
    "        additional_info['Link:'] = url\n",
    "    except: additional_info['Link:'] = np.nan\n",
    "        \n",
    "    \n",
    "    # If any of the elements is not found in the summary-field add None value to dictionary \n",
    "    if \"Price:\" not in additional_info:\n",
    "        additional_info['Price:'] = np.nan\n",
    "        \n",
    "    if \"Savings:\" not in additional_info:\n",
    "        additional_info['Savings:'] = np.nan\n",
    "        \n",
    "    if \"Expiry:\" not in additional_info:\n",
    "        additional_info['Expiry:'] = np.nan\n",
    "    \n",
    "    return additional_info # Return dictionary containing with information on price, saving and expiry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_comments(post:str, title:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves comments from all post pages.\n",
    "    \n",
    "    Arg:\n",
    "    post: url to the first page of a post\n",
    "    title: the title of the post\n",
    "    \n",
    "    Returns:\n",
    "    df_comments: DataFrame object. Each row corresponds to an indevidual comment.\n",
    "    A second column indicates the title of original post on which the comment was made\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data storage\n",
    "    df_comments = pd.DataFrame()\n",
    "    comment_list = []\n",
    "    \n",
    "    # subsequent pages can be retireved through: root_url + \"page#/\"\n",
    "    root_url = post\n",
    "    \n",
    "    # Get and parse content from first post page\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Retrive total number of pages for iteration\n",
    "    # There is only one page, if the \"pagination_menu_trigger\" class doesn't exist -> except\n",
    "    try:\n",
    "        pages = parser.select(\".pagination_menu_trigger\")[0].text\n",
    "        last_page = int(re.findall(\"\\d+\", pages)[1]) # first element is current page, second is last page\n",
    "    except:\n",
    "        last_page = 1\n",
    "    \n",
    "    # Iterate through pages and retrieve comments\n",
    "    for page in range(1,(last_page+1)):\n",
    "        if page == 1:\n",
    "            # print(title, \"\\nPage 1 is being scrapped for comments...\")\n",
    "            \n",
    "            #Get comments from first page\n",
    "            comments = parser.select(\".post_content .content\")\n",
    "            comment_list.extend(comment.text for comment in comments)\n",
    "            \n",
    "            # print(\"Comments found on page 1:\", len(comments))\n",
    "        else:\n",
    "            # print(\"Page {} is being scrapped for comments...\".format(page))\n",
    "            \n",
    "            # Parse next page\n",
    "            next_page = root_url + str(page) # next page\n",
    "            response = requests.get(next_page)\n",
    "            content = response.content\n",
    "            parser = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # Get comments\n",
    "            comments = parser.select(\".post_content .content\")\n",
    "            comment_list.extend(comment.text for comment in comments)\n",
    "            \n",
    "            # print(\"Comments found on page {}:\".format(page), len(comments))\n",
    "    \n",
    "    # Fill dataframe to return\n",
    "    title_col = pd.Series(title for i in range(len(comment_list)))\n",
    "    df_comments['title'] = title_col\n",
    "    df_comments['comments'] = pd.Series(comment_list)\n",
    "    return df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_table(posts: list) -> None:\n",
    "    '''\n",
    "    Extracts parsed data from current page and appends to the global table variable.\n",
    "    \n",
    "    Args:\n",
    "    posts - list of parsed post elements obtained through get_posts()\n",
    "    '''\n",
    "    \n",
    "    # Temporary DataFrame object that will be appended to the global 'table' variable\n",
    "    tmp_table = pd.DataFrame()\n",
    "    tmp_comments = pd.DataFrame()\n",
    "    \n",
    "    # Initializing columns for tmp_table\n",
    "    title_col = pd.Series()\n",
    "    source_col = pd.Series()\n",
    "    url_col = pd.Series()\n",
    "    votes_col = pd.Series()\n",
    "    replies_col = pd.Series()\n",
    "    views_col = pd.Series()\n",
    "    creation_date_col = pd.Series()\n",
    "    last_reply_col = pd.Series()\n",
    "    author_col = pd.Series()\n",
    "    price_col = pd.Series()\n",
    "    saving_col = pd.Series()\n",
    "    expiry_col = pd.Series()\n",
    "    parent_col = pd.Series()\n",
    "    thread_col = pd.Series()\n",
    "    \n",
    "    #Initializing columns for tmp_comments\n",
    "    comment_title = pd.Series()\n",
    "    comment_order = pd.Series()\n",
    "    comment_comment = pd.Series()\n",
    "    \n",
    "\n",
    "    # Iterate through post elements on current page and extract data for table\n",
    "    for post in posts:\n",
    "        \n",
    "        # Retailer corresponding to deal\n",
    "        try: \n",
    "            source = post.select(\".topictitle_retailer\")[0].text.split(\"\\n\")[0] # split and remove line-break characters\n",
    "            source_series = pd.Series(source) # transforming into Series object allows use of .append method\n",
    "        except: source_series = pd.Series(np.nan)\n",
    "        source_col = source_col.append(source_series, ignore_index=True)\n",
    "\n",
    "        # Number of votes\n",
    "        try: \n",
    "            votes = post.select(\".post_voting\")[0].text.split(\"\\n\")[1] \n",
    "            votes_series = pd.Series(votes) \n",
    "        except: votes_series = pd.Series(0)\n",
    "        votes_col = votes_col.append(votes_series, ignore_index=True)\n",
    "            \n",
    "        # Title \n",
    "        try:\n",
    "            topic = post.select(\".topic_title_link\") \n",
    "            title = topic[0].text.split('\\n')[1] \n",
    "            title_series = pd.Series(title)\n",
    "        except: title_series = pd.Series(np.nan)\n",
    "        title_col = title_col.append(title_series, ignore_index=True)\n",
    "\n",
    "        # Date of initial posting\n",
    "        try: \n",
    "            creation = post.select(\".first-post-time\")[0].text.split(\"\\n\")[0]\n",
    "            creation_series = pd.Series(creation)\n",
    "        except: creation_series = pd.Series(np.nan)\n",
    "        creation_date_col = creation_date_col.append(creation_series, ignore_index=True) \n",
    "        \n",
    "        # Date of most recent replie\n",
    "        try: \n",
    "            last_replie = post.select(\".last-post-time\")[0].text.split(\"\\n\")[0]\n",
    "            last_replie_series = pd.Series(last_replie)\n",
    "        except: last_replie_series = pd.Series(np.nan)\n",
    "        last_reply_col = last_reply_col.append(last_replie_series, ignore_index=True) \n",
    "        \n",
    "        # Author user-name\n",
    "        try:\n",
    "            author = post.select(\".thread_meta_author\")[0].text.split(\"\\n\")[0]\n",
    "            author_series = pd.Series(author)\n",
    "        except: author_series = pd.Series(np.nan)\n",
    "        author_col = author_col.append(author_series, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Number of replies\n",
    "        try:\n",
    "            replies = post.select(\".posts\")[0].text.split(\"\\n\")[0]\n",
    "            replies = replies.replace(\",\",\"\") # remove commas to facilitate data type conversion to integer\n",
    "            replies_series = pd.Series(replies)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        replies_col = replies_col.append(replies_series, ignore_index=True)\n",
    "        \n",
    "        # Number of views\n",
    "        try:\n",
    "            views = post.select(\".views\")[0].text.split(\"\\n\")[0]\n",
    "            views = views.replace(\",\",\"\") # remove commas to facilitate integer conversion\n",
    "            views_series = pd.Series(views)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        views_col = views_col.append(views_series, ignore_index=True)\n",
    "        \n",
    "        # Link to current post used to extract additional information\n",
    "        try:\n",
    "            link = str(topic).split('href=\"')[1] # split at href to extract link\n",
    "            link_clean = link.split('\">')[0] # remove superfluous characters\n",
    "        except: \n",
    "            link_clean = np.nan\n",
    "        \n",
    "        # Additional information post\n",
    "        if link_clean != None: # retrieve information from post, if url exists\n",
    "            post_url = (base_url + \"{}\").format(link_clean) # merge base-, and sub-url to generate the complete post-link\n",
    "            additional_info = get_additional_info(post_url) # get dictionary of additonal information on price, saving, etc.\n",
    "            \n",
    "            # Fill columns with additional information from additional_info dictionary\n",
    "            price_col = price_col.append(pd.Series(additional_info['Price:']), ignore_index=True)\n",
    "            saving_col = saving_col.append(pd.Series(additional_info['Savings:']), ignore_index=True)\n",
    "            expiry_col = expiry_col.append(pd.Series(additional_info['Expiry:']), ignore_index=True)\n",
    "            url_col = url_col.append(pd.Series(additional_info['Link:']), ignore_index=True)\n",
    "            parent_col = parent_col.append(pd.Series(additional_info['Parent:']), ignore_index=True)\n",
    "            thread_col = thread_col.append(pd.Series(additional_info['Thread:']), ignore_index=True)\n",
    "            \n",
    "            # get comments from post\n",
    "            comments_tmp = get_post_comments(post_url, title)\n",
    "        else:\n",
    "            price_col = price_col.append(np.nan)\n",
    "            saving_col = saving_col.append(np.nan)\n",
    "            expiry_col = expiry_col.append(np.nan)\n",
    "            url_col = url_col.append(np.nan)\n",
    "        \n",
    "            \n",
    "    # Fill temporary table\n",
    "    tmp_table['title'] = title_col\n",
    "    tmp_table['votes'] = votes_col.astype(int)\n",
    "    tmp_table['source'] = source_col\n",
    "    tmp_table['creation_date'] = creation_date_col\n",
    "    tmp_table['last_reply'] = last_reply_col\n",
    "    tmp_table['author'] = author_col\n",
    "    tmp_table['replies'] = replies_col.astype(int)\n",
    "    tmp_table['views'] = views_col.astype(int)\n",
    "    tmp_table['price'] = price_col\n",
    "    tmp_table['saving'] = saving_col\n",
    "    tmp_table['expiry'] = expiry_col\n",
    "    tmp_table['url'] = url_col\n",
    "    tmp_table['parent_category'] = parent_col\n",
    "    tmp_table['thread_category'] = thread_col\n",
    "           \n",
    "    # Append temporary objects to global variables \n",
    "    global main_table # gloabal keyword allows modification inside function\n",
    "    main_table = main_table.append(tmp_table)\n",
    "    global comment_table\n",
    "    comment_table = comment_table.append(comments_tmp)\n",
    "    \n",
    "    \n",
    "    # Logging progress\n",
    "    print(\"Current main_table length:\", main_table.shape[0])\n",
    "    print(\"Current comment_table lenth:\", comment_table.shape[0])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information from page: 1\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current main_table length: 30\n",
      "Current comment_table lenth: 97\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  2  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 60\n",
      "Current comment_table lenth: 801\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  3  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 90\n",
      "Current comment_table lenth: 841\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  4  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 120\n",
      "Current comment_table lenth: 844\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  5  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 150\n",
      "Current comment_table lenth: 909\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  6  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 180\n",
      "Current comment_table lenth: 1032\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  7  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 210\n",
      "Current comment_table lenth: 1057\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  8  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 240\n",
      "Current comment_table lenth: 1082\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  9  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 269\n",
      "Current comment_table lenth: 1119\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  10  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 299\n",
      "Current comment_table lenth: 1133\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  11  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 329\n",
      "Current comment_table lenth: 1185\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  12  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 359\n",
      "Current comment_table lenth: 1252\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  13  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 389\n",
      "Current comment_table lenth: 1266\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  14  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 419\n",
      "Current comment_table lenth: 1270\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  15  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 449\n",
      "Current comment_table lenth: 1319\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  16  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 479\n",
      "Current comment_table lenth: 1352\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  17  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 509\n",
      "Current comment_table lenth: 1364\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  18  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 539\n",
      "Current comment_table lenth: 1372\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  19  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 569\n",
      "Current comment_table lenth: 1387\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  20  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 599\n",
      "Current comment_table lenth: 1546\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  21  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 629\n",
      "Current comment_table lenth: 1574\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  22  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 659\n",
      "Current comment_table lenth: 1687\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  23  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 689\n",
      "Current comment_table lenth: 1689\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  24  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 719\n",
      "Current comment_table lenth: 1902\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  25  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 749\n",
      "Current comment_table lenth: 2037\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  26  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 779\n",
      "Current comment_table lenth: 2063\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  27  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 809\n",
      "Current comment_table lenth: 2076\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  28  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 839\n",
      "Current comment_table lenth: 2082\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  29  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 869\n",
      "Current comment_table lenth: 2090\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  30  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 899\n",
      "Current comment_table lenth: 2123\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  31  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 929\n",
      "Current comment_table lenth: 2156\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  32  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 959\n",
      "Current comment_table lenth: 2194\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  33  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 989\n",
      "Current comment_table lenth: 2201\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  34  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1019\n",
      "Current comment_table lenth: 2209\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  35  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1049\n",
      "Current comment_table lenth: 2230\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  36  of  44\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1079\n",
      "Current comment_table lenth: 2238\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  37  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1109\n",
      "Current comment_table lenth: 2249\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  38  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1139\n",
      "Current comment_table lenth: 2464\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  39  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1169\n",
      "Current comment_table lenth: 2495\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  40  of  45\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current main_table length: 1199\n",
      "Current comment_table lenth: 2504\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  41  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1229\n",
      "Current comment_table lenth: 2505\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  42  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1259\n",
      "Current comment_table lenth: 2506\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  43  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1289\n",
      "Current comment_table lenth: 2523\n",
      "==================================================\n",
      "\n",
      "Extracting information from page:  44  of  45\n",
      "--------------------------------------------------\n",
      "Current main_table length: 1319\n",
      "Current comment_table lenth: 2526\n",
      "==================================================\n",
      "Total time for scraping: 92.12827054659526 min.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Get first page information, and set total_pages through get_posts()\n",
    "print('Extracting information from page: 1')\n",
    "print(\"-\"*50)\n",
    "posts = get_posts(root_url)  \n",
    "# Extract infomation from first page to fill table with data\n",
    "fill_table(posts)\n",
    "\n",
    "#Loop through pages and add data to table\n",
    "for page in range(2, (total_pages + 1)):\n",
    "    next_url = root_url + str(page) + \"/\" # URL of next page: base-url + number + \"/\"\n",
    "    print('\\nExtracting information from page: ', page, \" of \", total_pages)\n",
    "    print(\"-\"*50)\n",
    "    # Generate list of posts on current page\n",
    "    posts = get_posts(next_url)\n",
    "\n",
    "    # Fill table from information on current page and posts\n",
    "    fill_table(posts)\n",
    "    \n",
    "print('Total time for scraping:', (time.time()-start_time)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to csv file\n",
    "main_table.to_csv('rfd_main.csv')\n",
    "comment_table.to_csv('rfd_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MercadorHG</td>\n",
       "      <td>Jul 16th, 2020 11:25 am</td>\n",
       "      <td>July 23, 2020</td>\n",
       "      <td>Jul 16th, 2020 8:17 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>0$</td>\n",
       "      <td>19</td>\n",
       "      <td>100%</td>\n",
       "      <td>Epic Games</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Epic free games 16 - 23 July Torchlight II</td>\n",
       "      <td>https://www.epicgames.com/store/en-US/product/...</td>\n",
       "      <td>4465</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>tmd2006</td>\n",
       "      <td>Jul 13th, 2020 11:49 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 8:13 pm</td>\n",
       "      <td>Kids &amp; Babies</td>\n",
       "      <td>199</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Costco.ca Geometric dome climbing structure $199</td>\n",
       "      <td>https://www.costco.ca/lifetime-geometric-dome-...</td>\n",
       "      <td>23084</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Blubbs</td>\n",
       "      <td>Jul 14th, 2020 6:39 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 8:11 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>$330.03</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lenovo Canada</td>\n",
       "      <td>Peripherals &amp; Accessories</td>\n",
       "      <td>Lenovo 28\" UHD 4K IPS Monitor $366.70 plus 10%...</td>\n",
       "      <td>http://lenovo.evyy.net/c/341376/225728/3899?u=...</td>\n",
       "      <td>3947</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>immad01</td>\n",
       "      <td>Jul 16th, 2020 8:10 pm</td>\n",
       "      <td>July 22, 2020</td>\n",
       "      <td>Jul 16th, 2020 8:10 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real Canadian Superstore</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>RCSS NO TAX ALL WEEK (July 16th through July 2...</td>\n",
       "      <td>https://www.realcanadiansuperstore.ca/print-flyer</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>taya1214</td>\n",
       "      <td>Jul 13th, 2020 1:49 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 8:10 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shell</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Shell Go+ for every Air Miles cardholder</td>\n",
       "      <td>http://shellgoplus.ca</td>\n",
       "      <td>15258</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author            creation_date         expiry              last_reply  \\\n",
       "0  MercadorHG  Jul 16th, 2020 11:25 am  July 23, 2020  Jul 16th, 2020 8:17 pm   \n",
       "1     tmd2006  Jul 13th, 2020 11:49 am            NaN  Jul 16th, 2020 8:13 pm   \n",
       "2      Blubbs   Jul 14th, 2020 6:39 pm            NaN  Jul 16th, 2020 8:11 pm   \n",
       "3     immad01   Jul 16th, 2020 8:10 pm  July 22, 2020  Jul 16th, 2020 8:10 pm   \n",
       "4    taya1214   Jul 13th, 2020 1:49 pm            NaN  Jul 16th, 2020 8:10 pm   \n",
       "\n",
       "           parent_category    price  replies saving                    source  \\\n",
       "0  Computers & Electronics       0$       19   100%                Epic Games   \n",
       "1            Kids & Babies      199       97    NaN                    Costco   \n",
       "2  Computers & Electronics  $330.03       18    NaN             Lenovo Canada   \n",
       "3                      NaN      NaN        0    NaN  Real Canadian Superstore   \n",
       "4                      NaN      NaN      121    NaN                     Shell   \n",
       "\n",
       "             thread_category  \\\n",
       "0                Video Games   \n",
       "1               Toys & Games   \n",
       "2  Peripherals & Accessories   \n",
       "3                  Groceries   \n",
       "4                 Automotive   \n",
       "\n",
       "                                               title  \\\n",
       "0         Epic free games 16 - 23 July Torchlight II   \n",
       "1   Costco.ca Geometric dome climbing structure $199   \n",
       "2  Lenovo 28\" UHD 4K IPS Monitor $366.70 plus 10%...   \n",
       "3  RCSS NO TAX ALL WEEK (July 16th through July 2...   \n",
       "4           Shell Go+ for every Air Miles cardholder   \n",
       "\n",
       "                                                 url  views  votes  \n",
       "0  https://www.epicgames.com/store/en-US/product/...   4465     98  \n",
       "1  https://www.costco.ca/lifetime-geometric-dome-...  23084     24  \n",
       "2  http://lenovo.evyy.net/c/341376/225728/3899?u=...   3947      9  \n",
       "3  https://www.realcanadiansuperstore.ca/print-flyer    122      1  \n",
       "4                              http://shellgoplus.ca  15258     30  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('rfd_main.csv').iloc[:,1:]\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1319 entries, 0 to 1318\n",
      "Data columns (total 14 columns):\n",
      "author             1319 non-null object\n",
      "creation_date      1319 non-null object\n",
      "expiry             377 non-null object\n",
      "last_reply         1319 non-null object\n",
      "parent_category    818 non-null object\n",
      "price              885 non-null object\n",
      "replies            1319 non-null int64\n",
      "saving             521 non-null object\n",
      "source             973 non-null object\n",
      "thread_category    1318 non-null object\n",
      "title              1319 non-null object\n",
      "url                1038 non-null object\n",
      "views              1319 non-null int64\n",
      "votes              1319 non-null int64\n",
      "dtypes: int64(3), object(11)\n",
      "memory usage: 144.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1319</td>\n",
       "      <td>1319</td>\n",
       "      <td>377</td>\n",
       "      <td>1319</td>\n",
       "      <td>818</td>\n",
       "      <td>885</td>\n",
       "      <td>1319.000000</td>\n",
       "      <td>521</td>\n",
       "      <td>973</td>\n",
       "      <td>1318</td>\n",
       "      <td>1319</td>\n",
       "      <td>1038</td>\n",
       "      <td>1319.000000</td>\n",
       "      <td>1319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>943</td>\n",
       "      <td>1272</td>\n",
       "      <td>72</td>\n",
       "      <td>1229</td>\n",
       "      <td>12</td>\n",
       "      <td>604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>289</td>\n",
       "      <td>145</td>\n",
       "      <td>54</td>\n",
       "      <td>1300</td>\n",
       "      <td>1010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>immad01</td>\n",
       "      <td>Jun 26th, 2020 10:03 am</td>\n",
       "      <td>July 2, 2020</td>\n",
       "      <td>Jul 16th, 2020 6:59 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Free</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50%</td>\n",
       "      <td>Amazon.ca</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>1-Year PlayStation Plus Membership (Digital De...</td>\n",
       "      <td>http://reebok-canada.sjv.io/c/341376/356400/52...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>345</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>205</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.786960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13826.701289</td>\n",
       "      <td>13.085671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>145.582268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33992.049347</td>\n",
       "      <td>29.693289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>-72.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2177.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4255.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11124.500000</td>\n",
       "      <td>14.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2787.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>506067.000000</td>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author            creation_date        expiry  \\\n",
       "count      1319                     1319           377   \n",
       "unique      943                     1272            72   \n",
       "top     immad01  Jun 26th, 2020 10:03 am  July 2, 2020   \n",
       "freq         34                        3            30   \n",
       "mean        NaN                      NaN           NaN   \n",
       "std         NaN                      NaN           NaN   \n",
       "min         NaN                      NaN           NaN   \n",
       "25%         NaN                      NaN           NaN   \n",
       "50%         NaN                      NaN           NaN   \n",
       "75%         NaN                      NaN           NaN   \n",
       "max         NaN                      NaN           NaN   \n",
       "\n",
       "                    last_reply          parent_category price      replies  \\\n",
       "count                     1319                      818   885  1319.000000   \n",
       "unique                    1229                       12   604          NaN   \n",
       "top     Jul 16th, 2020 6:59 pm  Computers & Electronics  Free          NaN   \n",
       "freq                         6                      345    13          NaN   \n",
       "mean                       NaN                      NaN   NaN    53.786960   \n",
       "std                        NaN                      NaN   NaN   145.582268   \n",
       "min                        NaN                      NaN   NaN    -1.000000   \n",
       "25%                        NaN                      NaN   NaN     6.000000   \n",
       "50%                        NaN                      NaN   NaN    16.000000   \n",
       "75%                        NaN                      NaN   NaN    45.000000   \n",
       "max                        NaN                      NaN   NaN  2787.000000   \n",
       "\n",
       "       saving     source          thread_category  \\\n",
       "count     521        973                     1318   \n",
       "unique    289        145                       54   \n",
       "top       50%  Amazon.ca  Computers & Electronics   \n",
       "freq       29        205                      172   \n",
       "mean      NaN        NaN                      NaN   \n",
       "std       NaN        NaN                      NaN   \n",
       "min       NaN        NaN                      NaN   \n",
       "25%       NaN        NaN                      NaN   \n",
       "50%       NaN        NaN                      NaN   \n",
       "75%       NaN        NaN                      NaN   \n",
       "max       NaN        NaN                      NaN   \n",
       "\n",
       "                                                    title  \\\n",
       "count                                                1319   \n",
       "unique                                               1300   \n",
       "top     1-Year PlayStation Plus Membership (Digital De...   \n",
       "freq                                                    2   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "                                                      url          views  \\\n",
       "count                                                1038    1319.000000   \n",
       "unique                                               1010            NaN   \n",
       "top     http://reebok-canada.sjv.io/c/341376/356400/52...            NaN   \n",
       "freq                                                    3            NaN   \n",
       "mean                                                  NaN   13826.701289   \n",
       "std                                                   NaN   33992.049347   \n",
       "min                                                   NaN     122.000000   \n",
       "25%                                                   NaN    2177.500000   \n",
       "50%                                                   NaN    4255.000000   \n",
       "75%                                                   NaN   11124.500000   \n",
       "max                                                   NaN  506067.000000   \n",
       "\n",
       "              votes  \n",
       "count   1319.000000  \n",
       "unique          NaN  \n",
       "top             NaN  \n",
       "freq            NaN  \n",
       "mean      13.085671  \n",
       "std       29.693289  \n",
       "min      -72.000000  \n",
       "25%        0.000000  \n",
       "50%        5.000000  \n",
       "75%       14.500000  \n",
       "max      318.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Hottest Dell deal I've seen in a while. Thanks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Something to note is that it seems like the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Isn't Rakuten 2% today? I can't see the 6%.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>OP you should block out your coupon code if yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MrMello wrote: \\nIsn't Rakuten 2% today? I ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments\n",
       "0  Hottest Dell deal I've seen in a while. Thanks...\n",
       "1  Something to note is that it seems like the co...\n",
       "2        Isn't Rakuten 2% today? I can't see the 6%.\n",
       "3  OP you should block out your coupon code if yo...\n",
       "4  MrMello wrote: \\nIsn't Rakuten 2% today? I ca..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = pd.read_csv(\"rfd_comments.csv\").loc[:,\"title\":]\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2526 entries, 0 to 2525\n",
      "Data columns (total 1 columns):\n",
      "comments    2525 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 19.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the the short exploration above we can see that we need to remove one row with missing values for the comments column. These probably correspond to comments without text. Further, comment strings will need to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2378 entries, 0 to 2379\n",
      "Data columns (total 2 columns):\n",
      "comments    2378 non-null object\n",
      "title       2378 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 55.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Delete rows with empty comments\n",
    "df_comments.dropna(axis=0, inplace=True)\n",
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hottest Dell deal I've seen in a while. Thanks to @Neovingian for bringing this to my attention.\\n\\nSign up with an email address and get a 15% off coupon that stacks with other discounts like 10%OFFMONITOR. Also Rakuten has special 6% cash back right now which makes the deals fantastic when combined with Cyber Event discounts, doorcrashers etc.\\n\\nOne thing is it works only with work side stuff, I tried applying the discount to Alienware monitors and it doesn't work (only 10%OFFMONITOR does).\\n\\nI'm guessing these are some of the best prices since lockdown hit. I just ordered a Dell U3415W for $620 - 6%CB = $582. Fantastic price, plus it comes with a 3 year advance exchange warranty and premium panel guarantee. I already own one of these monitors and they are my favourite thing. I couldn't justify the extra cost for the USB-C version, though my wife had to nix me buying the U3818W for ~$950.\\n\\nFor the 15% coupon I did have to wait overnight but it came in less than 24 hours. So, not sure how long Rakuten is doing its cash back but should at least stay at 2%.\\n\\nThis should work for laptops etc too, but since I was shopping here are a couple deals.\\n\\nAnd, to top it all off, if you happen to have access to a Business Platinum Amex, you are going to lose your mind because they have a promotion where you get a $250 statement credit for spending $250 at Dell (that's right, 100% back haha). I'm assuming that works with all this but I don't have one as it is a high cost card. dell.ca/amex\\n\\nPic below of a few of the deals - bear in mind this is before cash back. The 15% off coupon works on multiple items in a single order. The 10%OFFMONITOR works only once per order.\\n\\n\\n\\n\\n\\n\", 'Something to note is that it seems like the coupon code only works for Dell branded products.', \"Isn't Rakuten 2% today? I can't see the 6%.\", \"OP you should block out your coupon code if you don't want people stealing it\", \"MrMello wrote: \\nIsn't Rakuten 2% today? I can't see the 6%.\\n\\nYou have to go through Dell Technologies, the business side. Dell Home is 2%.\", \"yesnomaybe wrote: \\nOP you should block out your coupon code if you don't want people stealing it\\n\\nI used it... but yeah maybe. Thanks, I did it.\", 'southbeach82 wrote: \\nSomething to note is that it seems like the coupon code only works for Dell branded products.\\n\\nDoes that mean the Alienware 34\" monitor is excluded?', 'Blubbs wrote: \\nI used it... but yeah maybe. Thanks, I did it.\\n\\nafter you sighed up, did you get a confirmation email immediately? or only the email with coupon arrived after some time?', 'Does it work on Alienware ultrawide?', 'MrBigNose wrote: \\nafter you sighed up, did you get a confirmation email immediately? or only the email with coupon arrived after some time?\\nThe post says they had to wait overnight.', \"Last round of Dell coupons like this, Dell didn't ship most of the orders - wouldn't get excited until you see a shipping confirmation.\", 'amex $250 credit term says must purchase on dell/amex site. in this case when we use rakuten cashback the amex credit will still be applied? Thanks.', \"I signed up for a 15% coupon on the weekend, still haven't received one.\", \"holy this is a good deal. i just bought the u2719dc for ~$100 more. just 2 weeks ago. i guess im within the return period, but i also used dell rewards to purchase so i don't even know how that works. also returning a monitor seems like it'd be a huge pain.\", \"I signed up last week as well.. didn't get the coupon\", 'Signed up. Waiting for my coupon', 'moofachuka wrote: \\nDoes it work on Alienware ultrawide?\\n\\nseems to only work for monitors off the work site (select shop for business). there are no alienware monitors on that site.', \"Thanks OP, It's always nice to get a shout out and be appreciated  \\n\\nI assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!). \\n\\nDell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy!\", 'still waiting for coupon as well. Thanks Dell ', \"For people who haven't gotten an email, you may have signed up in the past or something else may have gone wrong, I'd try a few emails.\\n\\nCheers\", \"MrBigNose wrote: \\nafter you sighed up, did you get a confirmation email immediately? or only the email with coupon arrived after some time?\\n\\nOnly the email when it arrived. It's just radio silence which is a bit annoying.\", \"Neovingian wrote: \\nThanks OP, It's always nice to get a shout out and be appreciated  \\n\\nI assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!). \\n\\nDell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy!\\n\\nThanks again. Most coupons on the Dell Home side don't stack anymore which is I think why this is hot. I guess it must be a more common thing on the work side.\", \"Blubbs wrote: \\nFor people who haven't gotten an email, you may have signed up in the past or something else may have gone wrong, I'd try a few emails.\\n\\nCheers\\n\\nThanks for this post.. but I find these coupon hit and miss.. I have used an account that never signed up for any dell emails and still didn't get it.\\nAlienware used to have the 10% off coupon as well but not anymore\", 'Has anybody confirmed this works for laptops? This seems to be the assumption, but it looks like people have only bought monitors with this so far.', 'MentalAnarchy wrote: \\nHas anybody confirmed this works for laptops? This seems to be the assumption, but it looks like people have only bought monitors with this so far.\\n\\nWorks with this Doorbuster laptop, just tested. I think it will work anything that is not Alienware. And it works with multiple items in one order as stated.\\n\\n', \"Tried a new work email, and no email for 15% off. I'll give it a day here and see whats the hold up.\", 'MentalAnarchy wrote: \\nHas anybody confirmed this works for laptops? This seems to be the assumption, but it looks like people have only bought monitors with this so far.\\n\\nWorks with laptops if they are under work category. Sadly no ryzen 4000 laptops available...\\n\\nBtw code is good until August 15, maybe something decent will go on sale.', 'Can this deal somehow stack a 10% Unidays on top? That would sweet.', \"I miss the time when staples allowed to stack multiple coupons. \\nGet two 2TB 2.5 external disks for less than $60 (all in) each back in 2014 or 2015.\\nNeovingian wrote: \\nThanks OP, It's always nice to get a shout out and be appreciated  \\n\\nI assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!). \\n\\nDell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy!\\n\", \"Didn't get an e-mail from Dell even though I just signed up.\\nkatwittfan wrote: \\nCan this deal somehow stack a 10% Unidays on top? That would sweet.\\n\\nJust try and see\", \"I know those Staples coupon days of 14' 15 were golden, I miss those days much simpler time.\", \"Neovingian wrote: \\nThanks OP, It's always nice to get a shout out and be appreciated  \\n\\nI assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!). \\n\\nDell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy!\\n\\nHow do you get two 15% coupons? Using two emails to sign up?\", \"katwittfan wrote: \\nCan this deal somehow stack a 10% Unidays on top? That would sweet.\\n\\nI tried that and it doesn't work, not stacking and IIRC doesn't work at all for business side.\", 'Any suggestions for a good gaming monitor?', \"goodguy90 wrote: \\nAny suggestions for a good gaming monitor?\\n\\nThe 32 inch curved 165hz QHD is very very good, but the gaming items are excluded from the stacking 15%. You can still get the 10%OFFMONITOR and it's on sale, so around $500.\", 'up vote for quality post with instructions and examples', 'U3818DW - I paid 1250 before taxes lol dang . It is been less than a week. \\n\\nBut if anyone is holding back on getting 38 inch - this is a very very good price!', 'viktor89 wrote: \\nU3818DW - I paid 1250 before taxes lol dang . It is been less than a week. \\n\\nBut if anyone is holding back on getting 38 inch - this is a very very good price!\\n\\nI wanted it so bad... But I don\\'t need the extra vertical space to be honest. I already have one U3415W and this will be my second for the standing desk behind me, plus spousal use. At $582 vs $970 I just couldn\\'t justify the 38\" lol. Even knowing it was the best price!', 'A lot of their IPS monitors are stated to be 5ms grey-to-grey and 8ms something else, what does this mean in terms of comparison to a 10 year old IPS monitor simply stated to be 5ms response time?', 'These monitors need to stack with another 50% off for me to consider them ', 'Originalmotto wrote: \\nThese monitors need to stack with another 50% off for me to consider them \\n\\nThe coupon applies to any business monitor, there are plenty of cheaper ones on sale too. These are just examples. Check out the website!', 'does it work for dell desktops, like alienware r9?', 'My coupon took like 10 days to arrive. That was like few weeks ago. But i did sign up using another email few days later and both coupons came in around the same time. So i dont know if they only release these coupons every so often.\\n\\nI did chat with Dell agent, and was told that coupon normally takes 7 days to arrive.', 'Working for anyone else?', 'Also remember to get another 10% off if you have the American Express offer registered to your card:\\nGet 10% of total purchase as a credit. Up to $1,000 in total credits. Dell Technologies EXPIRES 2020-09-30', \"Bgddss wrote: \\nHow do you get two 15% coupons? Using two emails to sign up?\\nTo my knowledge 2 of those email sign ups won't work on the same order, codes are usually targeted like one 10-15% coupon and 1 promo like 10%-15% off monitors, keyboards or laptops + cb. In the past they used to have 2 promos running concurrently, like spend over $750 save save 10% which also stacks with other promos, SMB promos can be better than consumer, so it's good to check those out as well.\", \"Coupons get sent out in batches. When one of us gets it, all of us will get it. \\n\\nSource: I've bought way too many Dell products and talked to their CS far too often.\", \"Well if anyone has one they're not using, I'm looking for coupon.\", 'setlist wrote: \\nAlso remember to get another 10% off if you have the American Express offer registered to your card:\\nGet 10% of total purchase as a credit. Up to $1,000 in total credits. Dell Technologies EXPIRES 2020-09-30\\n\\nIs that for any American Express card?', \"Thanks OP, setting a more serious home office up so I was shopping for another monitor... Will see if I get a code.\\nMutantBaseball wrote: \\nIs that for any American Express card?\\n\\nNo, I think that's the platinum card. My gold card has the ol' fashion extra points promo where you get 1 point per dollar spent.\", 'Just tried the signup form. Got a 10% coupon within an hour of submission, not 15% like the OP though.\\n\\nCoupon expires in a month.', \"10%OFFMONITOR not working anymore for me. Home or Work site.\\n\\nI think it's a glitch as the coupon shows on their Coupons page:\\nhttps://www.dell.com/en-ca/shop/dell-co ... des?~ck=mn\", 'When I try to use the 10%OFFMONITOR coupon I get \"This is a valid coupon code, but there are no matching items currently in your cart.\"\\n\\nI\\'m only trying to buy an Ultrasharp monitor, it work for anyone? I tried on both the business and consumer website. \\n\\nedit: Student 10% code still works, but I haven\\'t received any email signup code.', 'Same thing, same error message. Tried with a U4919DW monitor.', 'I think they killed the deal', \"Neovingian wrote: \\nTo my knowledge 2 of those email sign ups won't work on the same order, codes are usually targeted like one 10-15% coupon and 1 promo like 10%-15% off monitors, keyboards or laptops + cb. In the past they used to have 2 promos running concurrently, like spend over $750 save save 10% which also stacks with other promos, SMB promos can be better than consumer, so it's good to check those out as well.\\n\\nWell, its probably as rare as the price error thing with Lenovo.\", 'Rip deal is dead :/', 'guavaman wrote: \\nRip deal is dead :/\\n\\nUgh really? I put so much work into this and wanted to spread the love!', 'Blubbs wrote: \\nUgh really? I put so much work into this and wanted to spread the love!\\n\\nYea the 10% code doesnt work but if you are a student you get 10% code, just waiting on the 15% code to pull the trigger', 'guavaman wrote: \\nYea the 10% code doesnt work but if you are a student you get 10% code, just waiting on the 15% code to pull the trigger\\n\\nWhere do you get the student code from?', 'MutantBaseball wrote: \\nWhere do you get the student code from?\\n\\nGo to Unidays', 'MutantBaseball wrote: \\nWhere do you get the student code from?\\n\\nGoogle dell student discount, sign up with unidays link with your student email!', 'guavaman wrote: \\nYea the 10% code doesnt work but if you are a student you get 10% code, just waiting on the 15% code to pull the trigger\\n\\ncan we stack unidays coupon with other coupons?', \"Tried 10%Off Monitor and won't work via business side and tried unidays and it said it's expired... But I just received it and signed up for unidays moments ago...\", \"I want two 4k 27-32in that'll run off single usbc, daisy chained to next monitor only for power/video.. Any idea what models will do this?\", 'Lettts gooo I got the 10% student discount + 10% with the coupon code OP mentioned.\\n\\nthanks OP!', '15% coupon didnt work on random laptops I added to cart(from the work website)', 'Anyone have a 15% coupon code they can share? I signed up for Dell a few days ago and havent received an email unfortunately', 'I would like 15% code too if anyone has a spare please.', 'Waiting for my code to arrive. Anybody know if it works on the new XPS 15 (9500)?', '15% Code showed up. The 10%OFFMONITOR code worked again after following the link from the email.\\n\\nI compared pricing to the same ultrasharp monitor that was bought pre-covid, and with both the 10% and 15% stacked coupons its still within +- 5% of the price I paid at the beginning of the year, so only a hot deal if you need a monitor now and can\\'t wait. Student code doesn\\'t stack with 10%OFFMONITOR FYI.\\n\\nedit: looked up my receipt for comparison with pre-covid with 10% off the price was $904+tax, now the price is $840+tax for the same 43\" 4k monitor, so a better deal than I thought. Good timing for those setting up a home office.\\n\\nThanks OP for doing the legwork.', 'How long did it take you guys to get the code? Registered yesterday, still no code.\\n\\nAlso, is the code still working for select laptops?', \"Thank you, OP.\\n\\nI signed up two gmail addresses and my company's email yesterday. Just got 1 15% coupon in my company email, but none in both gmail boxes. \\n10%offmonitor still worked for me and I was able to stack that with 15% coupon. \\nPlaced an order for a monitor with ridiculously high listed price. Fingers crossed it will be shipped out.\\nBlubbs wrote: \\nHottest Dell deal I've seen in a while. Thanks to @Neovingian for bringing this to my attention.\\n\\nSign up with an email address and get a 15% off coupon that stacks with other discounts like 10%OFFMONITOR. Also Rakuten has special 6% cash back right now which makes the deals fantastic when combined with Cyber Event discounts, doorcrashers etc.\\n\\nOne thing is it works only with work side stuff, I tried applying the discount to Alienware monitors and it doesn't work (only 10%OFFMONITOR does).\\n\\nI'm guessing these are some of the best prices since lockdown hit. I just ordered a Dell U3415W for $620 - 6%CB = $582. Fantastic price, plus it comes with a 3 year advance exchange warranty and premium panel guarantee. I already own one of these monitors and they are my favourite thing. I couldn't justify the extra cost for the USB-C version, though my wife had to nix me buying the U3818W for ~$950.\\n\\nFor the 15% coupon I did have to wait overnight but it came in less than 24 hours. So, not sure how long Rakuten is doing its cash back but should at least stay at 2%.\\n\\nThis should work for laptops etc too, but since I was shopping here are a couple deals.\\n\\nAnd, to top it all off, if you happen to have access to a Business Platinum Amex, you are going to lose your mind because they have a promotion where you get a $250 statement credit for spending $250 at Dell (that's right, 100% back haha). I'm assuming that works with all this but I don't have one as it is a high cost card. dell.ca/amex\\n\\nPic below of a few of the deals - bear in mind this is before cash back. The 15% off coupon works on multiple items in a single order. The 10%OFFMONITOR works only once per order.\\n\\n\\n\\n\\n\\n\\n\", \"MrBigNose wrote: \\nThank you, OP.\\n\\nI signed up two gmail addresses and my company's email yesterday. Just got 1 15% coupon in my company email, but none in both gmail boxes. \\n10%offmonitor still worked for me and I was able to stack that with 15% coupon. \\nPlaced an order for a monitor with ridiculously high listed price. Fingers crossed it will be shipped out.\\n\\nNice! Which one did you get?\", 'Stacking of coupons works now. Thanks OP was able to get your pricing on the u3419w', \"Still didn't get a 15% off code, how long does shipping take nowadays, still waiting a month or more?\\nAlmost want to go price match at BestBuy in person, but then wondering if the cash back situation could get better than 6%\", 'Neither did I. Bu some people did. I suppose it doesnt really come in batches.\\nBtw, I signed up yesterday!\\n\\nPs: Someone asked before, but I would like to second the question: when is expected shipping? Dell normally takes ages...', 'I entered my email yesterday got the code today.\\nShipping is relatively quick. I ordered another ultrawide for work and it arrived within 10 days.', 'ok, thanks! I dont think 10 days is really that quick, but ok.  \\nHope I get the code today', 'JoaoPauloB93228 wrote: \\nok, thanks! I dont think 10 days is really that quick, but ok.  \\nHope I get the code today\\n\\nSometimes Dell takes months, so for them 10 days is quick lol.', 'Is anyone still able to keep the 6% vs 2% CB when they go to the checkout process? It looks to swap from 6 to 2 when I get to checkout portion before submitting', \"signed up yesterday, haven't got any email so far\", 'I got the email today with 15% discount code', 'Any recommendations for WFH monitor during the day, and a general use monitor at night? General use as in basic photo editing, occasional gaming and browsing. Was thinking of U2720Q, never used a 4K monitor, is it a huge difference compared to U2719DC?', 'Not that hot. 15% coupon + 6% rakuten = roughly 21%. Normally you can get 10% unidays + 10% rakuten = roughly 20%, which is very similar. I also find prices more down-to-earth on the \"for home\" section of their site. Seems they like to gouge their business customers.', 'kk33tm wrote: \\nNot that hot. 15% coupon + 6% rakuten = roughly 21%. Normally you can get 10% unidays + 10% rakuten = roughly 20%, which is very similar. I also find prices more down-to-earth on the \"for home\" section of their site. Seems they like to gouge their business customers.\\n\\nyou forgot stacking 10%OFFMONITOR on top.\\n\\nDebbie Downer', 'Signed up yesterday, no luck, still waiting for the code. Does the code work for monitors from the Home section?', \"Blubbs wrote: \\nyou forgot stacking 10%OFFMONITOR on top.\\n\\nDebbie Downer\\n\\nI don't need a monitor, but I suppose that's good for people who do.\", 'Tried to get the SE2419HR or S2421HGF but got the error \"This is a valid coupon code, but there are no matching items currently in your cart.\" when adding the coupon.', 'Works on the U4320Q', \"What emails did you guys use? Free emails might not get the code. My Gmail didn't get any but my corporate email got one.\\n\\nshsiddiq wrote: Still didn't get a 15% off code, how long does shipping take nowadays, still waiting a month or more?\\nAlmost want to go price match at BestBuy in person, but then wondering if the cash back situation could get better than 6%\\nJoaoPauloB93228 wrote: Neither did I. Bu some people did. I suppose it doesnt really come in batches.\\nBtw, I signed up yesterday!\\n\\nPs: Someone asked before, but I would like to second the question: when is expected shipping? Dell normally takes ages...\\nforgetpwd wrote: signed up yesterday, haven't got any email so far\\nchtx900326 wrote: Signed up yesterday, no luck, still waiting for the code. Does the code work for monitors from the Home section?\", 'Hardkore wrote: \\nTried to get the SE2419HR or S2421HGF but got the error \"This is a valid coupon code, but there are no matching items currently in your cart.\" when adding the coupon.\\n\\nDid you choose \"Shop for Home\" or \"Shop for Business\"? Coupons are only valid on the \"Business\" side.', 'spyhero wrote: \\nDid you choose \"Shop for Home\" or \"Shop for Business\"? Coupons are only valid on the \"Business\" side.\\n\\nIt let me use the email coupon now but cant stack it with the 10%OFFMONITOR one', '|Naruto| wrote: \\nAny recommendations for WFH monitor during the day, and a general use monitor at night? General use as in basic photo editing, occasional gaming and browsing. Was thinking of U2720Q, never used a 4K monitor, is it a huge difference compared to U2719DC?\\n\\nAnything ultrasharp is amazing for business/photo editing and great build quality (I use one at my office that\\'s getting old - they never die they just get out of date and you give them away lol). Gaming is a completely different use case lol. It\\'s not a huge difference, but once you get used to it you can\\'t go back lol. If gaming is the primary look for a high end 1440p panel.\\n\\nI\\'ve had the 43\" 4k ultrasharp in my cart all morning but can\\'t justify spending ~$950ish on something I don\\'t really need.', 'Does it work for laptops? Is there any good deal on laptops from Dell that would qualify for discounts?', 'Hardkore wrote: \\nIt let me use the email coupon now but cant stack it with the 10%OFFMONITOR one\\n\\nYeah looks like it went down again. Maybe it goes down after sunset each day ', 'Anyone know if the perkopolis discounts are worth it? I cant remember my login info, but I was wondering if anyone knows it would be worth it to use, so I can buy an alienware gaming PC', 'Hello,\\n\\nPretty new to posting here so please excuse me if I do something wrong. I just received an email from Amex about the spend 10 and get 5 back, upto 10 times on my Business Platinum Card. It starts on June 24. \\n\\nThanks', 'nice deal. ymmv likely as we have two separate business platinums in the household, and neither have this offer.\\n\\nedit June 24: the offer has now appeared on both business platinums + one personal platinum. yay!', 'Got it on my Platinum. Love this card.']\n"
     ]
    }
   ],
   "source": [
    "# Print first 100 comments\n",
    "print([x for x in df_comments['comments'][0:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to clean these strings up will be through regular expressions. \n",
    "\n",
    "The following should be removed:  \n",
    "*  symbols\n",
    "* \\n new line characters\n",
    "* urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hottest Dell deal I've seen in a while. Thanks to @Neovingian for bringing this to my attention. Sign up with an email address and get a 15% off coupon that stacks with other discounts like 10%OFFMONITOR. Also Rakuten has special 6% cash back right now which makes the deals fantastic when combined with Cyber Event discounts, doorcrashers etc. One thing is it works only with work side stuff, I tried applying the discount to Alienware monitors and it doesn't work (only 10%OFFMONITOR does). I'm guessing these are some of the best prices since lockdown hit. I just ordered a Dell U3415W for $620 - 6%CB = $582. Fantastic price, plus it comes with a 3 year advance exchange warranty and premium panel guarantee. I already own one of these monitors and they are my favourite thing. I couldn't justify the extra cost for the USB-C version, though my wife had to nix me buying the U3818W for ~$950. For the 15% coupon I did have to wait overnight but it came in less than 24 hours. So, not sure how long Rakuten is doing its cash back but should at least stay at 2%. This should work for laptops etc too, but since I was shopping here are a couple deals. And, to top it all off, if you happen to have access to a Business Platinum Amex, you are going to lose your mind because they have a promotion where you get a $250 statement credit for spending $250 at Dell (that's right, 100% back haha). I'm assuming that works with all this but I don't have one as it is a high cost card. dell.ca/amex Pic below of a few of the deals - bear in mind this is before cash back. The 15% off coupon works on multiple items in a single order. The 10%OFFMONITOR works only once per order. \", 'Something to note is that it seems like the coupon code only works for Dell branded products.', \"Isn't Rakuten 2% today? I can't see the 6%.\", \"OP you should block out your coupon code if you don't want people stealing it\", \"MrMello wrote:  Isn't Rakuten 2% today? I can't see the 6%. You have to go through Dell Technologies, the business side. Dell Home is 2%.\", \"yesnomaybe wrote:  OP you should block out your coupon code if you don't want people stealing it I used it... but yeah maybe. Thanks, I did it.\", 'southbeach82 wrote:  Something to note is that it seems like the coupon code only works for Dell branded products. Does that mean the Alienware 34\" monitor is excluded?', 'Blubbs wrote:  I used it... but yeah maybe. Thanks, I did it. after you sighed up, did you get a confirmation email immediately? or only the email with coupon arrived after some time?', 'Does it work on Alienware ultrawide?', 'MrBigNose wrote:  after you sighed up, did you get a confirmation email immediately? or only the email with coupon arrived after some time? The post says they had to wait overnight.', \"Last round of Dell coupons like this, Dell didn't ship most of the orders - wouldn't get excited until you see a shipping confirmation.\", 'amex $250 credit term says must purchase on dell/amex site. in this case when we use rakuten cashback the amex credit will still be applied? Thanks.', \"I signed up for a 15% coupon on the weekend, still haven't received one.\", \"holy this is a good deal. i just bought the u2719dc for ~$100 more. just 2 weeks ago. i guess im within the return period, but i also used dell rewards to purchase so i don't even know how that works. also returning a monitor seems like it'd be a huge pain.\", \"I signed up last week as well.. didn't get the coupon\", 'Signed up. Waiting for my coupon', 'moofachuka wrote:  Does it work on Alienware ultrawide? seems to only work for monitors off the work site (select shop for business). there are no alienware monitors on that site.', \"Thanks OP, It's always nice to get a shout out and be appreciated   I assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!).  Dell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy!\", 'still waiting for coupon as well. Thanks Dell ', \"For people who haven't gotten an email, you may have signed up in the past or something else may have gone wrong, I'd try a few emails. Cheers\", \"MrBigNose wrote:  after you sighed up, did you get a confirmation email immediately? or only the email with coupon arrived after some time? Only the email when it arrived. It's just radio silence which is a bit annoying.\", \"Neovingian wrote:  Thanks OP, It's always nice to get a shout out and be appreciated   I assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!).  Dell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy! Thanks again. Most coupons on the Dell Home side don't stack anymore which is I think why this is hot. I guess it must be a more common thing on the work side.\", \"Blubbs wrote:  For people who haven't gotten an email, you may have signed up in the past or something else may have gone wrong, I'd try a few emails. Cheers Thanks for this post.. but I find these coupon hit and miss.. I have used an account that never signed up for any dell emails and still didn't get it. Alienware used to have the 10% off coupon as well but not anymore\", 'Has anybody confirmed this works for laptops? This seems to be the assumption, but it looks like people have only bought monitors with this so far.', 'MentalAnarchy wrote:  Has anybody confirmed this works for laptops? This seems to be the assumption, but it looks like people have only bought monitors with this so far. Works with this Doorbuster laptop, just tested. I think it will work anything that is not Alienware. And it works with multiple items in one order as stated. ', \"Tried a new work email, and no email for 15% off. I'll give it a day here and see whats the hold up.\", 'MentalAnarchy wrote:  Has anybody confirmed this works for laptops? This seems to be the assumption, but it looks like people have only bought monitors with this so far. Works with laptops if they are under work category. Sadly no ryzen 4000 laptops available... Btw code is good until August 15, maybe something decent will go on sale.', 'Can this deal somehow stack a 10% Unidays on top? That would sweet.', \"I miss the time when staples allowed to stack multiple coupons.  Get two 2TB 2.5 external disks for less than $60 (all in) each back in 2014 or 2015. Neovingian wrote:  Thanks OP, It's always nice to get a shout out and be appreciated   I assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!).  Dell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy! \", \"Didn't get an e-mail from Dell even though I just signed up. katwittfan wrote:  Can this deal somehow stack a 10% Unidays on top? That would sweet. Just try and see\", \"I know those Staples coupon days of 14' 15 were golden, I miss those days much simpler time.\", \"Neovingian wrote:  Thanks OP, It's always nice to get a shout out and be appreciated   I assumed the dell coupon stacking was common knowledge, the best double stacks, are when you can stack 2 X 15% +cb or super rare 20 %+15% + CB, even better when it works with Alienware products (hello mechanical keyboard & mouse combo, lol!).  Dell likes to pull the ole okie doke and change the prices of its products weekly and sometimes the next day, if it changes before your ship date you can usually still get it adjusted. Keep checking the price of your monitor and see if you can get any further savings by calling Dell, worst case you may have to cancel and do a new order but may lose out on promos by doing so. In my case, I did another order and the price of my items dropped by $75 a few days later. I was able to replicate the order but lost 5% due to a different code, but it was worth it. Enjoy! How do you get two 15% coupons? Using two emails to sign up?\", \"katwittfan wrote:  Can this deal somehow stack a 10% Unidays on top? That would sweet. I tried that and it doesn't work, not stacking and IIRC doesn't work at all for business side.\", 'Any suggestions for a good gaming monitor?', \"goodguy90 wrote:  Any suggestions for a good gaming monitor? The 32 inch curved 165hz QHD is very very good, but the gaming items are excluded from the stacking 15%. You can still get the 10%OFFMONITOR and it's on sale, so around $500.\", 'up vote for quality post with instructions and examples', 'U3818DW - I paid 1250 before taxes lol dang . It is been less than a week.  But if anyone is holding back on getting 38 inch - this is a very very good price!', 'viktor89 wrote:  U3818DW - I paid 1250 before taxes lol dang . It is been less than a week.  But if anyone is holding back on getting 38 inch - this is a very very good price! I wanted it so bad... But I don\\'t need the extra vertical space to be honest. I already have one U3415W and this will be my second for the standing desk behind me, plus spousal use. At $582 vs $970 I just couldn\\'t justify the 38\" lol. Even knowing it was the best price!', 'A lot of their IPS monitors are stated to be 5ms grey-to-grey and 8ms something else, what does this mean in terms of comparison to a 10 year old IPS monitor simply stated to be 5ms response time?', 'These monitors need to stack with another 50% off for me to consider them ', 'Originalmotto wrote:  These monitors need to stack with another 50% off for me to consider them  The coupon applies to any business monitor, there are plenty of cheaper ones on sale too. These are just examples. Check out the website!', 'does it work for dell desktops, like alienware r9?', 'My coupon took like 10 days to arrive. That was like few weeks ago. But i did sign up using another email few days later and both coupons came in around the same time. So i dont know if they only release these coupons every so often. I did chat with Dell agent, and was told that coupon normally takes 7 days to arrive.', 'Working for anyone else?', 'Also remember to get another 10% off if you have the American Express offer registered to your card: Get 10% of total purchase as a credit. Up to $1,000 in total credits. Dell Technologies EXPIRES 2020-09-30', \"Bgddss wrote:  How do you get two 15% coupons? Using two emails to sign up? To my knowledge 2 of those email sign ups won't work on the same order, codes are usually targeted like one 10-15% coupon and 1 promo like 10%-15% off monitors, keyboards or laptops + cb. In the past they used to have 2 promos running concurrently, like spend over $750 save save 10% which also stacks with other promos, SMB promos can be better than consumer, so it's good to check those out as well.\", \"Coupons get sent out in batches. When one of us gets it, all of us will get it.  Source: I've bought way too many Dell products and talked to their CS far too often.\", \"Well if anyone has one they're not using, I'm looking for coupon.\", 'setlist wrote:  Also remember to get another 10% off if you have the American Express offer registered to your card: Get 10% of total purchase as a credit. Up to $1,000 in total credits. Dell Technologies EXPIRES 2020-09-30 Is that for any American Express card?', \"Thanks OP, setting a more serious home office up so I was shopping for another monitor... Will see if I get a code. MutantBaseball wrote:  Is that for any American Express card? No, I think that's the platinum card. My gold card has the ol' fashion extra points promo where you get 1 point per dollar spent.\", 'Just tried the signup form. Got a 10% coupon within an hour of submission, not 15% like the OP though. Coupon expires in a month.', \"10%OFFMONITOR not working anymore for me. Home or Work site. I think it's a glitch as the coupon shows on their Coupons page:  \", 'When I try to use the 10%OFFMONITOR coupon I get \"This is a valid coupon code, but there are no matching items currently in your cart.\" I\\'m only trying to buy an Ultrasharp monitor, it work for anyone? I tried on both the business and consumer website.  edit: Student 10% code still works, but I haven\\'t received any email signup code.', 'Same thing, same error message. Tried with a U4919DW monitor.', 'I think they killed the deal', \"Neovingian wrote:  To my knowledge 2 of those email sign ups won't work on the same order, codes are usually targeted like one 10-15% coupon and 1 promo like 10%-15% off monitors, keyboards or laptops + cb. In the past they used to have 2 promos running concurrently, like spend over $750 save save 10% which also stacks with other promos, SMB promos can be better than consumer, so it's good to check those out as well. Well, its probably as rare as the price error thing with Lenovo.\", 'Rip deal is dead :/', 'guavaman wrote:  Rip deal is dead :/ Ugh really? I put so much work into this and wanted to spread the love!', 'Blubbs wrote:  Ugh really? I put so much work into this and wanted to spread the love! Yea the 10% code doesnt work but if you are a student you get 10% code, just waiting on the 15% code to pull the trigger', 'guavaman wrote:  Yea the 10% code doesnt work but if you are a student you get 10% code, just waiting on the 15% code to pull the trigger Where do you get the student code from?', 'MutantBaseball wrote:  Where do you get the student code from? Go to Unidays', 'MutantBaseball wrote:  Where do you get the student code from? Google dell student discount, sign up with unidays link with your student email!', 'guavaman wrote:  Yea the 10% code doesnt work but if you are a student you get 10% code, just waiting on the 15% code to pull the trigger can we stack unidays coupon with other coupons?', \"Tried 10%Off Monitor and won't work via business side and tried unidays and it said it's expired... But I just received it and signed up for unidays moments ago...\", \"I want two 4k 27-32in that'll run off single usbc, daisy chained to next monitor only for power/video.. Any idea what models will do this?\", 'Lettts gooo I got the 10% student discount + 10% with the coupon code OP mentioned. thanks OP!', '15% coupon didnt work on random laptops I added to cart(from the work website)', 'Anyone have a 15% coupon code they can share? I signed up for Dell a few days ago and havent received an email unfortunately', 'I would like 15% code too if anyone has a spare please.', 'Waiting for my code to arrive. Anybody know if it works on the new XPS 15 (9500)?', '15% Code showed up. The 10%OFFMONITOR code worked again after following the link from the email. I compared pricing to the same ultrasharp monitor that was bought pre-covid, and with both the 10% and 15% stacked coupons its still within +- 5% of the price I paid at the beginning of the year, so only a hot deal if you need a monitor now and can\\'t wait. Student code doesn\\'t stack with 10%OFFMONITOR FYI. edit: looked up my receipt for comparison with pre-covid with 10% off the price was $904+tax, now the price is $840+tax for the same 43\" 4k monitor, so a better deal than I thought. Good timing for those setting up a home office. Thanks OP for doing the legwork.', 'How long did it take you guys to get the code? Registered yesterday, still no code. Also, is the code still working for select laptops?', \"Thank you, OP. I signed up two gmail addresses and my company's email yesterday. Just got 1 15% coupon in my company email, but none in both gmail boxes.  10%offmonitor still worked for me and I was able to stack that with 15% coupon.  Placed an order for a monitor with ridiculously high listed price. Fingers crossed it will be shipped out. Blubbs wrote:  Hottest Dell deal I've seen in a while. Thanks to @Neovingian for bringing this to my attention. Sign up with an email address and get a 15% off coupon that stacks with other discounts like 10%OFFMONITOR. Also Rakuten has special 6% cash back right now which makes the deals fantastic when combined with Cyber Event discounts, doorcrashers etc. One thing is it works only with work side stuff, I tried applying the discount to Alienware monitors and it doesn't work (only 10%OFFMONITOR does). I'm guessing these are some of the best prices since lockdown hit. I just ordered a Dell U3415W for $620 - 6%CB = $582. Fantastic price, plus it comes with a 3 year advance exchange warranty and premium panel guarantee. I already own one of these monitors and they are my favourite thing. I couldn't justify the extra cost for the USB-C version, though my wife had to nix me buying the U3818W for ~$950. For the 15% coupon I did have to wait overnight but it came in less than 24 hours. So, not sure how long Rakuten is doing its cash back but should at least stay at 2%. This should work for laptops etc too, but since I was shopping here are a couple deals. And, to top it all off, if you happen to have access to a Business Platinum Amex, you are going to lose your mind because they have a promotion where you get a $250 statement credit for spending $250 at Dell (that's right, 100% back haha). I'm assuming that works with all this but I don't have one as it is a high cost card. dell.ca/amex Pic below of a few of the deals - bear in mind this is before cash back. The 15% off coupon works on multiple items in a single order. The 10%OFFMONITOR works only once per order. \", \"MrBigNose wrote:  Thank you, OP. I signed up two gmail addresses and my company's email yesterday. Just got 1 15% coupon in my company email, but none in both gmail boxes.  10%offmonitor still worked for me and I was able to stack that with 15% coupon.  Placed an order for a monitor with ridiculously high listed price. Fingers crossed it will be shipped out. Nice! Which one did you get?\", 'Stacking of coupons works now. Thanks OP was able to get your pricing on the u3419w', \"Still didn't get a 15% off code, how long does shipping take nowadays, still waiting a month or more? Almost want to go price match at BestBuy in person, but then wondering if the cash back situation could get better than 6%\", 'Neither did I. Bu some people did. I suppose it doesnt really come in batches. Btw, I signed up yesterday! Ps: Someone asked before, but I would like to second the question: when is expected shipping? Dell normally takes ages...', 'I entered my email yesterday got the code today. Shipping is relatively quick. I ordered another ultrawide for work and it arrived within 10 days.', 'ok, thanks! I dont think 10 days is really that quick, but ok.   Hope I get the code today', 'JoaoPauloB93228 wrote:  ok, thanks! I dont think 10 days is really that quick, but ok.   Hope I get the code today Sometimes Dell takes months, so for them 10 days is quick lol.', 'Is anyone still able to keep the 6% vs 2% CB when they go to the checkout process? It looks to swap from 6 to 2 when I get to checkout portion before submitting', \"signed up yesterday, haven't got any email so far\", 'I got the email today with 15% discount code', 'Any recommendations for WFH monitor during the day, and a general use monitor at night? General use as in basic photo editing, occasional gaming and browsing. Was thinking of U2720Q, never used a 4K monitor, is it a huge difference compared to U2719DC?', 'Not that hot. 15% coupon + 6% rakuten = roughly 21%. Normally you can get 10% unidays + 10% rakuten = roughly 20%, which is very similar. I also find prices more down-to-earth on the \"for home\" section of their site. Seems they like to gouge their business customers.', 'kk33tm wrote:  Not that hot. 15% coupon + 6% rakuten = roughly 21%. Normally you can get 10% unidays + 10% rakuten = roughly 20%, which is very similar. I also find prices more down-to-earth on the \"for home\" section of their site. Seems they like to gouge their business customers. you forgot stacking 10%OFFMONITOR on top. Debbie Downer', 'Signed up yesterday, no luck, still waiting for the code. Does the code work for monitors from the Home section?', \"Blubbs wrote:  you forgot stacking 10%OFFMONITOR on top. Debbie Downer I don't need a monitor, but I suppose that's good for people who do.\", 'Tried to get the SE2419HR or S2421HGF but got the error \"This is a valid coupon code, but there are no matching items currently in your cart.\" when adding the coupon.', 'Works on the U4320Q', \"What emails did you guys use? Free emails might not get the code. My Gmail didn't get any but my corporate email got one. shsiddiq wrote: Still didn't get a 15% off code, how long does shipping take nowadays, still waiting a month or more? Almost want to go price match at BestBuy in person, but then wondering if the cash back situation could get better than 6% JoaoPauloB93228 wrote: Neither did I. Bu some people did. I suppose it doesnt really come in batches. Btw, I signed up yesterday! Ps: Someone asked before, but I would like to second the question: when is expected shipping? Dell normally takes ages... forgetpwd wrote: signed up yesterday, haven't got any email so far chtx900326 wrote: Signed up yesterday, no luck, still waiting for the code. Does the code work for monitors from the Home section?\", 'Hardkore wrote:  Tried to get the SE2419HR or S2421HGF but got the error \"This is a valid coupon code, but there are no matching items currently in your cart.\" when adding the coupon. Did you choose \"Shop for Home\" or \"Shop for Business\"? Coupons are only valid on the \"Business\" side.', 'spyhero wrote:  Did you choose \"Shop for Home\" or \"Shop for Business\"? Coupons are only valid on the \"Business\" side. It let me use the email coupon now but cant stack it with the 10%OFFMONITOR one', '|Naruto| wrote:  Any recommendations for WFH monitor during the day, and a general use monitor at night? General use as in basic photo editing, occasional gaming and browsing. Was thinking of U2720Q, never used a 4K monitor, is it a huge difference compared to U2719DC? Anything ultrasharp is amazing for business/photo editing and great build quality (I use one at my office that\\'s getting old - they never die they just get out of date and you give them away lol). Gaming is a completely different use case lol. It\\'s not a huge difference, but once you get used to it you can\\'t go back lol. If gaming is the primary look for a high end 1440p panel. I\\'ve had the 43\" 4k ultrasharp in my cart all morning but can\\'t justify spending ~$950ish on something I don\\'t really need.', 'Does it work for laptops? Is there any good deal on laptops from Dell that would qualify for discounts?', 'Hardkore wrote:  It let me use the email coupon now but cant stack it with the 10%OFFMONITOR one Yeah looks like it went down again. Maybe it goes down after sunset each day ', 'Anyone know if the perkopolis discounts are worth it? I cant remember my login info, but I was wondering if anyone knows it would be worth it to use, so I can buy an alienware gaming PC', 'Hello, Pretty new to posting here so please excuse me if I do something wrong. I just received an email from Amex about the spend 10 and get 5 back, upto 10 times on my Business Platinum Card. It starts on June 24.  Thanks', 'nice deal. ymmv likely as we have two separate business platinums in the household, and neither have this offer. edit June 24: the offer has now appeared on both business platinums + one personal platinum. yay!', 'Got it on my Platinum. Love this card.']\n"
     ]
    }
   ],
   "source": [
    "#  symbols\n",
    "arrow_removed = [re.sub(\"+\",\"\", str(string)) for string in df_comments['comments']]\n",
    "# \\n characters\n",
    "newline_removed = [re.sub(\"\\\\n+\",\" \",string) for string in arrow_removed]\n",
    "# urls\n",
    "urls_removed = [re.sub(r\"\\bhttp.+\",\" \",string) for string in newline_removed]\n",
    "# Assign cleaned comments back\n",
    "df_comments['comments'] = pd.Series(urls_removed)\n",
    "\n",
    "# first 100 comments of cleaned table\n",
    "print([x for x in df_comments['comments'][0:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            comments\n",
      "0  Hottest Dell deal I've seen in a while. Thanks...\n",
      "1  Something to note is that it seems like the co...\n",
      "2        Isn't Rakuten 2% today? I can't see the 6%.\n",
      "3  OP you should block out your coupon code if yo...\n",
      "4  MrMello wrote:  Isn't Rakuten 2% today? I can'...\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned comment table as file\n",
    "df_comments.to_csv('rfd_comments.csv')\n",
    "\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation_date :  <class 'str'>\n",
      "expiry :  <class 'float'>\n",
      "last_reply :  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Copy of raw data set\n",
    "df = df_raw.copy()\n",
    "\n",
    "# List of tuples: (column name, column dtype)\n",
    "col_dtypes = [(col, type(x)) for x,col in zip(df.iloc[0], df.columns)]\n",
    "\n",
    "# Print tuple for columns containing dates\n",
    "for col in col_dtypes:\n",
    "    if col[0] in ['creation_date', 'last_reply', 'expiry']:\n",
    "        print(col[0], ': ', col[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the columns are formatted as datetime. To facilitate working with the dates, we will convert them to datetime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date columns to datetime dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(column: str) -> pd.Series:\n",
    "    \"\"\"Converts a column of format \"%b %d, %Y %I:%M %p\" from string to date-time\n",
    "    \n",
    "    Args:\n",
    "    date_column - name of column with dates encoded as strings\n",
    "    \n",
    "    Returns:\n",
    "    Column elements converted to datetime in a pandas.Series object\n",
    "    \"\"\"    \n",
    "    # Superfluous characters removed\n",
    "    column_clean = df[column].str.replace(\"st\",\"\").str.replace(\"nd\",\"\").str.replace(\"rd\",\"\").str.replace(\"th\",\"\").str.strip()\n",
    "    \n",
    "    # Check for correct length of cleaned column\n",
    "    column_len = len(column_clean)\n",
    "    print(\"Cleaned and original column are of equal lenght: \", column_len == len(df[column]), \"\\n\")\n",
    "    \n",
    "    # Convert from format \"%b %d, %Y %I:%M %p\" to datetime\n",
    "    date_column = []\n",
    "    try:\n",
    "        date_column = column_clean.apply(lambda x : datetime.datetime.strptime(str(x), \"%b %d, %Y %I:%M %p\"))\n",
    "    except: \n",
    "        print(\"\\\"%b %d, %Y %I:%M %p\\\" is incorrect format\")\n",
    "        pass\n",
    "    \n",
    "    # Convert from format \"%B %d, %Y\" to datetime\n",
    "    for date in df[column]:\n",
    "        if date is not np.nan:\n",
    "            try:\n",
    "                date_column.append(datetime.datetime.strptime(date, \"%B %d, %Y\"))\n",
    "            except: \n",
    "                print(\"\\\"%B %d, %Y\\\" is incorrect format for\", date)\n",
    "                break\n",
    "        else: \n",
    "            date_column.append(None)\n",
    "    \n",
    "    if len(date_column) != column_len:\n",
    "        print(\"\\n\", \"Incorrect column length!\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", \"Column has expected length!\\n\")\n",
    "    \n",
    "    return pd.Series(date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and original column are of equal lenght:  True \n",
      "\n",
      "\"%B %d, %Y\" is incorrect format for Jan 1st, 2020 8:32 pm\n",
      "\n",
      " Column has expected length!\n",
      "\n",
      "99    2020-07-08 17:53:00\n",
      "100   2020-07-11 20:38:00\n",
      "101   2020-06-12 12:22:00\n",
      "102   2020-07-13 11:35:00\n",
      "103   2020-04-23 13:01:00\n",
      "104   2020-07-13 11:58:00\n",
      "Name: creation_date, dtype: datetime64[ns] \n",
      "\n",
      "99       Jul 8th, 2020 5:53 pm\n",
      "100     Jul 11th, 2020 8:38 pm\n",
      "101    Jun 12th, 2020 12:22 pm\n",
      "102    Jul 13th, 2020 11:35 am\n",
      "103     Apr 23rd, 2020 1:01 pm\n",
      "104    Jul 13th, 2020 11:58 am\n",
      "Name: creation_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# creation_date column converted to datetime\n",
    "creation_date = to_datetime('creation_date')\n",
    "\n",
    "# Compare random slice of original and converted column\n",
    "print(creation_date.iloc[99:105], \"\\n\")\n",
    "print(df.loc[99:104, 'creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and original column are of equal lenght:  True \n",
      "\n",
      "\"%B %d, %Y\" is incorrect format for Jul 13th, 2020 3:25 pm\n",
      "\n",
      " Column has expected length!\n",
      "\n",
      "208   2020-07-12 20:04:00\n",
      "209   2020-07-12 19:53:00\n",
      "210   2020-07-12 19:47:00\n",
      "211   2020-07-12 19:43:00\n",
      "212   2020-07-12 19:28:00\n",
      "213   2020-07-12 19:28:00\n",
      "214   2020-07-12 19:10:00\n",
      "Name: last_reply, dtype: datetime64[ns] \n",
      "\n",
      "208    Jul 12th, 2020 8:04 pm\n",
      "209    Jul 12th, 2020 7:53 pm\n",
      "210    Jul 12th, 2020 7:47 pm\n",
      "211    Jul 12th, 2020 7:43 pm\n",
      "212    Jul 12th, 2020 7:28 pm\n",
      "213    Jul 12th, 2020 7:28 pm\n",
      "214    Jul 12th, 2020 7:10 pm\n",
      "Name: last_reply, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# last_reply column converted to datetime\n",
    "last_reply = to_datetime('last_reply')\n",
    "\n",
    "# Print original and new column for comparison\n",
    "print(last_reply.iloc[208:215], \"\\n\")\n",
    "print(df.loc[208:214, 'last_reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_datetime('expiry').iloc[50:57], \"\\n\")\n",
    "# print(df.loc[50:56, 'expiry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last logging:  \n",
    "Date:  **Augu 24, 2020**    \n",
    "date is not np.nan:  True  \n",
    "dtype of date:  <class 'str'>  \n",
    "\"%B %d, %Y\" is incorrect format for Augu 24, 2020  \n",
    "\n",
    "From the last logging, it becomes apparent that `st` has been removed from August due to the use of str.replace() in the to_datetime() function. This is not an issue for the columns with a `%b` format for months. The solution is to use the uncleaned data for the `expiry` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and original column are of equal lenght:  True \n",
      "\n",
      "\"%b %d, %Y %I:%M %p\" is incorrect format\n",
      "\n",
      " Column has expected length!\n",
      "\n",
      "50   2020-09-08\n",
      "51   2020-07-23\n",
      "52   2020-07-26\n",
      "53          NaT\n",
      "54          NaT\n",
      "55          NaT\n",
      "56          NaT\n",
      "dtype: datetime64[ns] \n",
      "\n",
      "50    September 8, 2020\n",
      "51        July 23, 2020\n",
      "52        July 26, 2020\n",
      "53                  NaN\n",
      "54                  NaN\n",
      "55                  NaN\n",
      "56                  NaN\n",
      "Name: expiry, dtype: object\n"
     ]
    }
   ],
   "source": [
    "expiry = to_datetime('expiry')\n",
    "print(expiry.iloc[50:57], \"\\n\")\n",
    "print(df.loc[50:56, 'expiry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The to_datetime() function appears to correctly convert each of the columns. The results can now be used in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trystee</td>\n",
       "      <td>2020-01-01 20:32:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-13 15:25:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>[Various Retailers] Gift Card Deals And Discou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365169</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yellowmp5</td>\n",
       "      <td>2020-07-13 13:29:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-13 15:23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>RYOBI 20% coupon barcode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2593</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>phoreoneone</td>\n",
       "      <td>2020-07-13 12:34:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-13 15:23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dell</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...</td>\n",
       "      <td>http://www.jdoqocy.com/click-749547-12105225?u...</td>\n",
       "      <td>2339</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Blackdove77</td>\n",
       "      <td>2020-07-06 12:49:00</td>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>2020-07-13 15:23:00</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>413</td>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Watchdogs 2 PC version free for Everyone</td>\n",
       "      <td>https://register.ubisoft.com/ubisoft-forward-r...</td>\n",
       "      <td>57748</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hkhorace</td>\n",
       "      <td>2020-07-06 09:56:00</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>2020-07-13 15:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>77</td>\n",
       "      <td>$200 off</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Quickjack 7000slx $200 off - $1300</td>\n",
       "      <td>https://www.costco.ca/quickjack-bl-7000slx-318...</td>\n",
       "      <td>15093</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       author       creation_date     expiry          last_reply  \\\n",
       "0           0      trystee 2020-01-01 20:32:00        NaT 2020-07-13 15:25:00   \n",
       "1           1    yellowmp5 2020-07-13 13:29:00        NaT 2020-07-13 15:23:00   \n",
       "2           2  phoreoneone 2020-07-13 12:34:00        NaT 2020-07-13 15:23:00   \n",
       "3           3  Blackdove77 2020-07-06 12:49:00 2020-07-19 2020-07-13 15:23:00   \n",
       "4           4     hkhorace 2020-07-06 09:56:00 2020-07-12 2020-07-13 15:22:00   \n",
       "\n",
       "           parent_category price  replies    saving      source  \\\n",
       "0                      NaN   NaN      672       NaN         NaN   \n",
       "1                      NaN   NaN       26       NaN  Home Depot   \n",
       "2                      NaN   NaN       23       NaN        Dell   \n",
       "3  Computers & Electronics   NaN      413      100%         NaN   \n",
       "4                      NaN  1300       77  $200 off      Costco   \n",
       "\n",
       "           thread_category                                              title  \\\n",
       "0                Groceries  [Various Retailers] Gift Card Deals And Discou...   \n",
       "1            Home & Garden                           RYOBI 20% coupon barcode   \n",
       "2  Computers & Electronics  Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...   \n",
       "3              Video Games           Watchdogs 2 PC version free for Everyone   \n",
       "4               Automotive                 Quickjack 7000slx $200 off - $1300   \n",
       "\n",
       "                                                 url   views  votes  \n",
       "0                                                NaN  365169    317  \n",
       "1                                                NaN    2593     28  \n",
       "2  http://www.jdoqocy.com/click-749547-12105225?u...    2339     15  \n",
       "3  https://register.ubisoft.com/ubisoft-forward-r...   57748    180  \n",
       "4  https://www.costco.ca/quickjack-bl-7000slx-318...   15093     26  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign datetime columns to DataFrame\n",
    "df.expiry = expiry\n",
    "df.last_reply = last_reply\n",
    "df.creation_date = creation_date\n",
    "\n",
    "# Verify dates\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data: `source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Various Retailers] Gift Card Deals And Discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>RYOBI 20% coupon barcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Dell</td>\n",
       "      <td>Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Watchdogs 2 PC version free for Everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Quickjack 7000slx $200 off - $1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                              title\n",
       "0         NaN  [Various Retailers] Gift Card Deals And Discou...\n",
       "1  Home Depot                           RYOBI 20% coupon barcode\n",
       "2        Dell  Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...\n",
       "3         NaN           Watchdogs 2 PC version free for Everyone\n",
       "4      Costco                 Quickjack 7000slx $200 off - $1300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['source', 'title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible, that users simply forgot to include the source of the deal. We will check if missing sources are mentioned in the corresponding title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sources:  163\n",
      "385 missing values in source column\n"
     ]
    }
   ],
   "source": [
    "# Set of entries in 'source' column\n",
    "retailer_set = set(df['source'].dropna())\n",
    "print(\"Number of unique sources: \", len(retailer_set))\n",
    "print(df.source.isnull().sum(), \"missing values in source column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large number of unique sources is promising! \n",
    "\n",
    "Next we will use the set previously created to itterate through the titles an check if any of the unique source names are present. If a source name from the set is found in `title` and no value is found in the corresponding `source` column, then the index as well as the source name are saved in the `replace` dictinoary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements found in 'title': 53\n"
     ]
    }
   ],
   "source": [
    "replace_dict = {} # key: index; value: retailer name to replace missing source value at index\n",
    "# found_in_title = {} # keys: retailer name; values: number of times name was found in a post-title\n",
    "# source_in_title = [] # names of retailers mentioned in at least one title\n",
    "# replaceable_value_count = 0\n",
    "\n",
    "# Iterate throuh set of unique values from source source column\n",
    "for retailer in retailer_set:\n",
    "    \"\"\"Fill replace dictioray with indecies and source names. Entries are made\n",
    "    when a source name is found in the title column while the corresponding source entry\n",
    "    is empty.\"\"\"\n",
    "    \n",
    "    # Iterate through 'source' and 'title' columns row-by-row\n",
    "    # Generate boolean array: True if unique source name (retailer) found in \"title\" and \"source\" is np.nan\n",
    "    source_missing_and_in_title = np.array([retailer in title \n",
    "                                     if source is np.nan else False\n",
    "                                     for title,source in zip(df.title, df.source)])\n",
    "    \n",
    "    # Indecies for which source_missing_and_in_title is True\n",
    "    replacement_indicies = np.where(source_missing_and_in_title == True)[0]\n",
    "    # Fill \"replace\" dictionary\n",
    "    for index in replacement_indicies:\n",
    "        if index not in replace_dict.keys():\n",
    "            replace_dict[index] = retailer\n",
    "\n",
    "print(\"Replacements found in 'title':\", len(replace_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53 missing sources can be replaced by information found in the title. We will use the indecies and values stored in `replace_dict` to replace the appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing source values before replacement: 385\n",
      "Missing source values after replacement: 332\n",
      "53 missing source records have been replaced!\n"
     ]
    }
   ],
   "source": [
    "source_list = list(df.source)\n",
    "missing_start = sum([x is np.nan for x in source_list])\n",
    "print(\"Missing source values before replacement:\", missing_start)\n",
    "\n",
    "for replace_source in replace.items():\n",
    "    source_list[replace_source[0]] = replace_source[1]\n",
    "\n",
    "missing_end = sum([x is np.nan for x in source_list])\n",
    "print(\"Missing source values after replacement:\", missing_end)\n",
    "replaced_count = missing_start-missing_end\n",
    "print(replaced_count, \"missing source records have been replaced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 53 identified `source` records have been replaced with apporiate names of retailers found in the corrseponding `title` column. The new `source` column can now replace the old one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values as expected: True\n"
     ]
    }
   ],
   "source": [
    "df.source = source_list\n",
    "print(\"Number of missing values as expected:\", (df.source.isnull().sum() == missing_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further substitutions for missing `source` values may be found in the `url` column. The objective is to extract company names from the urls and use them to further replace missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 missing source values have corresponding urls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                   NaN\n",
       "3     https://register.ubisoft.com/ubisoft-forward-r...\n",
       "17    http://go.redirectingat.com?id=2927x594702&amp...\n",
       "19    http://click.linksynergy.com/deeplink?id=CAqD7...\n",
       "22        https://www.costco.ca/.product.100476333.html\n",
       "Name: url, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'url' entries of rows with missing source values\n",
    "url_replacement = df[df.source.isnull()].url\n",
    "print(url_replacement.notnull().sum(), \"missing source values have corresponding urls\")\n",
    "url_replacement.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls need to be split and cleaned to extract the name of the organisation. The final cleaned values and their corresponding indicies in the DataFrame will be stores in the `clean_urls` disctionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: nan, 3: ['register', 'ubisoft', 'com'], 17: ['go', 'redirectingat', 'com'], 19: ['click', 'linksynergy', 'com'], 22: ['costco', 'ca'], 23: nan, 27: ['activebaby', 'ca'], 41: ['classic', 'avantlink', 'com'], 45: ['awin1', 'com'], 46: nan, 49: ['awin1', 'com'], 54: ['ebox', 'ca'], 59: ['awin1', 'com'], 62: ['store', 'insta360', 'com'], 63: ['amazon', 'ca'], 65: ['costco', 'ca'], 71: ['cityfone', 'net'], 77: ['alltrails', 'com'], 80: ['costco', 'ca'], 81: ['click', 'linksynergy', 'com'], 84: nan, 87: ['econsumer', 'equifax', 'ca'], 91: nan, 94: nan, 103: nan, 113: ['amazon', 'ca'], 122: nan, 141: ['svpsports', 'ca'], 147: ['amazon', 'ca'], 151: nan, 152: ['bcfasteners', 'com'], 156: ['costco', 'ca'], 157: ['staplescanada', '4u8mqw', 'net'], 165: nan, 166: ['register', 'ubisoft', 'com'], 168: ['apps', 'apple', 'com'], 169: ['amazon', 'ca'], 171: ['amazon', 'ca'], 172: ['mailchi', 'mp'], 178: ['amazon', 'ca'], 180: ['bccamera', 'com'], 183: ['amazon', 'ca'], 187: ['tkqlhce', 'com'], 191: ['amazon', 'ca'], 193: nan, 196: nan, 199: ['amazon', 'ca'], 205: ['pi-co', 'ca'], 207: ['awin1', 'com'], 211: ['cdkeys', 'com'], 215: ['go', 'redirectingat', 'com'], 230: ['kits', 'com'], 235: ['the-home-depot-ca', 'pxf', 'io'], 237: ['signup', 'easynews', 'com'], 238: ['tsc', 'ca'], 244: nan, 247: ['amazon', 'ca'], 248: ['amazon', 'ca'], 250: ['fadedsoul', 'com'], 251: ['apfco', 'com'], 253: ['the-home-depot-ca', 'pxf', 'io'], 254: nan, 255: ['canadiantire', 'ca'], 257: ['levis-canada', 'sjv', 'io'], 258: ['canadiantire', 'ca'], 260: ['fujifilmprintlife', 'ca'], 268: ['mysimplemarketplace', 'com'], 272: ['tanguay', 'ca'], 275: ['amazon', 'ca'], 277: ['scotiabank', 'com'], 280: ['amazon', 'ca'], 285: nan, 288: ['cdkeys', 'com'], 298: nan, 299: nan, 301: ['awin1', 'com'], 319: ['ca', 'miele', 'ca'], 323: nan, 328: nan, 329: nan, 330: ['cdkeys', 'com'], 332: ['kqzyfj', 'com'], 334: nan, 338: ['drop', 'com'], 339: ['svpsports', 'ca'], 344: ['rakuten', 'ca'], 345: ['outterlimits', 'com'], 350: ['go', 'redirectingat', 'com'], 351: ['playolg', 'ca'], 353: ['click', 'linksynergy', 'com'], 359: ['click', 'linksynergy', 'com'], 366: ['amazon', 'ca'], 367: ['getmyoffers', 'ca'], 368: ['canex', 'ca'], 371: nan, 376: ['mobvoi', 'com'], 381: ['costco', 'ca'], 385: ['kqzyfj', 'com'], 388: ['store', 'playstation', 'com'], 399: ['click', 'linksynergy', 'com'], 400: nan, 407: ['elac', 'com'], 411: ['facebook', 'com'], 412: nan, 416: nan, 417: nan, 418: nan, 419: ['wellwise', 'ca'], 436: nan, 438: ['amazon', 'ca'], 440: ['ampli', 'ca'], 443: ['burton', 'com'], 451: ['tkqlhce', 'com'], 453: ['cdkeys', 'com'], 458: ['store', 'playstation', 'com'], 463: ['altimatel', 'com'], 464: ['fastchargedocks', 'com'], 465: nan, 471: ['shop', 'lindt', 'ca'], 472: ['formula1', 'com'], 473: ['gotransit', 'com'], 475: ['carrytel', 'ca'], 476: nan, 480: ['the-home-depot-ca', 'pxf', 'io'], 482: nan, 493: ['try', 'fender', 'com'], 494: ['ionos', 'ca'], 498: ['simplii', 'com'], 499: ['amazon', 'ca'], 502: nan, 504: ['costco', 'ca'], 505: ['go', 'redirectingat', 'com'], 508: ['costco', 'ca'], 509: ['wellwise', 'ca'], 512: nan, 514: ['ninjakitchen', 'com'], 515: nan, 526: ['amazon', 'ca'], 527: nan, 555: ['bestbuyca', 'o93x', 'net'], 560: ['anrdoezrs', 'net'], 562: ['sherwin-williams', 'ca'], 576: ['aeroplan-bg-sso', 'points', 'com'], 578: ['bit', 'ly'], 593: ['anrdoezrs', 'net'], 606: ['click', 'linksynergy', 'com'], 608: ['googleadservices', 'com'], 610: ['kqzyfj', 'com'], 618: ['metopera', 'org'], 620: ['billing', 'frugalusenet', 'com'], 622: ['amazon', 'ca'], 624: ['awin1', 'com'], 628: ['therecroom', 'com'], 630: ['decathalon-canada', 'mkr3', 'net'], 639: ['oxio', 'ca'], 643: ['bestbuyca', 'o93x', 'net'], 645: ['amazon', 'ca'], 649: ['speakout7eleven', 'ca'], 660: ['uberats', 'ca'], 664: ['kelloggsgrocerycash', 'ca'], 666: ['click', 'linksynergy', 'com'], 670: ['edifier', 'com'], 674: nan, 675: ['ca', 'manscaped', 'com'], 678: ['click', 'linksynergy', 'com'], 680: nan, 681: ['click', 'linksynergy', 'com'], 685: ['awin1', 'com'], 689: nan, 697: ['play', 'google', 'com'], 705: ['everyonerides', 'org'], 706: ['detourcoffee', 'com'], 714: ['oculus', 'com'], 716: ['americanexpress', 'com'], 723: nan, 727: ['Www', 'nbc', 'ca'], 728: ['cashback', 'highinterestsavings', 'ca'], 735: nan, 740: ['lenovo', 'evyy', 'net'], 747: ['go', 'redirectingat', 'com'], 751: ['rallyforrestaurants', 'ca'], 754: ['oculus', 'com'], 756: ['idrinkcoffee', 'com'], 763: nan, 765: ['cosmeticscompanystore', 'com'], 766: ['napoleon', 'com'], 770: ['brampton', 'ca'], 771: ['treadmillfactory', 'ca'], 773: ['go', 'redirectingat', 'com'], 775: nan, 777: ['awin1', 'com'], 779: ['thecubenet', 'com'], 786: ['dispatchcoffee', 'ca'], 788: ['cwbeggs', 'com'], 789: ['click', 'linksynergy', 'com'], 796: ['lostcraft', 'ca'], 802: ['try', 'tidal', 'com'], 803: ['canada', 'bissell', 'com'], 807: nan, 809: ['go', 'redirectingat', 'com'], 817: ['go', 'redirectingat', 'com'], 823: ['store', 'playstation', 'com'], 824: ['store', 'playstation', 'com'], 825: nan, 827: ['radpowerbikes', 'ca'], 830: ['fitnessavenue', 'ca'], 833: nan, 837: ['enbridgesmartsavings', 'com'], 839: nan, 853: ['us', 'shop', 'battle', 'net'], 857: ['play', 'google', 'com'], 865: ['accounts', 'usenetserver', 'com'], 867: ['edifier', 'com'], 876: ['poppacorn', 'ca'], 878: ['amazon', 'com'], 879: ['amazon', 'ca'], 880: ['sportium', 'ca'], 882: ['mcgillpersonalfinance', 'com'], 884: ['uhaul', 'com'], 885: ['ecscoffee', 'com'], 889: ['teasante', 'com'], 891: ['thermoworks', 'com'], 895: nan, 896: nan, 900: ['bonusboom', 'airmiles', 'ca'], 905: ['outterlimits', 'com'], 908: ['completeequipment', 'ca'], 911: ['walmartphotocentre', 'ca'], 916: ['the-home-depot-ca', 'pxf', 'io'], 917: ['avantlink', 'ca'], 922: ['bestbuyca', 'o93x', 'net'], 928: nan, 932: ['ugo', 'ca'], 935: ['go', 'redirectingat', 'com'], 943: nan, 947: ['harryrosen', 'com'], 952: nan, 955: ['luckymobile', 'ca'], 962: ['aeroplan', 'com'], 964: nan, 974: ['waterloobrewing', 'com'], 980: ['gibbyselectronicsupermarket', 'ca'], 990: nan, 993: ['twitter', 'com'], 995: ['awin1', 'com'], 1000: ['deezer', 'com'], 1001: ['click', 'linksynergy', 'com'], 1006: ['software', 'pcworld', 'com'], 1013: ['dicksonbbq', 'com'], 1015: nan, 1019: ['amazon', 'ca'], 1023: ['fitnessavenue', 'ca'], 1032: ['go', 'redirectingat', 'com'], 1049: ['edifier', 'com'], 1051: ['amazon', 'ca'], 1058: ['stacksocial', 'com'], 1073: ['awin1', 'com'], 1089: ['apps', 'apple', 'com'], 1093: ['razer', 'com'], 1102: nan, 1108: ['store', 'playstation', 'com'], 1118: ['tkqlhce', 'com'], 1128: ['play', 'google', 'com'], 1144: ['store', 'playstation', 'com'], 1146: ['brownsshoes', 'com'], 1149: ['amazon', 'ca'], 1152: ['lecreuset', 'ca'], 1163: ['news', 'xbox', 'com'], 1168: nan, 1169: ['store', 'steampowered', 'com'], 1181: ['google', 'com'], 1183: ['storefront', 'points', 'com'], 1184: nan, 1191: nan, 1198: ['awin1', 'com'], 1216: ['shoprbc', 'com'], 1218: ['go', 'redirectingat', 'com'], 1223: ['cpapoutlet', 'ca'], 1225: ['circlekgames', 'ca'], 1228: ['humblebundle', 'com'], 1243: nan, 1248: ['store', 'playstation', 'com'], 1253: ['scholastic', 'ca'], 1259: ['go', 'redirectingat', 'com'], 1261: nan, 1266: nan, 1270: nan, 1277: ['warriorsandwonders', 'com'], 1278: ['nintendo', 'com'], 1280: ['ituonline', 'com'], 1282: ['facebook', 'com'], 1298: ['getsuperwrap', 'com'], 1307: ['play', 'google', 'com'], 1311: ['beanwise', 'ca'], 1321: nan, 1331: ['docs', 'google', 'com'], 1336: nan, 1347: ['ca', 'norlanglass', 'com'], 1364: ['gog', 'com'], 1370: nan, 1377: nan, 1386: ['usenetprime', 'com'], 1389: ['store', 'asuswebstorage', 'com'], 1392: ['pntrac', 'com'], 1393: ['irobot', 'ca'], 1399: ['instagram', 'com'], 1400: ['doordashca', 'launchgiftcards', 'com'], 1414: ['store', 'playstation', 'com'], 1415: ['hellodemello', 'com'], 1423: ['awin1', 'com'], 1424: ['detourcoffee', 'com'], 1439: nan, 1441: ['weber', 'com'], 1444: ['humblebundle', 'com'], 1448: ['atlas-machinery', 'com'], 1449: ['kits', 'com'], 1456: ['newsgroupdirect', 'com'], 1457: ['100dollar', 'darkhorseapp', 'net'], 1458: ['amazon', 'ca'], 1459: ['taappliance', 'com'], 1461: ['consumer', 'huawei', 'com'], 1465: nan, 1468: nan, 1471: ['viofo', 'com'], 1472: ['cdkeys', 'com'], 1473: ['play', 'google', 'com'], 1475: ['shecodes', 'io'], 1483: ['go', 'redirectingat', 'com'], 1485: ['community', 'stadia', 'com'], 1486: ['pc-canada', 'com'], 1494: nan, 1498: ['go', 'redirectingat', 'com']}\n"
     ]
    }
   ],
   "source": [
    "clean_urls = {} # key: index in df, value: cleaned url\n",
    "indicies = url_replacement.index\n",
    "\n",
    "for url in zip(indicies, url_replacement):\n",
    "    index = url[0]\n",
    "    replacement_url = url[1]\n",
    "    \n",
    "    # Clean if url value not missing\n",
    "    if replacement_url is not np.nan:\n",
    "        url_root = replacement_url.split(\"//\")[1].split(\"/\")[0].split(\"?\")[0].replace(\"www.\", \"\")\n",
    "        removed_domain = url_root.split(\".\")\n",
    "        clean_urls[index] = removed_domain\n",
    "    else:\n",
    "        clean_urls[index] = np.nan\n",
    "        \n",
    "print(clean_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify the company names from the url splits observed in the print above.\n",
    "The patterns shown in the table will facilitate this process. This is an oversimplification and will lead to some false extractions but the number of errors should be minimal.\n",
    "\n",
    "|Condition| Pattern|\n",
    "|---|---|\n",
    "|Lists length 2| company name is at index 0|\n",
    "|Lists length 3 and domain com, ca, or net| name is at index 1|\n",
    "|List length 3 and domain io| name is at index 0| \n",
    "|List length 4| no identifiable name|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_url_final = clean_urls.copy()\n",
    "\n",
    "for item in clean_url_final.items():\n",
    "    index = item[0]\n",
    "    url_split = item[1]\n",
    "    try:\n",
    "        if len(url_split) == 2:\n",
    "             # name at index 0\n",
    "            clean_url_final[index] = url_split[0].title()\n",
    "        \n",
    "        elif ((len(url_split) == 3) \n",
    "                        and ((url_split[-1] == \"com\") \n",
    "                                 or (url_split[-1] == \"ca\") \n",
    "                                 or (url_split[-1] == \"ca\"))):\n",
    "            # name at index 1\n",
    "            clean_url_final[index] = url_split[1].title()\n",
    "        \n",
    "        elif ((len(url_split) == 3) \n",
    "                        and (url_split[-1] == \"io\")):\n",
    "             # name at index 0\n",
    "            clean_url_final[index] = url_split[0].title()\n",
    "        else: \n",
    "              clean_url_final[index] = np.nan\n",
    "    except: value = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing source values remaining:  78\n"
     ]
    }
   ],
   "source": [
    "# Add url-derived company names to DataFrame\n",
    "df.loc[list(clean_url_final.keys()),'source'] = list(clean_url_final.values())\n",
    "print(\"Missing source values remaining: \", df.source.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data: `price`\n",
    "\n",
    "Users may have forgoten to tag prices associated with the deals they posted. We will verify if their are any `$` signs in the title for those rows that have missing price values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 missing values in 'price' column\n",
      "246 missing prices have '$' signs in the title\n"
     ]
    }
   ],
   "source": [
    "missing_prices_df = df[df.price.isnull()]\n",
    "price_in_title = [\"$\" in title for title in missing_prices_df.title]\n",
    "print(df.price.isnull().sum(), \"missing values in 'price' column\")\n",
    "print(sum(price_in_title), \"missing prices have '$' signs in the title\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, RTX 2060 $1,549 ($1425/$1285 Rakuten/Rakuten+Uni)',\n",
       " 'iTunes gift cards - $21.49 for $25 | $84.99 for $100 | $167.99 for $200 (Members only)',\n",
       " 'Lenovo Duet Chromebook 2-in-1: $399 - preorder',\n",
       " '[Uplay/Steam] Far Cry 5 (11.99$) - Far Cry 4 (7.99$) - Far Cry 3 (3.90$)',\n",
       " 'Ebay - Take $5 off minimum purchase of $5.01 YMMV',\n",
       " 'Samsung Galaxy S10 Lite (6.7\", 128GB, 8GB, SD855, 4500mah) US$431 / CAD$585 (and Note 10 Lite CAD$572)',\n",
       " 'Amex Business Platinum: $250 credit for $250 spend at dell.ca',\n",
       " 'Shopping.ca Gift Card by Ivanho Cambridge - Spend at least $100 and Earn $20 credit. Up to 3 times - YMMV',\n",
       " 'Samsung CRG9 - 49inch 1440p 120hrz monitor $1299 Best Buy',\n",
       " 'Book Outlet - Many Low Prices on Books (Free Shipping Over $45 & 16% Off)']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 10 title to evaluate if the missing price could be substituted\n",
    "replacement_titles = missing_prices_df[price_in_title].title\n",
    "[title for title in replacement_titles][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible replacements: 246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2                           [$1,549, $1425, $1285]\n",
       "12      [$21.49, $25, $84.99, $100, $167.99, $200]\n",
       "16                                          [$399]\n",
       "17                          [11.99$, 7.99$, 3.90$]\n",
       "23                                         [$5.01]\n",
       "                           ...                    \n",
       "1465                                         [$49]\n",
       "1485                                        [$139]\n",
       "1488                                    [$60, $20]\n",
       "1493                                    [$10, $14]\n",
       "1498                            [$169.95, $199.95]\n",
       "Name: title, Length: 246, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = \"[$]+[.,]*\\d+[.,]*\\d+\"\\\n",
    "        \"|[.,]*\\d+[.,]*\\d+[$]+\"\\\n",
    "        \"|[a-zA-Z]+[$]+[.,]*\\d+[.,]*\\d+\"\n",
    "price_replacements = replacement_titles.str.findall(regex)\n",
    "print(\"Number of possible replacements:\", len(price_replacements))\n",
    "price_replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume the first element in each list is most relevant and use it to replace missing price values. Some inaccuracies are likely to occure but the estimates should be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 replacements found.\n"
     ]
    }
   ],
   "source": [
    "replacement_dict = {} # key: index; value: price to replace missing value at index\n",
    "\n",
    "# Iterate through price lists found in price_replacements and corresonding indecies in DataFrame\n",
    "for replacement in zip(price_replacements, list(price_replacements.index)):\n",
    "    price_list = replacement[0]\n",
    "    index = replacement[1]\n",
    "    if price_list != []:\n",
    "        price = price_list[0]\n",
    "        price_clean = (re.search(r\"\\d+[.,]*\\d+\", price)).group().replace(\",\",\"\")\n",
    "        replacement_dict[index] = price_clean\n",
    "        \n",
    "print(len(replacement_dict), \"replacements found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 252\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values\n",
    "df.loc[list(replacement_dict.keys()), 'price'] = list(replacement_dict.values())\n",
    "print(\"Remaining missing values:\", df.price.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

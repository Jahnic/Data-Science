{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](rfd_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RedFlagDeals](https://forums.redflagdeals.com/hot-deals-f9/) is a forum where users can post sales or deals that they have come accross. The first part of this project is focused on scraping relevant information from the \"All Hot Deals\" section that includes all product categories. In the second and third part I will clean and visualize the data to extract and summarize useful information. \n",
    "\n",
    "Two tables will be scraped. The main table with a description of all columns is shown below. The scond table will store all comments that were made on a post. To insure, that comments can be linked back to the original post, we will also store the corresponding post titles in a second column.\n",
    "\n",
    "|Column name|Description|\n",
    "|---|---|\n",
    "|'title'| Title of post|\n",
    "|'votes'| Sum of up-, and down-votes|\n",
    "|'source'| Name of retailer offering the sale|\n",
    "|'creation_date'| Date of initial post|\n",
    "|'last_reply'| Date of most recent reply|\n",
    "|'author'| User name of post author|\n",
    "|'replies'| Number of replies|\n",
    "|'views'| Number of views|\n",
    "|'price'| Price of product on sale|\n",
    "|'saving'| Associated saving|\n",
    "|'expiry'| Expiry date of sale|\n",
    "|'url'| Link to deal|\n",
    "\n",
    "The comments can be used for natural language processing and analyzing sentiment more robustly. Tacking only upvotes and the number of replies into consideration may not be sufficient to accurately reflect the value of a deal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import requests # Scraping\n",
    "from bs4 import BeautifulSoup # HTML parsing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Retrieving data from the \"All Hot Deals\" sub-forum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"All Hot Deals\" section is an overview page of recent posts made on the forum. Information about posts such as title and number of upvotes are summarized and listed. The summaries in the \"All Hot Deals\" section as well as the comments on individual posts are organized into several pages.\n",
    "\n",
    "To scrap as much information as possible, we need to iterate through the different pages on \"All Hot Deals\". Further, we need to access each indevidual post to obtain additional information not found in the summary. For each post, we will also scrape the comments made by users. For this will need to iterate through the available pages on each post.\n",
    "\n",
    "\n",
    "URL format for different pages: `root-url/page#/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables used to iterate over web-pages.\n",
    "current_page = \"\" # page number; used to format root URL\n",
    "total_pages = 1 # endpoint for iteration; set through get_posts()\n",
    "root_url = \"https://forums.redflagdeals.com/hot-deals-f9/\" # base url for \"All Hot Deals\" sub-forum\n",
    "\n",
    "# URL base to generate links to specific posts\n",
    "base_url = \"https://forums.redflagdeals.com\"\n",
    "\n",
    "# Dataframe to store scraped data\n",
    "main_table = pd.DataFrame(columns=\n",
    "    ['title',\n",
    "    'votes',\n",
    "    'source',\n",
    "    'creation_date',\n",
    "    'last_reply',\n",
    "    'author',\n",
    "    'replies',\n",
    "    'views',\n",
    "    'price',\n",
    "    'saving',\n",
    "    'expiry',\n",
    "    'url'])\n",
    "\n",
    "# Dataframe to to store post comments\n",
    "comment_table = pd.DataFrame(columns=\n",
    "                       ['title',\n",
    "                       'comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(page: str) -> list:\n",
    "    \"\"\"\n",
    "    Returns list of parsed object containing all post elements from\n",
    "    the current 'page' and sets gloabl variable 'total_pages'\n",
    "    \n",
    "    Args:\n",
    "    page - url string of current page\n",
    "    \n",
    "    Returns:\n",
    "    topics - all parsed elements of class 'row topic'\n",
    "    total_pages - sets global variable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initalize list of posts on page class=\"row topic\"\n",
    "    posts = []\n",
    "    \n",
    "    # Get entire page content\n",
    "    response = requests.get(page)\n",
    "    content = response.content\n",
    "    \n",
    "    # Find total number of pages and set global variable accordingly\n",
    "    # Format of text: \" {current page #} of {total page #} \"\n",
    "    # Need to strip white space and extract total page #\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    pages = parser.select(\".pagination_menu_trigger\")[0].text.strip().split(\"of \")[1]\n",
    "    global total_pages\n",
    "    total_pages = int(pages)\n",
    "    \n",
    "    # Find and return topics\n",
    "    topics = parser.find_all(\"li\", class_=\"row topic\")\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_info(post: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts and returns additional information from a RedFlagDeals post:\n",
    "    url-link to the deal, the price of the product, the discount saving, \n",
    "    the expiry date, the parent/thread categories of the product and a list of\n",
    "    comments found on all pages. \n",
    "    \n",
    "    Args:\n",
    "    post - url string linking to a specific post\n",
    "    \n",
    "    Returns:\n",
    "    additional_info - dictionary containing additional information about the post.\n",
    "    Stores NaN values in \"additional_info\" for objects that are not found.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Additional information found in post\n",
    "    additional_info = {}\n",
    "    # Reviews found on all pages of post\n",
    "    \n",
    "    # Get content of post\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Thread-header with information on parent and thread category\n",
    "    try: # parent category\n",
    "        parent_category = parser.select(\".thread_parent_category\")[0].text\n",
    "        additional_info['Parent:'] = parent_category\n",
    "    except: additional_info['Parent:'] = np.nan # NaN if category not found\n",
    "    try: # thread category\n",
    "        thread_category = parser.select(\".thread_category\")[0].text\n",
    "        additional_info['Thread:'] = thread_category\n",
    "    except: additional_info['Thread:'] = np.nan # NaN if category not found\n",
    "    \n",
    "    \n",
    "    # Offer-summary field: may contain deal link, price, saving, and retailer\n",
    "    summary = parser.select(\".post_offer_fields\") # format example: \"Price:\\n$200\\nSaving:\\n70%\"\n",
    "    try:\n",
    "        summary_list = summary[0].text.split(\"\\n\") \n",
    "    except: summary_list = []\n",
    "        \n",
    "    # Example format of summary_list: [\"\", \"Price:\", \"200$\", \"Saving:\", 50%, \"Expiry:\", \"July 23, 2020\"]\n",
    "    for i in range(1, (len(summary_list) -1), 2): # index 0 is empty string\n",
    "        current_element = summary_list[i] # content of current list element\n",
    "        next_element = summary_list[i+1] # next list element containing the information\n",
    "        \n",
    "        # Price, saving, and expiry date information contained in the next list element will be saved\n",
    "        if current_element.startswith(\"Price\") or current_element.startswith(\"Saving\") or current_element.startswith(\"Expiry\"):\n",
    "            additional_info[current_element]  = next_element # next elements corrsponds to content\n",
    "            \n",
    "    # URL to link. Full link not available through .text\n",
    "    try: \n",
    "        url = str(summary[0]).split('href=\"')[1].split('\"')[0] # select link between href=\" and \"\n",
    "        additional_info['Link:'] = url\n",
    "    except: additional_info['Link:'] = np.nan\n",
    "        \n",
    "    \n",
    "    # If any of the elements is not found in the summary-field add None value to dictionary \n",
    "    if \"Price:\" not in additional_info:\n",
    "        additional_info['Price:'] = np.nan\n",
    "        \n",
    "    if \"Savings:\" not in additional_info:\n",
    "        additional_info['Savings:'] = np.nan\n",
    "        \n",
    "    if \"Expiry:\" not in additional_info:\n",
    "        additional_info['Expiry:'] = np.nan\n",
    "    \n",
    "    return additional_info # Return dictionary containing with information on price, saving and expiry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_comments(post:str, title:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves comments from all post pages.\n",
    "    \n",
    "    Arg:\n",
    "    post: url to the first page of a post\n",
    "    title: the title of the post\n",
    "    \n",
    "    Returns:\n",
    "    df_comments: DataFrame object. Each row corresponds to an indevidual comment.\n",
    "    A second column indicates the title of original post on which the comment was made\n",
    "    \"\"\"\n",
    "    \n",
    "    # Data storage\n",
    "    df_comments = pd.DataFrame()\n",
    "    comment_list = []\n",
    "    \n",
    "    # subsequent pages can be retireved through: root_url + \"page#/\"\n",
    "    root_url = post\n",
    "    \n",
    "    # Get and parse content from first post page\n",
    "    response = requests.get(post)\n",
    "    content = response.content\n",
    "    parser = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Retrive total number of pages for iteration\n",
    "    # There is only one page, if the \"pagination_menu_trigger\" class doesn't exist -> except\n",
    "    try:\n",
    "        pages = parser.select(\".pagination_menu_trigger\")[0].text\n",
    "        last_page = int(re.findall(\"\\d+\", pages)[1]) # first element is current page, second is last page\n",
    "    except:\n",
    "        last_page = 1\n",
    "    \n",
    "    # Iterate through pages and retrieve comments\n",
    "    for page in range(1,(last_page+1)):\n",
    "        if page == 1:\n",
    "            # print(title, \"\\nPage 1 is being scrapped for comments...\")\n",
    "            \n",
    "            #Get comments from first page\n",
    "            comments = parser.select(\".post_content .content\")\n",
    "            comment_list.extend(comment.text for comment in comments)\n",
    "            \n",
    "            # print(\"Comments found on page 1:\", len(comments))\n",
    "        else:\n",
    "            # print(\"Page {} is being scrapped for comments...\".format(page))\n",
    "            \n",
    "            # Parse next page\n",
    "            next_page = root_url + str(page) # next page\n",
    "            response = requests.get(next_page)\n",
    "            content = response.content\n",
    "            parser = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # Get comments\n",
    "            comments = parser.select(\".post_content .content\")\n",
    "            comment_list.extend(comment.text for comment in comments)\n",
    "            \n",
    "            # print(\"Comments found on page {}:\".format(page), len(comments))\n",
    "    \n",
    "    # Fill dataframe to return\n",
    "    title_col = pd.Series(title for i in range(len(comment_list)))\n",
    "    df_comments['title'] = title_col\n",
    "    df_comments['comments'] = pd.Series(comment_list)\n",
    "    return df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_table(posts: list) -> None:\n",
    "    '''\n",
    "    Extracts parsed data from current page and appends to the global table variable.\n",
    "    \n",
    "    Args:\n",
    "    posts - list of parsed post elements obtained through get_posts()\n",
    "    '''\n",
    "    \n",
    "    # Temporary DataFrame object that will be appended to the global 'table' variable\n",
    "    tmp_table = pd.DataFrame()\n",
    "    tmp_comments = pd.DataFrame()\n",
    "    \n",
    "    # Initializing columns for tmp_table\n",
    "    title_col = pd.Series()\n",
    "    source_col = pd.Series()\n",
    "    url_col = pd.Series()\n",
    "    votes_col = pd.Series()\n",
    "    replies_col = pd.Series()\n",
    "    views_col = pd.Series()\n",
    "    creation_date_col = pd.Series()\n",
    "    last_reply_col = pd.Series()\n",
    "    author_col = pd.Series()\n",
    "    price_col = pd.Series()\n",
    "    saving_col = pd.Series()\n",
    "    expiry_col = pd.Series()\n",
    "    parent_col = pd.Series()\n",
    "    thread_col = pd.Series()\n",
    "    \n",
    "    #Initializing columns for tmp_comments\n",
    "    comment_title = pd.Series()\n",
    "    comment_order = pd.Series()\n",
    "    comment_comment = pd.Series()\n",
    "    \n",
    "\n",
    "    # Iterate through post elements on current page and extract data for table\n",
    "    for post in posts:\n",
    "        \n",
    "        # Retailer corresponding to deal\n",
    "        try: \n",
    "            source = post.select(\".topictitle_retailer\")[0].text.split(\"\\n\")[0] # split and remove line-break characters\n",
    "            source_series = pd.Series(source) # transforming into Series object allows use of .append method\n",
    "        except: source_series = pd.Series(np.nan)\n",
    "        source_col = source_col.append(source_series, ignore_index=True)\n",
    "\n",
    "        # Number of votes\n",
    "        try: \n",
    "            votes = post.select(\".post_voting\")[0].text.split(\"\\n\")[1] \n",
    "            votes_series = pd.Series(votes) \n",
    "        except: votes_series = pd.Series(0)\n",
    "        votes_col = votes_col.append(votes_series, ignore_index=True)\n",
    "            \n",
    "        # Title \n",
    "        try:\n",
    "            topic = post.select(\".topic_title_link\") \n",
    "            title = topic[0].text.split('\\n')[1] \n",
    "            title_series = pd.Series(title)\n",
    "        except: title_series = pd.Series(np.nan)\n",
    "        title_col = title_col.append(title_series, ignore_index=True)\n",
    "\n",
    "        # Date of initial posting\n",
    "        try: \n",
    "            creation = post.select(\".first-post-time\")[0].text.split(\"\\n\")[0]\n",
    "            creation_series = pd.Series(creation)\n",
    "        except: creation_series = pd.Series(np.nan)\n",
    "        creation_date_col = creation_date_col.append(creation_series, ignore_index=True) \n",
    "        \n",
    "        # Date of most recent replie\n",
    "        try: \n",
    "            last_replie = post.select(\".last-post-time\")[0].text.split(\"\\n\")[0]\n",
    "            last_replie_series = pd.Series(last_replie)\n",
    "        except: last_replie_series = pd.Series(np.nan)\n",
    "        last_reply_col = last_reply_col.append(last_replie_series, ignore_index=True) \n",
    "        \n",
    "        # Author user-name\n",
    "        try:\n",
    "            author = post.select(\".thread_meta_author\")[0].text.split(\"\\n\")[0]\n",
    "            author_series = pd.Series(author)\n",
    "        except: author_series = pd.Series(np.nan)\n",
    "        author_col = author_col.append(author_series, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # Number of replies\n",
    "        try:\n",
    "            replies = post.select(\".posts\")[0].text.split(\"\\n\")[0]\n",
    "            replies = replies.replace(\",\",\"\") # remove commas to facilitate data type conversion to integer\n",
    "            replies_series = pd.Series(replies)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        replies_col = replies_col.append(replies_series, ignore_index=True)\n",
    "        \n",
    "        # Number of views\n",
    "        try:\n",
    "            views = post.select(\".views\")[0].text.split(\"\\n\")[0]\n",
    "            views = views.replace(\",\",\"\") # remove commas to facilitate integer conversion\n",
    "            views_series = pd.Series(views)\n",
    "        except: replies_series = pd.Series(np.nan)\n",
    "        views_col = views_col.append(views_series, ignore_index=True)\n",
    "        \n",
    "        # Link to current post used to extract additional information\n",
    "        try:\n",
    "            link = str(topic).split('href=\"')[1] # split at href to extract link\n",
    "            link_clean = link.split('\">')[0] # remove superfluous characters\n",
    "        except: \n",
    "            link_clean = np.nan\n",
    "        \n",
    "        # Additional information post\n",
    "        if link_clean != None: # retrieve information from post, if url exists\n",
    "            post_url = (base_url + \"{}\").format(link_clean) # merge base-, and sub-url to generate the complete post-link\n",
    "            additional_info = get_additional_info(post_url) # get dictionary of additonal information on price, saving, etc.\n",
    "            \n",
    "            # Fill columns with additional information from additional_info dictionary\n",
    "            price_col = price_col.append(pd.Series(additional_info['Price:']), ignore_index=True)\n",
    "            saving_col = saving_col.append(pd.Series(additional_info['Savings:']), ignore_index=True)\n",
    "            expiry_col = expiry_col.append(pd.Series(additional_info['Expiry:']), ignore_index=True)\n",
    "            url_col = url_col.append(pd.Series(additional_info['Link:']), ignore_index=True)\n",
    "            parent_col = parent_col.append(pd.Series(additional_info['Parent:']), ignore_index=True)\n",
    "            thread_col = thread_col.append(pd.Series(additional_info['Thread:']), ignore_index=True)\n",
    "            \n",
    "            # get comments from post\n",
    "            comments_tmp = get_post_comments(post_url, title)\n",
    "        else:\n",
    "            price_col = price_col.append(np.nan)\n",
    "            saving_col = saving_col.append(np.nan)\n",
    "            expiry_col = expiry_col.append(np.nan)\n",
    "            url_col = url_col.append(np.nan)\n",
    "        \n",
    "            \n",
    "    # Fill temporary table\n",
    "    tmp_table['title'] = title_col\n",
    "    tmp_table['votes'] = votes_col.astype(int)\n",
    "    tmp_table['source'] = source_col\n",
    "    tmp_table['creation_date'] = creation_date_col\n",
    "    tmp_table['last_reply'] = last_reply_col\n",
    "    tmp_table['author'] = author_col\n",
    "    tmp_table['replies'] = replies_col.astype(int)\n",
    "    tmp_table['views'] = views_col.astype(int)\n",
    "    tmp_table['price'] = price_col\n",
    "    tmp_table['saving'] = saving_col\n",
    "    tmp_table['expiry'] = expiry_col\n",
    "    tmp_table['url'] = url_col\n",
    "    tmp_table['parent_category'] = parent_col\n",
    "    tmp_table['thread_category'] = thread_col\n",
    "           \n",
    "    # Append temporary objects to global variables \n",
    "    global main_table # gloabal keyword allows modification inside function\n",
    "    main_table = main_table.append(tmp_table)\n",
    "    global comment_table\n",
    "    comment_table = comment_table.append(comments_tmp)\n",
    "    \n",
    "    \n",
    "    # Logging progress\n",
    "    print(\"Current main_table length:\", main_table.shape[0])\n",
    "    print(\"Current comment_table lenth:\", comment_table.shape[0])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information from page: 1\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Get first page information, and set total_pages through get_posts()\n",
    "print('Extracting information from page: 1')\n",
    "print(\"-\"*50)\n",
    "posts = get_posts(root_url)  \n",
    "# Extract infomation from first page to fill table with data\n",
    "fill_table(posts)\n",
    "\n",
    "#Loop through pages and add data to table\n",
    "for page in range(2, (total_pages + 1)):\n",
    "    next_url = root_url + str(page) + \"/\" # URL of next page: base-url + number + \"/\"\n",
    "    print('\\nExtracting information from page: ', page, \" of \", total_pages)\n",
    "    print(\"-\"*50)\n",
    "    # Generate list of posts on current page\n",
    "    posts = get_posts(next_url)\n",
    "\n",
    "    # Fill table from information on current page and posts\n",
    "    fill_table(posts)\n",
    "    \n",
    "print('Total time for scraping:', (time.time()-start_time)/60, \"min.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to csv file\n",
    "main_table.to_csv('rfd_main.csv')\n",
    "comment_table.to_csv('rfd_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>KrayZeeMofo</td>\n",
       "      <td>Jul 14th, 2020 8:35 pm</td>\n",
       "      <td>July 17, 2020</td>\n",
       "      <td>Jul 16th, 2020 5:10 pm</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>$50.60</td>\n",
       "      <td>61</td>\n",
       "      <td>48% off</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Home Improvement &amp; Tools</td>\n",
       "      <td>Ryobi Impact Driver with charger and 2 batteri...</td>\n",
       "      <td>https://the-home-depot-ca.pxf.io/c/341376/5836...</td>\n",
       "      <td>12702</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flora0222</td>\n",
       "      <td>Jul 16th, 2020 8:29 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 5:09 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.99</td>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Staples</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>One Step Hand Sanitizer, Fragrance-Free, 473mL...</td>\n",
       "      <td>https://staplescanada.4u8mqw.net/c/341376/7554...</td>\n",
       "      <td>9065</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>angel_wing0</td>\n",
       "      <td>Jul 16th, 2020 1:29 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 5:09 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adidas</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Adidas Creators Club - 30% OFF SITE WIDE</td>\n",
       "      <td>https://adidas-canada.sjv.io/c/341376/358661/5...</td>\n",
       "      <td>2456</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Blubbs</td>\n",
       "      <td>Jul 15th, 2020 2:41 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 5:07 pm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dell</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Dell Business 15% Off Email Signup Coupon, *St...</td>\n",
       "      <td>http://go.redirectingat.com?id=2927x594702&amp;amp...</td>\n",
       "      <td>13306</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>airartaura</td>\n",
       "      <td>Jul 12th, 2020 10:37 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 16th, 2020 5:07 pm</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>16%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Books, Music, Movies, Magazines</td>\n",
       "      <td>Book Outlet - Many Low Prices on Books (Free S...</td>\n",
       "      <td>https://www.awin1.com/awclick.php?gid=340116&amp;a...</td>\n",
       "      <td>12003</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author            creation_date         expiry  \\\n",
       "0  KrayZeeMofo   Jul 14th, 2020 8:35 pm  July 17, 2020   \n",
       "1    flora0222   Jul 16th, 2020 8:29 am            NaN   \n",
       "2  angel_wing0   Jul 16th, 2020 1:29 pm            NaN   \n",
       "3       Blubbs   Jul 15th, 2020 2:41 pm            NaN   \n",
       "4   airartaura  Jul 12th, 2020 10:37 am            NaN   \n",
       "\n",
       "               last_reply parent_category   price  replies   saving  \\\n",
       "0  Jul 16th, 2020 5:10 pm   Home & Garden  $50.60       61  48% off   \n",
       "1  Jul 16th, 2020 5:09 pm             NaN    6.99       63      NaN   \n",
       "2  Jul 16th, 2020 5:09 pm             NaN     NaN       12      NaN   \n",
       "3  Jul 16th, 2020 5:07 pm             NaN     NaN       85      NaN   \n",
       "4  Jul 16th, 2020 5:07 pm   Entertainment     NaN       45      16%   \n",
       "\n",
       "       source                  thread_category  \\\n",
       "0  Home Depot         Home Improvement & Tools   \n",
       "1     Staples                    Home & Garden   \n",
       "2      adidas                          Apparel   \n",
       "3        Dell          Computers & Electronics   \n",
       "4         NaN  Books, Music, Movies, Magazines   \n",
       "\n",
       "                                               title  \\\n",
       "0  Ryobi Impact Driver with charger and 2 batteri...   \n",
       "1  One Step Hand Sanitizer, Fragrance-Free, 473mL...   \n",
       "2           Adidas Creators Club - 30% OFF SITE WIDE   \n",
       "3  Dell Business 15% Off Email Signup Coupon, *St...   \n",
       "4  Book Outlet - Many Low Prices on Books (Free S...   \n",
       "\n",
       "                                                 url  views  votes  \n",
       "0  https://the-home-depot-ca.pxf.io/c/341376/5836...  12702     14  \n",
       "1  https://staplescanada.4u8mqw.net/c/341376/7554...   9065     90  \n",
       "2  https://adidas-canada.sjv.io/c/341376/358661/5...   2456      4  \n",
       "3  http://go.redirectingat.com?id=2927x594702&amp...  13306     34  \n",
       "4  https://www.awin1.com/awclick.php?gid=340116&a...  12003     29  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('rfd_main.csv').iloc[:,1:]\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1397 entries, 0 to 1396\n",
      "Data columns (total 14 columns):\n",
      "author             1397 non-null object\n",
      "creation_date      1397 non-null object\n",
      "expiry             395 non-null object\n",
      "last_reply         1397 non-null object\n",
      "parent_category    839 non-null object\n",
      "price              933 non-null object\n",
      "replies            1397 non-null int64\n",
      "saving             559 non-null object\n",
      "source             1035 non-null object\n",
      "thread_category    1396 non-null object\n",
      "title              1397 non-null object\n",
      "url                1102 non-null object\n",
      "views              1397 non-null int64\n",
      "votes              1397 non-null int64\n",
      "dtypes: int64(3), object(11)\n",
      "memory usage: 152.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1397</td>\n",
       "      <td>1397</td>\n",
       "      <td>395</td>\n",
       "      <td>1397</td>\n",
       "      <td>839</td>\n",
       "      <td>933</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>559</td>\n",
       "      <td>1035</td>\n",
       "      <td>1396</td>\n",
       "      <td>1397</td>\n",
       "      <td>1102</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>938</td>\n",
       "      <td>1267</td>\n",
       "      <td>73</td>\n",
       "      <td>1228</td>\n",
       "      <td>12</td>\n",
       "      <td>599</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290</td>\n",
       "      <td>146</td>\n",
       "      <td>54</td>\n",
       "      <td>1294</td>\n",
       "      <td>1004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>immad01</td>\n",
       "      <td>Jul 2nd, 2020 10:03 am</td>\n",
       "      <td>July 16, 2020</td>\n",
       "      <td>Jul 16th, 2020 5:02 pm</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Free</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50%</td>\n",
       "      <td>Amazon.ca</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Ryobi 4.0 Ah Battery (2-Pack) $102.4</td>\n",
       "      <td>https://www.kits.com/freeglasses.html</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>341</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>213</td>\n",
       "      <td>193</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.626342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13654.871152</td>\n",
       "      <td>13.382248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.508463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32993.426110</td>\n",
       "      <td>28.809868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4293.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11878.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2786.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>505892.000000</td>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author           creation_date         expiry  \\\n",
       "count      1397                    1397            395   \n",
       "unique      938                    1267             73   \n",
       "top     immad01  Jul 2nd, 2020 10:03 am  July 16, 2020   \n",
       "freq         34                       6             39   \n",
       "mean        NaN                     NaN            NaN   \n",
       "std         NaN                     NaN            NaN   \n",
       "min         NaN                     NaN            NaN   \n",
       "25%         NaN                     NaN            NaN   \n",
       "50%         NaN                     NaN            NaN   \n",
       "75%         NaN                     NaN            NaN   \n",
       "max         NaN                     NaN            NaN   \n",
       "\n",
       "                    last_reply          parent_category price      replies  \\\n",
       "count                     1397                      839   933  1397.000000   \n",
       "unique                    1228                       12   599          NaN   \n",
       "top     Jul 16th, 2020 5:02 pm  Computers & Electronics  Free          NaN   \n",
       "freq                        12                      341    13          NaN   \n",
       "mean                       NaN                      NaN   NaN    53.626342   \n",
       "std                        NaN                      NaN   NaN   140.508463   \n",
       "min                        NaN                      NaN   NaN    -1.000000   \n",
       "25%                        NaN                      NaN   NaN     6.000000   \n",
       "50%                        NaN                      NaN   NaN    17.000000   \n",
       "75%                        NaN                      NaN   NaN    48.000000   \n",
       "max                        NaN                      NaN   NaN  2786.000000   \n",
       "\n",
       "       saving     source          thread_category  \\\n",
       "count     559       1035                     1396   \n",
       "unique    290        146                       54   \n",
       "top       50%  Amazon.ca  Computers & Electronics   \n",
       "freq       34        213                      193   \n",
       "mean      NaN        NaN                      NaN   \n",
       "std       NaN        NaN                      NaN   \n",
       "min       NaN        NaN                      NaN   \n",
       "25%       NaN        NaN                      NaN   \n",
       "50%       NaN        NaN                      NaN   \n",
       "75%       NaN        NaN                      NaN   \n",
       "max       NaN        NaN                      NaN   \n",
       "\n",
       "                                       title  \\\n",
       "count                                   1397   \n",
       "unique                                  1294   \n",
       "top     Ryobi 4.0 Ah Battery (2-Pack) $102.4   \n",
       "freq                                       5   \n",
       "mean                                     NaN   \n",
       "std                                      NaN   \n",
       "min                                      NaN   \n",
       "25%                                      NaN   \n",
       "50%                                      NaN   \n",
       "75%                                      NaN   \n",
       "max                                      NaN   \n",
       "\n",
       "                                          url          views        votes  \n",
       "count                                    1102    1397.000000  1397.000000  \n",
       "unique                                   1004            NaN          NaN  \n",
       "top     https://www.kits.com/freeglasses.html            NaN          NaN  \n",
       "freq                                        6            NaN          NaN  \n",
       "mean                                      NaN   13654.871152    13.382248  \n",
       "std                                       NaN   32993.426110    28.809868  \n",
       "min                                       NaN       7.000000   -71.000000  \n",
       "25%                                       NaN    2210.000000     1.000000  \n",
       "50%                                       NaN    4293.000000     5.000000  \n",
       "75%                                       NaN   11878.000000    15.000000  \n",
       "max                                       NaN  505892.000000   318.000000  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [comments]\n",
       "Index: []"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = pd.read_csv(\"rfd_comments.csv\").loc[:,\"comments\":]\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 1 columns):\n",
      "comments    0 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the the short exploration above we can see that we need to remove two rows with missing values for the comments column. These probably correspond to comments without text. Further, commet of the comment strings will need to be cleaned. For this we will explore some more comments in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2378 entries, 0 to 2379\n",
      "Data columns (total 2 columns):\n",
      "comments    2378 non-null object\n",
      "title       2378 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 55.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Delete rows with empty comments\n",
    "df_comments.dropna(axis=0, inplace=True)\n",
    "df_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Clarisonic is going out of business and everything is 50% off.\\n\\nSale is valid at multiple authorized resellers like Sephora and Amazon.\\n\\n', \"My wife isn't going to like this news lol. Seemed like a profitable business, wonder what went wrong. Does that mean replacement brush heads won't be made anymore? That sucks...\", 'Overpriced even after 50% off', \"redkulat wrote: ↑\\nMy wife isn't going to like this news lol. Seemed like a profitable business, wonder what went wrong. Does that mean replacement brush heads won't be made anymore? That sucks...\\n\\nI guess this is the time to stock up on those.\", 'Why shutting down?', \"What's the differences between women and men clarisonics?\", \"redkulat wrote: ↑\\nMy wife isn't going to like this news lol. Seemed like a profitable business, wonder what went wrong. Does that mean replacement brush heads won't be made anymore? That sucks...\\n\\nProb the various clones that are way cheaper. Olay has some for like 20 bucks? You can probably find a tonne of similar things on aliexpress. I got my gf these things years back and they were still around 100 USD on sale.\\n\\nJust took a quick look. I saw a $4 facial pore cleaner. It's prob trash, but we're talking $4.\", \"Chaselap wrote: ↑\\nWhat's the differences between women and men clarisonics?\\n\\nThe color? I have a grey one that I use (when I remember too, but often don't) and my gf's is pink. I think the power is pretty similar. It's more about the brush you use anyways. Soft or rough/deep cleansing.\", \"Chaselap wrote: ↑\\nWhat's the differences between women and men clarisonics?\\n\\nA woman wants one man to satisfy her every need. A man wants every woman to satisfy his one need.\\nhttp://users.ox.ac.uk/~peter/humour/menjokes.html\", \"Chaselap wrote: ↑\\nWhat's the differences between women and men clarisonics?\\n\\ncolour and something about beards\", '50% is still overpriced like people mentioned.\\n\\nOlay has one for $39.\\n\\nhttps://www.amazon.ca/Olay-Microdermabr ... oogcana-20', 'Miraculous. \\nThey have a \"3D Face Sculptor\". \\nI can reconstruct my face for that $20 brush add-on accessory.\\nThe future is now.\\n\\n(Defines the difference between men\\'s shopping and women\\'s shopping.)', \"strife wrote: ↑\\nProb the various clones that are way cheaper. Olay has some for like 20 bucks? You can probably find a tonne of similar things on aliexpress. I got my gf these things years back and they were still around 100 USD on sale.\\n\\nJust took a quick look. I saw a $4 facial pore cleaner. It's prob trash, but we're talking $4.\\n\\nYour face will tear up\", 'Paging resident skincare expert @MyNameWasTaken', \"Don't forget 5% CB on Rakuten.\", \"alanbrenton wrote: ↑\\nA woman wants one man to satisfy her every need. A man wants every woman to satisfy his one need.\\nhttp://users.ox.ac.uk/~peter/humour/menjokes.html\\n\\nGeez, yeah, let's reinforce gender stereotypes and further contribute to misogyny and sexism. Men just stupid, women just for sex. Got it.\", \"There was a post a few weeks ago where it was 50% off on amazon.ca and Shoppers Beauty Boutique. Must be prelude to the official announcement. \\n\\nThe sale is also happening at Shoppers locations with Beauty Boutique. Seems like there's still in store stock of Mia Smart. But the brush heads are mostly cleared out...\", \"FYI I think these things don't actually spin, they vibrate.\", 'EuroTrance88 wrote: ↑\\n50% is still overpriced like people mentioned.\\n\\nOlay has one for $39.\\n\\nhttps://www.amazon.ca/Olay-Microdermabr ... oogcana-20\\n\\nI have both and they don’t even compare. Clarisonic is a sonic device, its brush head oscillates so little movements back n forth, just like how the Philips Sonicare works. In fact they are created by the same people. Do you think Sonicare is worth the price?\\n50% off is a good price to stock up on brush heads, but I wouldn’t get into the system as a new user at this point.', \"litebrite wrote: ↑There was a post a few weeks ago where it was 50% off on amazon.ca and Shoppers Beauty Boutique. Must be prelude to the official announcement. \\n\\nThe sale is also happening at Shoppers locations with Beauty Boutique. Seems like there's still in store stock of Mia Smart. But the brush heads are mostly cleared out...\\nAnyone got a stock track for shoppers? I'd really appreciate it!\", \"RedFlagCrake wrote: ↑\\nGeez, yeah, let's reinforce gender stereotypes and further contribute to misogyny and sexism. Men just stupid, women just for sex. Got it.\\n\\nThis isn't anthropology 101 kid.\", \"c229chen wrote: ↑\\nI have both and they don’t even compare. Clarisonic is a sonic device, its brush head oscillates so little movements back n forth, just like how the Philips Sonicare works. In fact they are created by the same people. Do you think Sonicare is worth the price?\\n50% off is a good price to stock up on brush heads, but I wouldn’t get into the system as a new user at this point.\\n\\nMaybe because I am a guy. My fiancee has both and when I tried it, I can't tell the difference. Each their own.\", \"Chaselap wrote: ↑\\nAnyone got a stock track for shoppers? I'd really appreciate it!\\n\\nI don't think there is, but on the SDM store locator, you can filter for Beauty Boutiques. It's the Beauty Boutiques that carry Clarisonic. I called a few today and they gnerally answer the phone pretty quick and are very helpful letting you know what's in stock.\", \"When you register a new device on the website for the warranty, you get sent a $25 off $99+ coupon. That coupon stacks with the sale pricing! I was able to stock up on brush heads for a long time... lol\\n\\nEnded up buying 3 of the dual pack deep pore brush heads ($29.00 each) and one cashmere brush head ($19.50). With the coupon, my final sub-total was only $81.5; $91.28 w/ tax. Which brings it to ~$13 all in for each brush head. Don't forget 5% rakuten!\\n\\nI've used the generic no-name brush heads before and didn't like them. I found the name brand Clarisonic ones superior and I'm sticking to those as long as I can.\", \"MashGhasem wrote: ↑\\nThis isn't anthology 101 kid.\\n\\nIt sure isn't.... anthology 101, that's for sure.\", \"RedFlagCrake wrote: ↑\\nIt sure isn't.... anthology 101, that's for sure.\\n\\nObvious spelling mistake. Meant to say anthropology.\\n\\nBut seriously... Buzz off with your nonsense.\", \"RedFlagCrake wrote: ↑\\nIt sure isn't.... anthology 101, that's for sure.\\nAre you invited to parties, more than once?\", \"litebrite wrote: ↑When you register a new device on the website for the warranty, you get sent a $25 off $99+ coupon. That coupon stacks with the sale pricing! I was able to stock up on brush heads for a long time... lol\\n\\nEnded up buying 3 of the dual pack deep pore brush heads ($29.00 each) and one cashmere brush head ($19.50). With the coupon, my final sub-total was only $81.5; $91.28 w/ tax. Which brings it to ~$13 all in for each brush head. Don't forget 5% rakuten!\\n\\nI've used the generic no-name brush heads before and didn't like them. I found the name brand Clarisonic ones superior and I'm sticking to those as long as I can.\\nAny chances that the coupon is generic?\", 'Who knows if Clarisonic will actually payout to Rakuten since they’re going under.', \"RedFlagCrake wrote: ↑\\nGeez, yeah, let's reinforce gender stereotypes and further contribute to misogyny and sexism. Men just stupid, women just for sex. Got it.\\n\\nYea and did you realize that the first part of his comment is not true anymore...\", 'Ok 50% off is a good deal, but consider the fact that if your device goes kaput after only a few months, even a year, you’re basically SOL now that the company is out of business. My Clarisonic wouldn’t hold a charge after only using it less than 50 times (but I didn’t use it often so it had been over a year by the time it stopped working so Clarisonic wouldn’t replace it). Anyways just be aware that if they go out of business there’s basically no recourse for you if you get a faulty device. I’m passing on this, but thank you OP for posting...', 'These are crazy prices, when I hear words like anti-aging you know its marketing scam. \\n\\nReal men use a random orbital palm sander, adjust to a higher grit for a smoother youthful appearance. Lower grit for the scruff manly look.', 'sirxterminator wrote: ↑\\nThese are crazy prices, when I hear words like anti-aging you know its marketing scam. \\n\\nReal men use a random orbital palm sander, adjust to a higher grit for a smoother youthful appearance. Lower grit for the scruff manly look.\\n\\nI use an 8000 grit whetstone when I want to look really sharp. First pass at 1000, though.', 'Just tried applying the WELCOME15 (15%) code on top, but it does nothing ', \"RedFlagCrake wrote: ↑\\nGeez, yeah, let's reinforce gender stereotypes and further contribute to misogyny and sexism. Men just stupid, women just for sex. Got it.\\n\\nYou must be a real blast at a party!\", \"Chaselap wrote: ↑\\nWhat's the differences between women and men clarisonics?\\n\\nOne is pink and the other is black\", '$80+ for a product that only lasts for 3 months and theres no replacement brushes available..', 'Looking to get replacement brush heads and it seems like amazon doesn’t have the sale prices.. Sephora does but very limited styles', 'xChocolate wrote: ↑\\nLooking to get replacement brush heads and it seems like amazon doesn’t have the sale prices.. Sephora does but very limited styles\\n\\ncan’t find much on sephora', 'I guess people are doing less skincare during quarantine?', \"cooper83 wrote: ↑\\nPaging resident skincare expert @MyNameWasTaken\\n\\nLOL \\n\\nI recommend NOT buying. Physical exfoliation is a thing of the past, and it will do more longterm harm to your face with short term superficial benefits. You need to be gentle on your skin. More brands and people are moving over to direct acids hence why The Ordinary became so popular (and of course the price point of the brand). Invest your money in skincare with products that contain A(HA), B(HA) C(Vitamin) depending on your skin type. Do your research to see which one works best for you. \\n\\nRecommended Products:\\nPaula Choice Skin Perfecting BHA 2% \\nSKinceutical Vitamin C (it's patented and hella expensive but it does work) - their is also a dupe called Timeless Vitamin C which is much cheaper, but uneven reviews.\\nCerave Hydrating Cleanser\\n\\nIf you have a private insurance plan, I suggest Retin A as a retinol. There are varying strengths, but this will provide you the best value to improve skin texture and over time minimize scarring. This is a strong product so using a little will go a long way.\\n\\nLastly, all of your efforts will be futile if you don't invest in a good sunscreen, healthy eating habits (drink water and keep hydrated), and patience. Whatever you put inside your body will reflect on the outside too!\\n\\nEdit - a good dermatologist can help you too with recommendations on topical and oral treatments, just be careful that they don't push their brands on you and be transparent in what you want\", \"sirxterminator wrote: ↑\\nThese are crazy prices, when I hear words like anti-aging you know its marketing scam. \\n\\nReal men use a random orbital palm sander, adjust to a higher grit for a smoother youthful appearance. Lower grit for the scruff manly look.\\n\\nLol the amount of St. Ives apricot scrubs I've seen might as well have been a palm sander \", 'time to move over to nu skin or pmd or foreo', 'I wouldnt buy it with 50% off', 'Yes, NuSkin LumiSpa is recommended.\\nPochacco wrote: ↑\\ntime to move over to nu skin\\n', 'BatmanDeals wrote: ↑\\nI wouldnt buy it with 50% off\\n\\nNeeds 80% or better', \"geotr wrote: ↑\\nNeeds 80% or better\\n\\nI prefer 90% of my skin off but that's just me\", 'With that price, I prefer to select the best one NuSkin LumiSpa.', \"Chaselap wrote: ↑\\nAny chances that the coupon is generic?\\n\\nIt doesn't look like it's generic. Looks like a unique code generated after I register a device.\", \"RedFlagCrake wrote: ↑\\nGeez, yeah, let's reinforce gender stereotypes and further contribute to misogyny and sexism. Men just stupid, women just for sex. Got it.\\n\\nThe comment was clearly a joke (even the link @alanbrenton put in his reply has '.../humour/menjokes.html'). But clearly, you wanted to be offended, so ignored the obvious and you found what you needed. For your behaviour, I'm offended. Apologize.\", \"The whole exfoliation market is a bit of a racket. They convince you that you need to wash your face 2+ times a day, then they sell you products for the dry skin that overwashing creates. All you really need to do is wash your face when it's dirty or oily and not otherwise and this whole problem goes away.\", 'Honestly confused with all the hate in the comments. \\n\\nMy brother is a dermatology resident and he gifted me one of these for Christmas and I love it. He also got one for himself and his fiancée.\\n\\nIn the 6 months of owning this and using it twice a day I have noticed a huge difference in my skin. It looks and feels much better. Just in the first week I noticed plenty of breakouts and I assume that is due to it cleaning the dirt hidden deep in my pores.\\n\\nI use a cetaphil cleanser with this and it works perfect. They recommend changing every 3 months but even after 6 months my brush head feels new as I clean and maintain it.\\n\\nSure, I understand $30 for a brush head is steep and $100+ for the initial machine, but you would be surprised how much people (including myself) have spent on trying to obtain great skin.\\n\\nI love this product and can justify the price point. I also use Retina regularly, apply sunscreen regularly and drink plenty of water along with eating healthy, which contributes to great skin. But, just from the Clairsonic itself I noticed a huge change and so did my family. \\n\\nEvery person is different but from the 3 other people I have seen use this product we all love it. As well for ladies the makeup attachments are great my girlfriend loves the foundation applicator. \\n\\nThey have so many different types of brush heads available and there’s one for every skin type. \\n\\nMy only concern is finding proper brush heads now due to them closing, I don’t know the quality of the knock offs.\\n\\nClairsonic is unique as they have patented technology, they use sonic technology where as other ones on the market physically spin and can be too rough on the skin, leading to damaging skin. Ultimately, if you are to use a brush head I would use Clarisonic.\\n\\nThanks OP!', \"MyNameWasTaken wrote: ↑\\nLOL \\n\\nI recommend NOT buying. Physical exfoliation is a thing of the past, and it will do more longterm harm to your face with short term superficial benefits. You need to be gentle on your skin. More brands and people are moving over to direct acids hence why The Ordinary became so popular (and of course the price point of the brand). Invest your money in skincare with products that contain A(HA), B(HA) C(Vitamin) depending on your skin type. Do your research to see which one works best for you. \\n\\nRecommended Products:\\nPaula Choice Skin Perfecting BHA 2% \\nSKinceutical Vitamin C (it's patented and hella expensive but it does work) - their is also a dupe called Timeless Vitamin C which is much cheaper, but uneven reviews.\\nCerave Hydrating Cleanser\\n\\nIf you have a private insurance plan, I suggest Retin A as a retinol. There are varying strengths, but this will provide you the best value to improve skin texture and over time minimize scarring. This is a strong product so using a little will go a long way.\\n\\nLastly, all of your efforts will be futile if you don't invest in a good sunscreen, healthy eating habits (drink water and keep hydrated), and patience. Whatever you put inside your body will reflect on the outside too!\\n\\nEdit - a good dermatologist can help you too with recommendations on topical and oral treatments, just be careful that they don't push their brands on you and be transparent in what you want\\n\\nWould that skinceutical product be a good gift for the gf? Specifically, do I need to do a bunch of research to find out if it'll work well for her first, or since it's so damn premium, it's guaranteed to have at least some nice effects? Thanks!\", \"@Thecheapindian skincare is never straightforward because everyone is different. To be frank, if you noticed plenty of breakouts it may be because you weren't exfoliating enough before. My point of view is that if you're going to shell out this much money for a product about to be discontinued, then you're better off tailoring your skincare for something that works for you. Not to completely knock Clairsonic because it had its time and got people to recognize the importance of exfoliating. Clarisonic on its own is not going to provide you with the exfoliation your skin needs for deep cell turnover. Chemical exfoliation is a better way to go, and this is why you're seeing the turnover because of the Retin A. Also, some people are blessed with resilient skin and that's just genetics.\\n\\n@strife I'm not sure about a gift depending on how your relationship is lol. I know some people may feel offended if you give them skincare products, while others may not care. If you go to skinceutical website they have an online chat, and they may be better off to provide insight. Now, the good thing about ordering direct from them is that they offer a 100% satisfaction guaranteed, so if she is not satisfied it can be returned for a credit (although it may take longer than 30 days for results). From personal experience and reviews, this is a good product and will make a difference with consistent usage (recommended to keep in the fridge to extend shelf life). It will help brighten your skin and assist in diminishing marks over time. I would only recommend either the Vitamin C E Ferulic or Phloretin CF from Skinceuticals. You can also order from Geebeauty (authorized reseller in Toronto) and they offer 10% off if you sign up for their email, but their return policy differs. I hope that helps!\", \"quasarito wrote: ↑\\nThe comment was clearly a joke (even the link @alanbrenton put in his reply has '.../humour/menjokes.html'). But clearly, you wanted to be offended, so ignored the obvious and you found what you needed. For your behaviour, I'm offended. Apologize.\\n\\nJust because something is a joke doesn't make it alright. A racist joke is still ... racist. I've clearly caused a lot of fragile egos to recoil in this thread. Equality matters, and respect for the people around us matters, even on an online deals forum.\", \"litebrite wrote: ↑\\nWhen you register a new device on the website for the warranty, you get sent a $25 off $99+ coupon. That coupon stacks with the sale pricing! I was able to stock up on brush heads for a long time... lol\\n\\nEnded up buying 3 of the dual pack deep pore brush heads ($29.00 each) and one cashmere brush head ($19.50). With the coupon, my final sub-total was only $81.5; $91.28 w/ tax. Which brings it to ~$13 all in for each brush head. Don't forget 5% rakuten!\\n\\nI've used the generic no-name brush heads before and didn't like them. I found the name brand Clarisonic ones superior and I'm sticking to those as long as I can.\\n\\nThanks for this tip! I registered my old device that I never registered and stocked up on different types of brushes. The $25 coupon did stack with the 50% off while the newsletter coupon of 15% off did not.\\n\\nThey say the brushes need to be thrown away after 3 months, but they don't. There are lots of tip online for sanitizing the brushes so you can keep using them, like soaking them in dish soap or vinegar and baking soda. The inner part also comes away from the outer part for easier cleaning. The brushes themselves don't deteriorate like toothbrushes do.\", 'silverele wrote: ↑\\nThanks for this tip! I registered my old device that I never registered and stocked up on different types of brushes. The $25 coupon did stack with the 50% off while the newsletter coupon of 15% off did not.\\n\\nThey say the brushes need to be thrown away after 3 months, but they don\\'t. There are lots of tip online for sanitizing the brushes so you can keep using them, like soaking them in dish soap or vinegar and baking soda. The inner part also comes away from the outer part for easier cleaning. The brushes themselves don\\'t deteriorate like toothbrushes do.\\n\\nOh wow, good to know about the old device. I have the original Mia that I don\\'t think I ever registered.. hmm wonder if I should buy more brush heads.. lol. \\n\\nI haven\\'t been throwing mine out after every 3 months. I have been using the deep pore brush head currently and find that it\\'s now at the right \"softness\" that\\'s effective for my skin.', \"MyNameWasTaken wrote: ↑\\n@Thecheapindian skincare is never straightforward because everyone is different. To be frank, if you noticed plenty of breakouts it may be because you weren't exfoliating enough before. My point of view is that if you're going to shell out this much money for a product about to be discontinued, then you're better off tailoring your skincare for something that works for you. Not to completely knock Clairsonic because it had its time and got people to recognize the importance of exfoliating. Clarisonic on its own is not going to provide you with the exfoliation your skin needs for deep cell turnover. Chemical exfoliation is a better way to go, and this is why you're seeing the turnover because of the Retin A. Also, some people are blessed with resilient skin and that's just genetics.\\n\\n@strife I'm not sure about a gift depending on how your relationship is lol. I know some people may feel offended if you give them skincare products, while others may not care. If you go to skinceutical website they have an online chat, and they may be better off to provide insight. Now, the good thing about ordering direct from them is that they offer a 100% satisfaction guaranteed, so if she is not satisfied it can be returned for a credit (although it may take longer than 30 days for results). From personal experience and reviews, this is a good product and will make a difference with consistent usage (recommended to keep in the fridge to extend shelf life). It will help brighten your skin and assist in diminishing marks over time. I would only recommend either the Vitamin C E Ferulic or Phloretin CF from Skinceuticals. You can also order from Geebeauty (authorized reseller in Toronto) and they offer 10% off if you sign up for their email, but their return policy differs. I hope that helps!\\n\\nI always did exfoliate actually prior to the Clairsonic. \\n\\nI never replaced clairsonic with exfoliation, just for cleansing. Clairsonic is only meant for cleansing and that’s fine for me as it gets the job done, 2 times a day and my skin is clean and refreshed. \\n\\nAlso, with Retin-A I was on and off over the past few months and didn’t start is seriously until May, so the results prior to that were due to Clairsonic as no other variables were changed. \\n\\nI’m not telling people to purchase it now as it’s getting discontinued, but rather giving it recognition that it was one of the best cleansing brushes on the market and now it’s sad to see it go. \\n\\nI would only recommend people get this if they can find brush heads that work well and are compatible, because at the end of the day the machine is that part that has the sonic vibrations built in not the brush head.\", 'RedFlagCrake wrote: ↑\\nJust because something is a joke doesn\\'t make it alright. A racist joke is still ... racist. I\\'ve clearly caused a lot of fragile egos to recoil in this thread. Equality matters, and respect for the people around us matters, even on an online deals forum.\\n100% agree. We all need to do better. By saying \"let it go, it\\'s just a joke\" we\\'re just perpetuating and normalizing negative stereotypes. Find something else to joke about, the English language is rich with other words and topics, no need to settle for lazy sexist humour.', 'Thecheapindian wrote: ↑\\nHonestly confused with all the hate in the comments. \\n\\nMy brother is a dermatology resident and he gifted me one of these for Christmas and I love it. He also got one for himself and his fiancée.\\n\\nIn the 6 months of owning this and using it twice a day I have noticed a huge difference in my skin. It looks and feels much better. Just in the first week I noticed plenty of breakouts and I assume that is due to it cleaning the dirt hidden deep in my pores.\\n\\nI use a cetaphil cleanser with this and it works perfect. They recommend changing every 3 months but even after 6 months my brush head feels new as I clean and maintain it.\\n\\nSure, I understand $30 for a brush head is steep and $100+ for the initial machine, but you would be surprised how much people (including myself) have spent on trying to obtain great skin.\\n\\nI love this product and can justify the price point. I also use Retina regularly, apply sunscreen regularly and drink plenty of water along with eating healthy, which contributes to great skin. But, just from the Clairsonic itself I noticed a huge change and so did my family. \\n\\nEvery person is different but from the 3 other people I have seen use this product we all love it. As well for ladies the makeup attachments are great my girlfriend loves the foundation applicator. \\n\\nThey have so many different types of brush heads available and there’s one for every skin type. \\n\\nMy only concern is finding proper brush heads now due to them closing, I don’t know the quality of the knock offs.\\n\\nClairsonic is unique as they have patented technology, they use sonic technology where as other ones on the market physically spin and can be too rough on the skin, leading to damaging skin. Ultimately, if you are to use a brush head I would use Clarisonic.\\n\\nThanks OP!\\n\\nSecond this. Clarisonic is a cleansing device that’s effective in removing makeup/dirt/oil/dead cells, not meant to replace other exfoliating treatments. I only use it at night, and my current skin condition is significantly better than my teenager/young adult years.\\n\\nTheir technology is solid, but after the L’Oréal acquisition their product lines have become disconnected and confusing over the years. I’m just pissed that I won’t be able to get genuine brush heads once my current 4-5 year supply runs out. I tried well-reviewed knock offs on Amazon before but my skin can feel the difference and prefer the genuine ones.', 'Thanks so much for the heads up... I ordered a ton of replacement brushes and I can only hope that another company will come out with something similar. This product works so well for me for twice daily cleansing.', 'Thx for the heads up. will stock up the head brushes.', 'c229chen wrote: ↑\\nSecond this. Clarisonic is a cleansing device that’s effective in removing makeup/dirt/oil/dead cells, not meant to replace other exfoliating treatments. I only use it at night, and my current skin condition is significantly better than my teenager/young adult years.\\n\\nTheir technology is solid, but after the L’Oréal acquisition their product lines have become disconnected and confusing over the years. I’m just pissed that I won’t be able to get genuine brush heads once my current 4-5 year supply runs out. I tried well-reviewed knock offs on Amazon before but my skin can feel the difference and prefer the genuine ones.\\n\\nHormones and trial and error plays into your skin looking better than your teenage skin. However, for those who use the brush, if it works for you then continue using it. Skincare is complicated as it is to find that one product that resonates with you. My posts are mainly to show that their are better options out there for those on the fence looking to start using Clarisonic.', 'MyNameWasTaken wrote: ↑\\nHormones and trial and error plays into your skin looking better than your teenage skin. However, for those who use the brush, if it works for you then continue using it. Skincare is complicated as it is to find that one product that resonates with you. My posts are mainly to show that their are better options out there for those on the fence looking to start using Clarisonic.\\n\\nHormone changes is 1 factor leading to less breakouts, however when I first used the device in my university years, I immediately noticed clearer skin. For those with oily, not-too-sensitive skin, Clarisonic is not abrasive with the right brush head, and can change the skin texture to be smoother & softer simply by doing a good cleaning job. \\nChemical exfoliation works but one will have to find the time and budget. For those with skin that isn’t sensitive but very allergic, using chemicals can be problematic too. Personally I’ve reacted to some salicylic acid face wash, so never attempted again. I’ve had success with Retinol products, but that’s a different story as we’re talking about cleansing here.\\nI wouldn’t recommend the system due to the lack of future support, not because it’s a trash product.', \"As of today, every AM cardholder can take advantage of Shell Go+ benefits. No need to sign up as long as you are an existing AM cardholder.\\n\\nhttps://shellgoplus.ca/en/home\\n\\nWelcome to Shell Go+\\nNow all AIR MILES® Collectors get even more at Shell\\nOver the years, it’s been our privilege to fuel the journey of Canadians from coast to coast. Now, as our communities feel the impact of recent events, we've given all Collectors membership in Shell Go+ for the remainder of 2020 as part of our commitment to give you more with every visit.\\n\\nHighlights:\\n\\n100% more Miles when you purchase 10 L or more of Shell V-Power premium fuels.1\\n\\n50% more Miles when you purchase 20 L or more of Shell Bronze, Silver and Diesel fuels.2\\n\\n100% more Miles when you spend $5 or more in-store.3\\n\\n10 Bonus Miles with the purchase of any Shell Ultimate car wash.4\\n\\n10 Bonus Miles when you use 95 Cash Miles towards your purchase.5\\n\\n20 Bonus Miles when you spend $20 or more in-store.6\\n\\n5X the Miles Use any BMO AIR MILES credit card to get topped up to 5X the Miles at Shell.7\\n\\nIn addition to your Shell Go+ member benefits, all AIR MILES Collectors will get more from Shell’s new and improved AIR MILES Base Offer on every visit.\\n1 Mile for every 10 L Shell V-Power premium fuels +\\n1 Mile for every 20 L Shell Bronze, Silver and Diesel fuels9 +\\n1 Mile for every $5 in-store\", 'So what do I do exactly?', 'Is this automatically applied to my AM?', 'Akizuki wrote: ↑\\nIs this automatically applied to my AM?\\n\\nYes, it appears so. Just added benefit for all AM cardholders.', 'How is this a deal?', 'For people who use V-Power, this is less than the amount of miles we would usually receive with the 10x offer.', 'john.canad wrote: ↑\\nHow is this a deal?\\n\\nOver the years, there have been many posts regarding GO+ offers. Just trying to share with fellow RFDers. \\nIt is a game changer for me since I carry BMO AM card so I would get alot more AM. For example, for 30L of premium gas, I would normally get only 1AM. Now with using BMO AM card, I would get 15 additional AMs (3 for 30L & 12 for 5X on BMO AM MC).', 'adortok wrote: ↑\\nFor people who use V-Power, this is less than the amount of miles we would usually receive with the 10x offer.\\n\\nGO+ benefit would be on top of whatever the regular offer is going around.', 'Can these offer combine? Probably going to try out myself\\n\\nFor example, buying $20 premium gas, pay in store and redeem 95 miles will get me\\n\\n1 (base) + 1(Offer 1 100%) + 20 (offer 6) + 10 (offer 5) = 32 miles? That is about 16.8% off the listing price', 'taya1214 wrote: ↑\\nGO+ benefit would be on top of whatever the regular offer is going around.\\n\\nYes, but they conveniently stopped this offer now.. It has been going all year before this.', 'taya1214 wrote: ↑\\nGO+ benefit would be on top of whatever the regular offer is going around.\\n\\n10x is gone I believe, cannot longer find it in the app.', 'lyh_chnxm wrote: ↑\\nCan these offer combine? Probably going to try out myself\\n\\nFor example, buying $20 premium gas, pay in store and redeem 95 miles will get me\\n\\n1 (base) + 1(Offer 1 100%) + 20 (offer 6) + 10 (offer 5) = 32 miles? That is about 16.8% off the listing price\\n\\nPretty sure gas does not count as in-store purchases. But the base seems to be 1 mile for EVERY 10 L now, instead of 1 mile for the full transaction regardless of how many L.', '\"50% more Miles when you purchase 20 L or more of Shell Bronze, Silver and Diesel fuels\"\\n\\nSo, 1.5 miles?', 'ImpKnight wrote: ↑\\n\"50% more Miles when you purchase 20 L or more of Shell Bronze, Silver and Diesel fuels\"\\n\\nSo, 1.5 miles?\\n\\nThat\\'s a very good question.... I don\\'t think they do \"0.5\" miles, do they? I\\'d be curious to know how they round it out.', 'nick182 wrote: ↑\\nThat\\'s a very good question.... I don\\'t think they do \"0.5\" miles, do they? I\\'d be curious to know how they round it out.\\n\\nIt\\'s an additional 1AM for every 20L. I am guessing they are calculating 50% from 1AM for 10L. Based on what I see on Go+, I think Shell is changing the regular AM structure to 1AM per every 10L gas where it\\'s currently 1AM only for any fuel purchase over 15L.', 'Only for the remainder of 2020?', '\"1 AIR MILES® Reward Mile per 10 litres of Shell V-Power® NiTRO+ or Shell V-Power Diesel fuel purchased in a single transaction. Offer valid at participating Shell retail locations. Maximum 50 Reward Miles from all Base Offers in a 24-hour period.\\n\\n\\n1 AIR MILES® Reward Mile per 20 litres of Shell Bronze, Shell Silver or Shell Diesel fuel purchased in a single transaction. Offer valid at participating Shell retail locations. Maximum 50 Reward Miles from all Base Offers in a 24-hour period.\"\\n\\nThis is good.', 'ImpKnight wrote: ↑\\n\"1 AIR MILES® Reward Mile per 10 litres of Shell V-Power® NiTRO+ or Shell V-Power Diesel fuel purchased in a single transaction. Offer valid at participating Shell retail locations. Maximum 50 Reward Miles from all Base Offers in a 24-hour period.\\n\\n\\n1 AIR MILES® Reward Mile per 20 litres of Shell Bronze, Shell Silver or Shell Diesel fuel purchased in a single transaction. Offer valid at participating Shell retail locations. Maximum 50 Reward Miles from all Base Offers in a 24-hour period.\"\\n\\nThis is good.\\n\\nThe base Shell Bronze reward got devalued. It was 1 AM/15L prior.\\nSo now to qualify for the extra 50% AM for 20L minimum, it appears you will have to fill 40L = 2AM + 1BAM\\nI guess still better than 15L x 3 = 45L = 3AM. Saves having to gas up three times vs once.', 'i have aways only gotten 1 AM on Bronze and it needs to be 25L or more. how you guys getting 1AM for every 15L?', 'Could you share with me your experience using Shell EasyPay™ app?\\nI\\'ve used Esso\\'s \"Speedpass+\" for the last 2-3 years, and it\\'s great: pay using phone (can store cc info and choose which cc to use), no need to carry loyalty reward cards (and can choose between Optimum VS Esso Extra each time I refuel), no need to carry my Price Privilege card because it\\'s linked in the app.', 'zzricezz wrote: ↑\\ni have aways only gotten 1 AM on Bronze and it needs to be 25L or more. how you guys getting 1AM for every 15L?\\n\\nhttps://www.shell.ca/en_ca/motorists/lo ... -fuel.html\\n\\n\"3. Get 1 Mile on a minimum 15 litre fuel purchase in a single transaction3.\"', 'dantey wrote: ↑\\nhttps://www.shell.ca/en_ca/motorists/lo ... -fuel.html\\n\\n\"3. Get 1 Mile on a minimum 15 litre fuel purchase in a single transaction3.\"\\n\\nok looking at some previous shell go+ offers, they always have a minimum requirement of 25L.', 'btw does this exclude lotto max and 6/49 purchases, not sure why they put bracket instant and scratch? today we get AM for lottery tickets, so if we get the bonus it will be a game changer.\\n\\nlottery tickets (instant and scratch)', 'adaniel wrote: ↑\\nCould you share with me your experience using Shell EasyPay™ app?\\nI\\'ve used Esso\\'s \"Speedpass+\" for the last 2-3 years, and it\\'s great: pay using phone (can store cc info and choose which cc to use), no need to carry loyalty reward cards (and can choose between Optimum VS Esso Extra each time I refuel), no need to carry my Price Privilege card because it\\'s linked in the app.\\n\\nI\\'ve been using the Shell Easypay app ever since it came out because it takes me way too long to walk into the kiosk to pay with my mobility issues. Only thing you have to watch for is making sure that you enter the right pump number (and the app won\\'t take advantage of any in store exclusive promos like 50% off a second car wash)\\n\\nNefCanuck', 'adaniel wrote: ↑\\nCould you share with me your experience using Shell EasyPay™ app?\\nI\\'ve used Esso\\'s \"Speedpass+\" for the last 2-3 years, and it\\'s great: pay using phone (can store cc info and choose which cc to use), no need to carry loyalty reward cards (and can choose between Optimum VS Esso Extra each time I refuel), no need to carry my Price Privilege card because it\\'s linked in the app.\\n\\nwhen it works it great, but i had some issues in the past. I think you can only use the app to pay for gas once a day. When i tried a second transaction it does not work. Anyone else have experience to do multiple payment a day?', \"dantey wrote: ↑\\nThe base Shell Bronze reward got devalued. It was 1 AM/15L prior.\\nSo now to qualify for the extra 50% AM for 20L minimum, it appears you will have to fill 40L = 2AM + 1BAM\\nI guess still better than 15L x 3 = 45L = 3AM. Saves having to gas up three times vs once.\\n\\nI would get 1 AM for an 80L fill up. Now I'll get 6.\", 'zzricezz wrote: ↑\\nwhen it works it great, but i had some issues in the past. I think you can only use the app to pay for gas once a day. When i tried a second transaction it does not work. Anyone else have experience to do multiple payment a day?\\n\\nCan you redeem AM on the mobile?', 'zzricezz wrote: ↑\\nwhen it works it great, but i had some issues in the past. I think you can only use the app to pay for gas once a day. When i tried a second transaction it does not work. Anyone else have experience to do multiple payment a day?\\n\\nI did 3 transactions in a row using EasyPay to maximize my bonus AM return a couple of weeks ago. No problems.', 'Program is worse than before...', \"Can't wait to fill up when we get those 1 AM for every 1 liter of bronze gasoline fill up.\", 'not working for me, it just takes me to signup page...u need to signup for new AMs account to get this or existing members', 'mikka2017 wrote: ↑\\nnot working for me, it just takes me to signup page...u need to signup for new AMs account to get this or existing members\\n\\nYes, that is correct, you have to be an Air Miles member, either existing or recently signed up.', '\"10 Bonus Miles when you use 95 Cash Miles towards your purchase.\"\\n\\nProbably will use that one every time, but wouldn\\'t bother doing multiple transactions of it. Too bad can\\'t redeem AM through the app.', 'Electrah wrote: ↑\\n\"10 Bonus Miles when you use 95 Cash Miles towards your purchase.\"\\n\\nProbably will use that one every time, but wouldn\\'t bother doing multiple transactions of it. Too bad can\\'t redeem AM through the app.\\n\\nBefore using the app on July 5, I was forced to upgrade to a new version (good thing I checked it at home). I was able to redeem Air Miles. However, I was not awarded any Air Miles. I had to do an inquiry, and they were awarded fairly quickly.', \"I used to take advantage of the 10x offer, but this isn't bad for the V-Power fill-ups.\\n\\nTwo 50L fill-ups get me 100 miles, so ~$10 off on every 3rd fill-up with 10AM bonus for redeeming it.\\n\\nEDIT:\\nEdited for my lack of reading comprehension! This is worse than the previous 10x offer.\", '\"50% more Miles when you purchase 20 L or more of Shell Bronze, Silver and Diesel fuels\"\\n\\nSo, 1.5 miles?\\n------\\n\\nYou never earn partial miles, they always round down on anything else I\\'ve seen that it was possible.\\n\\nSo it\\'s a really stupid offer. Useless for me since I drive a hybrid, never fill up 40L+ . So before the supposed boost, I earned 1 air miles per fill up, now I\\'ll earn 1 mile per fill up. Maybe sometimes zero actually since the min L to get 1, is higher.\\n\\nNever cared about the earning on gas anyways though since I spend so little.\\n\\nThe in store stuff seems nice for buying vitamin waters. Offers are overly complicated though and since i\\'ve had issues in the past with shell go+ crediting properly, don\\'t have high hopes these will work well either.\\n\\nThis one in particular:\\n5X the Miles Use any BMO AIR MILES credit card to get topped up to 5X the Miles at Shell.7\\nThe terms and conditions make no sense. They even go as far to give you an example, but than use the most worthless and impossibly unlike example that would be easy to guess how it worked anyways (Gas, if it magically was always just $1.00 per L)\\n\\nIt\\'s like the guys writing the terms actually had no idea how it works either, so just didn\\'t bother to explain it. It simply does not account for the fact in-store earnings don\\'t match up with the way the credit card earns, thus the way it\\'s written can be interpreted too many different ways. Nor does it address stacking, instead just ignores it even though in all cases for this offer, some stacking would happen. The one example even contradicts the most possibly pessimistic? (Not sure the word to use here - consumer unfriendly in any case) interpretation of \"5x BMO credit card miles\" - because it assumes you never earned miles on the dollars spent a shell in the first place on gas (with the bmo CC), which isn\\'t true. I guess all that\\'s for certain is you won\\'t earn 5 times anything, it will be 4 times something, plus something.\\n\\nHere\\'s the full fine print for it \\n \"Offer valid for transactions made from July 13, 2020 through to December 31, 2020 (“Offer Period”), when you use your valid BMO AIR MILES Mastercard to make an eligible purchase at participating Shell locations. You must also enter your AIR MILES Collector Number at the time of transaction.\\nThis 5x top-up offer (“Top-Up Offer”) applies to the base AIR MILES Reward Miles collected from Shell during the Offer Period. Base Miles are awarded by Shell for the following purchases in a single transaction (“Eligible Purchases”):\\n1 AIR MILES Reward Mile for every 20 L of Shell Bronze, Shell Silver or Shell Diesel fuel; or\\n1 AIR MILES Reward Mile for every 10 L of Shell V-Power® Premium fuels; or\\n1 AIR MILES Reward Mile for every $5 spent at a Shell convenience store (“Convenience Store Transactions”), which exclude tobacco products, lottery tickets (instant and scratch), Shell gift cards, partner/third-party gift cards, prepaid credit cards, phone cards, and any other goods or services specified from time to time or as excluded by law.\\nThe Top-Up Offer will be calculated based on the total Eligible Purchase transactions that earn base Miles from Shell during the Offer Period. The total Miles awarded in connection with the Top-Up Offer will be a combination of base Miles awarded by Shell, base Miles awarded by BMO, including any accelerator offer Miles as applicable, and Top-Up Offer Miles that, in sum, will equal 5x the base Miles awarded by Shell. A maximum of 2,500 top-up Miles may be awarded per cardholder during the Offer Period for Eligible Purchases at Convenience Store Transactions.\\nAs an illustrative example of how you’ll earn 5x the Shell base Miles as part of this Top-Up Offer: A customer purchases 40 L of Shell Bronze fuel at $1.00 per litre and pays using a BMO AIR MILES World Elite Mastercard, at a total cost of $40.00. The customer gets two (2) base Miles awarded by Shell (based on an earn rate of one (1) Mile per 20 L of fuel in a single transaction). By paying for this transaction with a BMO AIR MILES World Elite Mastercard, in addition to the two (2) base Miles for this transaction, the customer gets four (4) base Miles awarded by BMO (based on an earn rate of one (1) Mile for every $10 spent by customer in a statement cycle period) and four (4) Top-Up Offer Miles, bringing the total Miles collected to ten (10) AIR MILES Reward Miles, which is 5x the two (2) base Miles awarded by Shell.\\nAIR MILES Reward Miles earned in connection with this Top-Up Offer will appear in your AIR MILES Collector Account, under a combination of base Miles awarded by Shell, base Miles awarded by BMO, including any accelerator offer Miles as applicable, and Top-Up Offer Miles. The Top-Up Offer Miles may post earlier than the base Miles awarded by BMO.\\nAIR MILES Reward Miles earned in connection with purchases on your BMO AIR MILES Mastercard will be calculated for purchases charged to your account (less refunds and excluding cash advances, cash-like transactions, interest charges, fees, credit or debit adjustments) and awarded once per statement period and are not awarded if your account is closed or not in good standing. No cash surrender value. The number of Reward Miles will be rounded down to the nearest whole number. Fractions of Reward Miles will not be awarded.\" ']\n"
     ]
    }
   ],
   "source": [
    "# Print first 100 comments\n",
    "print([x for x in df_comments['comments'][0:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to clean these strings up will be through regular expressions. \n",
    "\n",
    "The following should be removed:  \n",
    "* ↑ symbols\n",
    "* \\n new line characters\n",
    "* urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Clarisonic is going out of business and everyt...</td>\n",
       "      <td>Clarisonic - Shutting Down Sale - 50% Off Ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>My wife isn't going to like this news lol. See...</td>\n",
       "      <td>Clarisonic - Shutting Down Sale - 50% Off Ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Overpriced even after 50% off</td>\n",
       "      <td>Clarisonic - Shutting Down Sale - 50% Off Ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>redkulat wrote:  My wife isn't going to like t...</td>\n",
       "      <td>Clarisonic - Shutting Down Sale - 50% Off Ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Why shutting down?</td>\n",
       "      <td>Clarisonic - Shutting Down Sale - 50% Off Ever...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "0  Clarisonic is going out of business and everyt...   \n",
       "1  My wife isn't going to like this news lol. See...   \n",
       "2                      Overpriced even after 50% off   \n",
       "3  redkulat wrote:  My wife isn't going to like t...   \n",
       "4                                 Why shutting down?   \n",
       "\n",
       "                                               title  \n",
       "0  Clarisonic - Shutting Down Sale - 50% Off Ever...  \n",
       "1  Clarisonic - Shutting Down Sale - 50% Off Ever...  \n",
       "2  Clarisonic - Shutting Down Sale - 50% Off Ever...  \n",
       "3  Clarisonic - Shutting Down Sale - 50% Off Ever...  \n",
       "4  Clarisonic - Shutting Down Sale - 50% Off Ever...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ↑ symbols\n",
    "arrow_removed = [re.sub(\"↑+\",\"\",string) for string in df_comments['comments']]\n",
    "# \\n characters\n",
    "newline_removed = [re.sub(\"\\\\n+\",\" \",string) for string in arrow_removed]\n",
    "# urls\n",
    "urls_removed = [re.sub(r\"\\bhttp.+\",\" \",string) for string in newline_removed]\n",
    "# Assign cleaned comments back\n",
    "df_comments['comments'] = pd.Series(urls_removed)\n",
    "\n",
    "#print([x for x in df_comments['comments'][0:100]])\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation_date :  <class 'str'>\n",
      "expiry :  <class 'float'>\n",
      "last_reply :  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Copy of raw data set\n",
    "df = df_raw.copy()\n",
    "\n",
    "# List of tuples: (column name, column dtype)\n",
    "col_dtypes = [(col, type(x)) for x,col in zip(df.iloc[0], df.columns)]\n",
    "\n",
    "# Print tuple for columns containing dates\n",
    "for col in col_dtypes:\n",
    "    if col[0] in ['creation_date', 'last_reply', 'expiry']:\n",
    "        print(col[0], ': ', col[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the columns are formatted as datetime. To facilitate working with the dates, we will convert them to datetime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date columns to datetime dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(column: str) -> pd.Series:\n",
    "    \"\"\"Converts a column of format \"%b %d, %Y %I:%M %p\" from string to date-time\n",
    "    \n",
    "    Args:\n",
    "    date_column - name of column with dates encoded as strings\n",
    "    \n",
    "    Returns:\n",
    "    Column elements converted to datetime in a pandas.Series object\n",
    "    \"\"\"    \n",
    "    # Superfluous characters removed\n",
    "    column_clean = df[column].str.replace(\"st\",\"\").str.replace(\"nd\",\"\").str.replace(\"rd\",\"\").str.replace(\"th\",\"\").str.strip()\n",
    "    \n",
    "    # Check for correct length of cleaned column\n",
    "    column_len = len(column_clean)\n",
    "    print(\"Cleaned and original column are of equal lenght: \", column_len == len(df[column]), \"\\n\")\n",
    "    \n",
    "    # Convert from format \"%b %d, %Y %I:%M %p\" to datetime\n",
    "    date_column = []\n",
    "    try:\n",
    "        date_column = column_clean.apply(lambda x : datetime.datetime.strptime(str(x), \"%b %d, %Y %I:%M %p\"))\n",
    "    except: \n",
    "        print(\"\\\"%b %d, %Y %I:%M %p\\\" is incorrect format\")\n",
    "        pass\n",
    "    \n",
    "    # Convert from format \"%B %d, %Y\" to datetime\n",
    "    for date in df[column]:\n",
    "        if date is not np.nan:\n",
    "            try:\n",
    "                date_column.append(datetime.datetime.strptime(date, \"%B %d, %Y\"))\n",
    "            except: \n",
    "                print(\"\\\"%B %d, %Y\\\" is incorrect format for\", date)\n",
    "                break\n",
    "        else: \n",
    "            date_column.append(None)\n",
    "    \n",
    "    if len(date_column) != column_len:\n",
    "        print(\"\\n\", \"Incorrect column length!\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", \"Column has expected length!\\n\")\n",
    "    \n",
    "    return pd.Series(date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and original column are of equal lenght:  True \n",
      "\n",
      "\"%B %d, %Y\" is incorrect format for Jan 1st, 2020 8:32 pm\n",
      "\n",
      " Column has expected length!\n",
      "\n",
      "99    2020-07-08 17:53:00\n",
      "100   2020-07-11 20:38:00\n",
      "101   2020-06-12 12:22:00\n",
      "102   2020-07-13 11:35:00\n",
      "103   2020-04-23 13:01:00\n",
      "104   2020-07-13 11:58:00\n",
      "Name: creation_date, dtype: datetime64[ns] \n",
      "\n",
      "99       Jul 8th, 2020 5:53 pm\n",
      "100     Jul 11th, 2020 8:38 pm\n",
      "101    Jun 12th, 2020 12:22 pm\n",
      "102    Jul 13th, 2020 11:35 am\n",
      "103     Apr 23rd, 2020 1:01 pm\n",
      "104    Jul 13th, 2020 11:58 am\n",
      "Name: creation_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# creation_date column converted to datetime\n",
    "creation_date = to_datetime('creation_date')\n",
    "\n",
    "# Compare random slice of original and converted column\n",
    "print(creation_date.iloc[99:105], \"\\n\")\n",
    "print(df.loc[99:104, 'creation_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and original column are of equal lenght:  True \n",
      "\n",
      "\"%B %d, %Y\" is incorrect format for Jul 13th, 2020 3:25 pm\n",
      "\n",
      " Column has expected length!\n",
      "\n",
      "208   2020-07-12 20:04:00\n",
      "209   2020-07-12 19:53:00\n",
      "210   2020-07-12 19:47:00\n",
      "211   2020-07-12 19:43:00\n",
      "212   2020-07-12 19:28:00\n",
      "213   2020-07-12 19:28:00\n",
      "214   2020-07-12 19:10:00\n",
      "Name: last_reply, dtype: datetime64[ns] \n",
      "\n",
      "208    Jul 12th, 2020 8:04 pm\n",
      "209    Jul 12th, 2020 7:53 pm\n",
      "210    Jul 12th, 2020 7:47 pm\n",
      "211    Jul 12th, 2020 7:43 pm\n",
      "212    Jul 12th, 2020 7:28 pm\n",
      "213    Jul 12th, 2020 7:28 pm\n",
      "214    Jul 12th, 2020 7:10 pm\n",
      "Name: last_reply, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# last_reply column converted to datetime\n",
    "last_reply = to_datetime('last_reply')\n",
    "\n",
    "# Print original and new column for comparison\n",
    "print(last_reply.iloc[208:215], \"\\n\")\n",
    "print(df.loc[208:214, 'last_reply'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_datetime('expiry').iloc[50:57], \"\\n\")\n",
    "# print(df.loc[50:56, 'expiry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last logging:  \n",
    "Date:  **Augu 24, 2020**    \n",
    "date is not np.nan:  True  \n",
    "dtype of date:  <class 'str'>  \n",
    "\"%B %d, %Y\" is incorrect format for Augu 24, 2020  \n",
    "\n",
    "From the last logging, it becomes apparent that `st` has been removed from August due to the use of str.replace() in the to_datetime() function. This is not an issue for the columns with a `%b` format for months. The solution is to use the uncleaned data for the `expiry` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and original column are of equal lenght:  True \n",
      "\n",
      "\"%b %d, %Y %I:%M %p\" is incorrect format\n",
      "\n",
      " Column has expected length!\n",
      "\n",
      "50   2020-09-08\n",
      "51   2020-07-23\n",
      "52   2020-07-26\n",
      "53          NaT\n",
      "54          NaT\n",
      "55          NaT\n",
      "56          NaT\n",
      "dtype: datetime64[ns] \n",
      "\n",
      "50    September 8, 2020\n",
      "51        July 23, 2020\n",
      "52        July 26, 2020\n",
      "53                  NaN\n",
      "54                  NaN\n",
      "55                  NaN\n",
      "56                  NaN\n",
      "Name: expiry, dtype: object\n"
     ]
    }
   ],
   "source": [
    "expiry = to_datetime('expiry')\n",
    "print(expiry.iloc[50:57], \"\\n\")\n",
    "print(df.loc[50:56, 'expiry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The to_datetime() function appears to correctly convert each of the columns. The results can now be used in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>expiry</th>\n",
       "      <th>last_reply</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>price</th>\n",
       "      <th>replies</th>\n",
       "      <th>saving</th>\n",
       "      <th>source</th>\n",
       "      <th>thread_category</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trystee</td>\n",
       "      <td>2020-01-01 20:32:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-13 15:25:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>[Various Retailers] Gift Card Deals And Discou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>365169</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yellowmp5</td>\n",
       "      <td>2020-07-13 13:29:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-13 15:23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>RYOBI 20% coupon barcode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2593</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>phoreoneone</td>\n",
       "      <td>2020-07-13 12:34:00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-13 15:23:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dell</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...</td>\n",
       "      <td>http://www.jdoqocy.com/click-749547-12105225?u...</td>\n",
       "      <td>2339</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Blackdove77</td>\n",
       "      <td>2020-07-06 12:49:00</td>\n",
       "      <td>2020-07-19</td>\n",
       "      <td>2020-07-13 15:23:00</td>\n",
       "      <td>Computers &amp; Electronics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>413</td>\n",
       "      <td>100%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Watchdogs 2 PC version free for Everyone</td>\n",
       "      <td>https://register.ubisoft.com/ubisoft-forward-r...</td>\n",
       "      <td>57748</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hkhorace</td>\n",
       "      <td>2020-07-06 09:56:00</td>\n",
       "      <td>2020-07-12</td>\n",
       "      <td>2020-07-13 15:22:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1300</td>\n",
       "      <td>77</td>\n",
       "      <td>$200 off</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>Quickjack 7000slx $200 off - $1300</td>\n",
       "      <td>https://www.costco.ca/quickjack-bl-7000slx-318...</td>\n",
       "      <td>15093</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       author       creation_date     expiry          last_reply  \\\n",
       "0           0      trystee 2020-01-01 20:32:00        NaT 2020-07-13 15:25:00   \n",
       "1           1    yellowmp5 2020-07-13 13:29:00        NaT 2020-07-13 15:23:00   \n",
       "2           2  phoreoneone 2020-07-13 12:34:00        NaT 2020-07-13 15:23:00   \n",
       "3           3  Blackdove77 2020-07-06 12:49:00 2020-07-19 2020-07-13 15:23:00   \n",
       "4           4     hkhorace 2020-07-06 09:56:00 2020-07-12 2020-07-13 15:22:00   \n",
       "\n",
       "           parent_category price  replies    saving      source  \\\n",
       "0                      NaN   NaN      672       NaN         NaN   \n",
       "1                      NaN   NaN       26       NaN  Home Depot   \n",
       "2                      NaN   NaN       23       NaN        Dell   \n",
       "3  Computers & Electronics   NaN      413      100%         NaN   \n",
       "4                      NaN  1300       77  $200 off      Costco   \n",
       "\n",
       "           thread_category                                              title  \\\n",
       "0                Groceries  [Various Retailers] Gift Card Deals And Discou...   \n",
       "1            Home & Garden                           RYOBI 20% coupon barcode   \n",
       "2  Computers & Electronics  Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...   \n",
       "3              Video Games           Watchdogs 2 PC version free for Everyone   \n",
       "4               Automotive                 Quickjack 7000slx $200 off - $1300   \n",
       "\n",
       "                                                 url   views  votes  \n",
       "0                                                NaN  365169    317  \n",
       "1                                                NaN    2593     28  \n",
       "2  http://www.jdoqocy.com/click-749547-12105225?u...    2339     15  \n",
       "3  https://register.ubisoft.com/ubisoft-forward-r...   57748    180  \n",
       "4  https://www.costco.ca/quickjack-bl-7000slx-318...   15093     26  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign datetime columns to DataFrame\n",
    "df.expiry = expiry\n",
    "df.last_reply = last_reply\n",
    "df.creation_date = creation_date\n",
    "\n",
    "# Verify dates\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data: `source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Various Retailers] Gift Card Deals And Discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>RYOBI 20% coupon barcode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Dell</td>\n",
       "      <td>Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Watchdogs 2 PC version free for Everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Costco</td>\n",
       "      <td>Quickjack 7000slx $200 off - $1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                              title\n",
       "0         NaN  [Various Retailers] Gift Card Deals And Discou...\n",
       "1  Home Depot                           RYOBI 20% coupon barcode\n",
       "2        Dell  Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, R...\n",
       "3         NaN           Watchdogs 2 PC version free for Everyone\n",
       "4      Costco                 Quickjack 7000slx $200 off - $1300"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['source', 'title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible, that users simply forgot to include the source of the deal. We will check if missing sources are mentioned in the corresponding title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sources:  163\n",
      "385 missing values in source column\n"
     ]
    }
   ],
   "source": [
    "# Set of entries in 'source' column\n",
    "retailer_set = set(df['source'].dropna())\n",
    "print(\"Number of unique sources: \", len(retailer_set))\n",
    "print(df.source.isnull().sum(), \"missing values in source column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large number of unique sources is promising! \n",
    "\n",
    "Next we will use the set previously created to itterate through the titles an check if any of the unique source names are present. If a source name from the set is found in `title` and no value is found in the corresponding `source` column, then the index as well as the source name are saved in the `replace` dictinoary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements found in 'title': 53\n"
     ]
    }
   ],
   "source": [
    "replace_dict = {} # key: index; value: retailer name to replace missing source value at index\n",
    "# found_in_title = {} # keys: retailer name; values: number of times name was found in a post-title\n",
    "# source_in_title = [] # names of retailers mentioned in at least one title\n",
    "# replaceable_value_count = 0\n",
    "\n",
    "# Iterate throuh set of unique values from source source column\n",
    "for retailer in retailer_set:\n",
    "    \"\"\"Fill replace dictioray with indecies and source names. Entries are made\n",
    "    when a source name is found in the title column while the corresponding source entry\n",
    "    is empty.\"\"\"\n",
    "    \n",
    "    # Iterate through 'source' and 'title' columns row-by-row\n",
    "    # Generate boolean array: True if unique source name (retailer) found in \"title\" and \"source\" is np.nan\n",
    "    source_missing_and_in_title = np.array([retailer in title \n",
    "                                     if source is np.nan else False\n",
    "                                     for title,source in zip(df.title, df.source)])\n",
    "    \n",
    "    # Indecies for which source_missing_and_in_title is True\n",
    "    replacement_indicies = np.where(source_missing_and_in_title == True)[0]\n",
    "    # Fill \"replace\" dictionary\n",
    "    for index in replacement_indicies:\n",
    "        if index not in replace_dict.keys():\n",
    "            replace_dict[index] = retailer\n",
    "\n",
    "print(\"Replacements found in 'title':\", len(replace_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53 missing sources can be replaced by information found in the title. We will use the indecies and values stored in `replace_dict` to replace the appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing source values before replacement: 385\n",
      "Missing source values after replacement: 332\n",
      "53 missing source records have been replaced!\n"
     ]
    }
   ],
   "source": [
    "source_list = list(df.source)\n",
    "missing_start = sum([x is np.nan for x in source_list])\n",
    "print(\"Missing source values before replacement:\", missing_start)\n",
    "\n",
    "for replace_source in replace.items():\n",
    "    source_list[replace_source[0]] = replace_source[1]\n",
    "\n",
    "missing_end = sum([x is np.nan for x in source_list])\n",
    "print(\"Missing source values after replacement:\", missing_end)\n",
    "replaced_count = missing_start-missing_end\n",
    "print(replaced_count, \"missing source records have been replaced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 53 identified `source` records have been replaced with apporiate names of retailers found in the corrseponding `title` column. The new `source` column can now replace the old one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values as expected: True\n"
     ]
    }
   ],
   "source": [
    "df.source = source_list\n",
    "print(\"Number of missing values as expected:\", (df.source.isnull().sum() == missing_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further substitutions for missing `source` values may be found in the `url` column. The objective is to extract company names from the urls and use them to further replace missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 missing source values have corresponding urls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                   NaN\n",
       "3     https://register.ubisoft.com/ubisoft-forward-r...\n",
       "17    http://go.redirectingat.com?id=2927x594702&amp...\n",
       "19    http://click.linksynergy.com/deeplink?id=CAqD7...\n",
       "22        https://www.costco.ca/.product.100476333.html\n",
       "Name: url, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'url' entries of rows with missing source values\n",
    "url_replacement = df[df.source.isnull()].url\n",
    "print(url_replacement.notnull().sum(), \"missing source values have corresponding urls\")\n",
    "url_replacement.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The urls need to be split and cleaned to extract the name of the organisation. The final cleaned values and their corresponding indicies in the DataFrame will be stores in the `clean_urls` disctionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: nan, 3: ['register', 'ubisoft', 'com'], 17: ['go', 'redirectingat', 'com'], 19: ['click', 'linksynergy', 'com'], 22: ['costco', 'ca'], 23: nan, 27: ['activebaby', 'ca'], 41: ['classic', 'avantlink', 'com'], 45: ['awin1', 'com'], 46: nan, 49: ['awin1', 'com'], 54: ['ebox', 'ca'], 59: ['awin1', 'com'], 62: ['store', 'insta360', 'com'], 63: ['amazon', 'ca'], 65: ['costco', 'ca'], 71: ['cityfone', 'net'], 77: ['alltrails', 'com'], 80: ['costco', 'ca'], 81: ['click', 'linksynergy', 'com'], 84: nan, 87: ['econsumer', 'equifax', 'ca'], 91: nan, 94: nan, 103: nan, 113: ['amazon', 'ca'], 122: nan, 141: ['svpsports', 'ca'], 147: ['amazon', 'ca'], 151: nan, 152: ['bcfasteners', 'com'], 156: ['costco', 'ca'], 157: ['staplescanada', '4u8mqw', 'net'], 165: nan, 166: ['register', 'ubisoft', 'com'], 168: ['apps', 'apple', 'com'], 169: ['amazon', 'ca'], 171: ['amazon', 'ca'], 172: ['mailchi', 'mp'], 178: ['amazon', 'ca'], 180: ['bccamera', 'com'], 183: ['amazon', 'ca'], 187: ['tkqlhce', 'com'], 191: ['amazon', 'ca'], 193: nan, 196: nan, 199: ['amazon', 'ca'], 205: ['pi-co', 'ca'], 207: ['awin1', 'com'], 211: ['cdkeys', 'com'], 215: ['go', 'redirectingat', 'com'], 230: ['kits', 'com'], 235: ['the-home-depot-ca', 'pxf', 'io'], 237: ['signup', 'easynews', 'com'], 238: ['tsc', 'ca'], 244: nan, 247: ['amazon', 'ca'], 248: ['amazon', 'ca'], 250: ['fadedsoul', 'com'], 251: ['apfco', 'com'], 253: ['the-home-depot-ca', 'pxf', 'io'], 254: nan, 255: ['canadiantire', 'ca'], 257: ['levis-canada', 'sjv', 'io'], 258: ['canadiantire', 'ca'], 260: ['fujifilmprintlife', 'ca'], 268: ['mysimplemarketplace', 'com'], 272: ['tanguay', 'ca'], 275: ['amazon', 'ca'], 277: ['scotiabank', 'com'], 280: ['amazon', 'ca'], 285: nan, 288: ['cdkeys', 'com'], 298: nan, 299: nan, 301: ['awin1', 'com'], 319: ['ca', 'miele', 'ca'], 323: nan, 328: nan, 329: nan, 330: ['cdkeys', 'com'], 332: ['kqzyfj', 'com'], 334: nan, 338: ['drop', 'com'], 339: ['svpsports', 'ca'], 344: ['rakuten', 'ca'], 345: ['outterlimits', 'com'], 350: ['go', 'redirectingat', 'com'], 351: ['playolg', 'ca'], 353: ['click', 'linksynergy', 'com'], 359: ['click', 'linksynergy', 'com'], 366: ['amazon', 'ca'], 367: ['getmyoffers', 'ca'], 368: ['canex', 'ca'], 371: nan, 376: ['mobvoi', 'com'], 381: ['costco', 'ca'], 385: ['kqzyfj', 'com'], 388: ['store', 'playstation', 'com'], 399: ['click', 'linksynergy', 'com'], 400: nan, 407: ['elac', 'com'], 411: ['facebook', 'com'], 412: nan, 416: nan, 417: nan, 418: nan, 419: ['wellwise', 'ca'], 436: nan, 438: ['amazon', 'ca'], 440: ['ampli', 'ca'], 443: ['burton', 'com'], 451: ['tkqlhce', 'com'], 453: ['cdkeys', 'com'], 458: ['store', 'playstation', 'com'], 463: ['altimatel', 'com'], 464: ['fastchargedocks', 'com'], 465: nan, 471: ['shop', 'lindt', 'ca'], 472: ['formula1', 'com'], 473: ['gotransit', 'com'], 475: ['carrytel', 'ca'], 476: nan, 480: ['the-home-depot-ca', 'pxf', 'io'], 482: nan, 493: ['try', 'fender', 'com'], 494: ['ionos', 'ca'], 498: ['simplii', 'com'], 499: ['amazon', 'ca'], 502: nan, 504: ['costco', 'ca'], 505: ['go', 'redirectingat', 'com'], 508: ['costco', 'ca'], 509: ['wellwise', 'ca'], 512: nan, 514: ['ninjakitchen', 'com'], 515: nan, 526: ['amazon', 'ca'], 527: nan, 555: ['bestbuyca', 'o93x', 'net'], 560: ['anrdoezrs', 'net'], 562: ['sherwin-williams', 'ca'], 576: ['aeroplan-bg-sso', 'points', 'com'], 578: ['bit', 'ly'], 593: ['anrdoezrs', 'net'], 606: ['click', 'linksynergy', 'com'], 608: ['googleadservices', 'com'], 610: ['kqzyfj', 'com'], 618: ['metopera', 'org'], 620: ['billing', 'frugalusenet', 'com'], 622: ['amazon', 'ca'], 624: ['awin1', 'com'], 628: ['therecroom', 'com'], 630: ['decathalon-canada', 'mkr3', 'net'], 639: ['oxio', 'ca'], 643: ['bestbuyca', 'o93x', 'net'], 645: ['amazon', 'ca'], 649: ['speakout7eleven', 'ca'], 660: ['uberats', 'ca'], 664: ['kelloggsgrocerycash', 'ca'], 666: ['click', 'linksynergy', 'com'], 670: ['edifier', 'com'], 674: nan, 675: ['ca', 'manscaped', 'com'], 678: ['click', 'linksynergy', 'com'], 680: nan, 681: ['click', 'linksynergy', 'com'], 685: ['awin1', 'com'], 689: nan, 697: ['play', 'google', 'com'], 705: ['everyonerides', 'org'], 706: ['detourcoffee', 'com'], 714: ['oculus', 'com'], 716: ['americanexpress', 'com'], 723: nan, 727: ['Www', 'nbc', 'ca'], 728: ['cashback', 'highinterestsavings', 'ca'], 735: nan, 740: ['lenovo', 'evyy', 'net'], 747: ['go', 'redirectingat', 'com'], 751: ['rallyforrestaurants', 'ca'], 754: ['oculus', 'com'], 756: ['idrinkcoffee', 'com'], 763: nan, 765: ['cosmeticscompanystore', 'com'], 766: ['napoleon', 'com'], 770: ['brampton', 'ca'], 771: ['treadmillfactory', 'ca'], 773: ['go', 'redirectingat', 'com'], 775: nan, 777: ['awin1', 'com'], 779: ['thecubenet', 'com'], 786: ['dispatchcoffee', 'ca'], 788: ['cwbeggs', 'com'], 789: ['click', 'linksynergy', 'com'], 796: ['lostcraft', 'ca'], 802: ['try', 'tidal', 'com'], 803: ['canada', 'bissell', 'com'], 807: nan, 809: ['go', 'redirectingat', 'com'], 817: ['go', 'redirectingat', 'com'], 823: ['store', 'playstation', 'com'], 824: ['store', 'playstation', 'com'], 825: nan, 827: ['radpowerbikes', 'ca'], 830: ['fitnessavenue', 'ca'], 833: nan, 837: ['enbridgesmartsavings', 'com'], 839: nan, 853: ['us', 'shop', 'battle', 'net'], 857: ['play', 'google', 'com'], 865: ['accounts', 'usenetserver', 'com'], 867: ['edifier', 'com'], 876: ['poppacorn', 'ca'], 878: ['amazon', 'com'], 879: ['amazon', 'ca'], 880: ['sportium', 'ca'], 882: ['mcgillpersonalfinance', 'com'], 884: ['uhaul', 'com'], 885: ['ecscoffee', 'com'], 889: ['teasante', 'com'], 891: ['thermoworks', 'com'], 895: nan, 896: nan, 900: ['bonusboom', 'airmiles', 'ca'], 905: ['outterlimits', 'com'], 908: ['completeequipment', 'ca'], 911: ['walmartphotocentre', 'ca'], 916: ['the-home-depot-ca', 'pxf', 'io'], 917: ['avantlink', 'ca'], 922: ['bestbuyca', 'o93x', 'net'], 928: nan, 932: ['ugo', 'ca'], 935: ['go', 'redirectingat', 'com'], 943: nan, 947: ['harryrosen', 'com'], 952: nan, 955: ['luckymobile', 'ca'], 962: ['aeroplan', 'com'], 964: nan, 974: ['waterloobrewing', 'com'], 980: ['gibbyselectronicsupermarket', 'ca'], 990: nan, 993: ['twitter', 'com'], 995: ['awin1', 'com'], 1000: ['deezer', 'com'], 1001: ['click', 'linksynergy', 'com'], 1006: ['software', 'pcworld', 'com'], 1013: ['dicksonbbq', 'com'], 1015: nan, 1019: ['amazon', 'ca'], 1023: ['fitnessavenue', 'ca'], 1032: ['go', 'redirectingat', 'com'], 1049: ['edifier', 'com'], 1051: ['amazon', 'ca'], 1058: ['stacksocial', 'com'], 1073: ['awin1', 'com'], 1089: ['apps', 'apple', 'com'], 1093: ['razer', 'com'], 1102: nan, 1108: ['store', 'playstation', 'com'], 1118: ['tkqlhce', 'com'], 1128: ['play', 'google', 'com'], 1144: ['store', 'playstation', 'com'], 1146: ['brownsshoes', 'com'], 1149: ['amazon', 'ca'], 1152: ['lecreuset', 'ca'], 1163: ['news', 'xbox', 'com'], 1168: nan, 1169: ['store', 'steampowered', 'com'], 1181: ['google', 'com'], 1183: ['storefront', 'points', 'com'], 1184: nan, 1191: nan, 1198: ['awin1', 'com'], 1216: ['shoprbc', 'com'], 1218: ['go', 'redirectingat', 'com'], 1223: ['cpapoutlet', 'ca'], 1225: ['circlekgames', 'ca'], 1228: ['humblebundle', 'com'], 1243: nan, 1248: ['store', 'playstation', 'com'], 1253: ['scholastic', 'ca'], 1259: ['go', 'redirectingat', 'com'], 1261: nan, 1266: nan, 1270: nan, 1277: ['warriorsandwonders', 'com'], 1278: ['nintendo', 'com'], 1280: ['ituonline', 'com'], 1282: ['facebook', 'com'], 1298: ['getsuperwrap', 'com'], 1307: ['play', 'google', 'com'], 1311: ['beanwise', 'ca'], 1321: nan, 1331: ['docs', 'google', 'com'], 1336: nan, 1347: ['ca', 'norlanglass', 'com'], 1364: ['gog', 'com'], 1370: nan, 1377: nan, 1386: ['usenetprime', 'com'], 1389: ['store', 'asuswebstorage', 'com'], 1392: ['pntrac', 'com'], 1393: ['irobot', 'ca'], 1399: ['instagram', 'com'], 1400: ['doordashca', 'launchgiftcards', 'com'], 1414: ['store', 'playstation', 'com'], 1415: ['hellodemello', 'com'], 1423: ['awin1', 'com'], 1424: ['detourcoffee', 'com'], 1439: nan, 1441: ['weber', 'com'], 1444: ['humblebundle', 'com'], 1448: ['atlas-machinery', 'com'], 1449: ['kits', 'com'], 1456: ['newsgroupdirect', 'com'], 1457: ['100dollar', 'darkhorseapp', 'net'], 1458: ['amazon', 'ca'], 1459: ['taappliance', 'com'], 1461: ['consumer', 'huawei', 'com'], 1465: nan, 1468: nan, 1471: ['viofo', 'com'], 1472: ['cdkeys', 'com'], 1473: ['play', 'google', 'com'], 1475: ['shecodes', 'io'], 1483: ['go', 'redirectingat', 'com'], 1485: ['community', 'stadia', 'com'], 1486: ['pc-canada', 'com'], 1494: nan, 1498: ['go', 'redirectingat', 'com']}\n"
     ]
    }
   ],
   "source": [
    "clean_urls = {} # key: index in df, value: cleaned url\n",
    "indicies = url_replacement.index\n",
    "\n",
    "for url in zip(indicies, url_replacement):\n",
    "    index = url[0]\n",
    "    replacement_url = url[1]\n",
    "    \n",
    "    # Clean if url value not missing\n",
    "    if replacement_url is not np.nan:\n",
    "        url_root = replacement_url.split(\"//\")[1].split(\"/\")[0].split(\"?\")[0].replace(\"www.\", \"\")\n",
    "        removed_domain = url_root.split(\".\")\n",
    "        clean_urls[index] = removed_domain\n",
    "    else:\n",
    "        clean_urls[index] = np.nan\n",
    "        \n",
    "print(clean_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify the company names from the url splits observed in the print above.\n",
    "The patterns shown in the table will facilitate this process. This is an oversimplification and will lead to some false extractions but the number of errors should be minimal.\n",
    "\n",
    "|Condition| Pattern|\n",
    "|---|---|\n",
    "|Lists length 2| company name is at index 0|\n",
    "|Lists length 3 and domain com, ca, or net| name is at index 1|\n",
    "|List length 3 and domain io| name is at index 0| \n",
    "|List length 4| no identifiable name|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_url_final = clean_urls.copy()\n",
    "\n",
    "for item in clean_url_final.items():\n",
    "    index = item[0]\n",
    "    url_split = item[1]\n",
    "    try:\n",
    "        if len(url_split) == 2:\n",
    "             # name at index 0\n",
    "            clean_url_final[index] = url_split[0].title()\n",
    "        \n",
    "        elif ((len(url_split) == 3) \n",
    "                        and ((url_split[-1] == \"com\") \n",
    "                                 or (url_split[-1] == \"ca\") \n",
    "                                 or (url_split[-1] == \"ca\"))):\n",
    "            # name at index 1\n",
    "            clean_url_final[index] = url_split[1].title()\n",
    "        \n",
    "        elif ((len(url_split) == 3) \n",
    "                        and (url_split[-1] == \"io\")):\n",
    "             # name at index 0\n",
    "            clean_url_final[index] = url_split[0].title()\n",
    "        else: \n",
    "              clean_url_final[index] = np.nan\n",
    "    except: value = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing source values remaining:  78\n"
     ]
    }
   ],
   "source": [
    "# Add url-derived company names to DataFrame\n",
    "df.loc[list(clean_url_final.keys()),'source'] = list(clean_url_final.values())\n",
    "print(\"Missing source values remaining: \", df.source.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data: `price`\n",
    "\n",
    "Users may have forgoten to tag prices associated with the deals they posted. We will verify if their are any `$` signs in the title for those rows that have missing price values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 missing values in 'price' column\n",
      "246 missing prices have '$' signs in the title\n"
     ]
    }
   ],
   "source": [
    "missing_prices_df = df[df.price.isnull()]\n",
    "price_in_title = [\"$\" in title for title in missing_prices_df.title]\n",
    "print(df.price.isnull().sum(), \"missing values in 'price' column\")\n",
    "print(sum(price_in_title), \"missing prices have '$' signs in the title\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dell G5 - 15\" 144Hz, i7-10750H, 16GB, 512GB, RTX 2060 $1,549 ($1425/$1285 Rakuten/Rakuten+Uni)',\n",
       " 'iTunes gift cards - $21.49 for $25 | $84.99 for $100 | $167.99 for $200 (Members only)',\n",
       " 'Lenovo Duet Chromebook 2-in-1: $399 - preorder',\n",
       " '[Uplay/Steam] Far Cry 5 (11.99$) - Far Cry 4 (7.99$) - Far Cry 3 (3.90$)',\n",
       " 'Ebay - Take $5 off minimum purchase of $5.01 YMMV',\n",
       " 'Samsung Galaxy S10 Lite (6.7\", 128GB, 8GB, SD855, 4500mah) US$431 / CAD$585 (and Note 10 Lite CAD$572)',\n",
       " 'Amex Business Platinum: $250 credit for $250 spend at dell.ca',\n",
       " 'Shopping.ca Gift Card by Ivanhoé Cambridge - Spend at least $100 and Earn $20 credit. Up to 3 times - YMMV',\n",
       " 'Samsung CRG9 - 49inch 1440p 120hrz monitor $1299 Best Buy',\n",
       " 'Book Outlet - Many Low Prices on Books (Free Shipping Over $45 & 16% Off)']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 10 title to evaluate if the missing price could be substituted\n",
    "replacement_titles = missing_prices_df[price_in_title].title\n",
    "[title for title in replacement_titles][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible replacements: 246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2                           [$1,549, $1425, $1285]\n",
       "12      [$21.49, $25, $84.99, $100, $167.99, $200]\n",
       "16                                          [$399]\n",
       "17                          [11.99$, 7.99$, 3.90$]\n",
       "23                                         [$5.01]\n",
       "                           ...                    \n",
       "1465                                         [$49]\n",
       "1485                                        [$139]\n",
       "1488                                    [$60, $20]\n",
       "1493                                    [$10, $14]\n",
       "1498                            [$169.95, $199.95]\n",
       "Name: title, Length: 246, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = \"[$]+[.,]*\\d+[.,]*\\d+\"\\\n",
    "        \"|[.,]*\\d+[.,]*\\d+[$]+\"\\\n",
    "        \"|[a-zA-Z]+[$]+[.,]*\\d+[.,]*\\d+\"\n",
    "price_replacements = replacement_titles.str.findall(regex)\n",
    "print(\"Number of possible replacements:\", len(price_replacements))\n",
    "price_replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll assume the first element in each list is most relevant and use it to replace missing price values. Some inaccuracies are likely to occure but the estimates should be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 replacements found.\n"
     ]
    }
   ],
   "source": [
    "replacement_dict = {} # key: index; value: price to replace missing value at index\n",
    "\n",
    "# Iterate through price lists found in price_replacements and corresonding indecies in DataFrame\n",
    "for replacement in zip(price_replacements, list(price_replacements.index)):\n",
    "    price_list = replacement[0]\n",
    "    index = replacement[1]\n",
    "    if price_list != []:\n",
    "        price = price_list[0]\n",
    "        price_clean = (re.search(r\"\\d+[.,]*\\d+\", price)).group().replace(\",\",\"\")\n",
    "        replacement_dict[index] = price_clean\n",
    "        \n",
    "print(len(replacement_dict), \"replacements found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values: 252\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values\n",
    "df.loc[list(replacement_dict.keys()), 'price'] = list(replacement_dict.values())\n",
    "print(\"Remaining missing values:\", df.price.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Statistics' from 'math' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-16d35d5a8ad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mStatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Statistics' from 'math' (unknown location)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

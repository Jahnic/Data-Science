{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import time\n",
    "import re\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "# Scraping through Chrome webdriver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting URLs\n",
    "centris = \"https://www.centris.ca/en/properties~for-sale?view=Thumbnail\"\n",
    "duproprio = \"https://duproprio.com/en/search/list?search=true&is_for_sale=1&with_builders=1&parent=1&pageNumber=1&sort=-published_at\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centris:\n",
    "    \"\"\"\n",
    "    This class represents a chrome webdriver object with access to centris.ca.\n",
    "    \n",
    "    Attr:\n",
    "    self.url - starting url for scraping process\n",
    "    self.data - pandas.DataFrame object containing scraped data\n",
    "    self.driver - Chrome webdriver\n",
    "    self.containers - List of web-elements containing information on listings\n",
    "        - eg: link to listing, price, picture, address,...\n",
    "    self.links_to_listings - List of web-elements, each containing the link to a listing\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, url=\"https://www.centris.ca/en/houses~for-sale~lac-simon/11851081?view=Summary&uc=3\"): \n",
    "        self.url = url\n",
    "        self.data = pd.DataFrame()\n",
    "        # Path to Chromedriver\n",
    "        self.DRIVER_PATH = 'C:/webdriver/chromedriver.exe'\n",
    "        self.driver = None\n",
    "        # Verification for new DOM\n",
    "        self.old_DOM = {\\\n",
    "                        'title' : [],\\\n",
    "                        'address' : [],\\\n",
    "                        'price' : [],\\\n",
    "                        'lat' : [],\\\n",
    "                        'long' : [],\\\n",
    "                        'descriptions' : [],\\\n",
    "                        'neighbourhood_top' : [],\\\n",
    "                        'neighbourhood_mid' : [],\\\n",
    "                        'neighbourhood_buttom' : [],\\\n",
    "                        'demographics_buttons' : [],\\\n",
    "                    }\n",
    "        \n",
    "    def reset_old_DOM(self):\n",
    "        self.old_DOM = {\\\n",
    "                        'title' : [],\\\n",
    "                        'address' : [],\\\n",
    "                        'price' : [],\\\n",
    "                        'lat' : [],\\\n",
    "                        'long' : [],\\\n",
    "                        'descriptions' : [],\\\n",
    "                        'neighbourhood_top' : [],\\\n",
    "                        'neighbourhood_mid' : [],\\\n",
    "                        'neighbourhood_buttom' : [],\\\n",
    "                        'demographics_buttons' : [],\\\n",
    "                    }\n",
    "\n",
    "    def append_data(self, title, address, price,\\\n",
    "            lat, long, descriptions, neighbourhood_indicators,\\\n",
    "            population, demographics):\n",
    "        \"\"\"Appends new data to existing data frame.\n",
    "        \n",
    "        Args:\n",
    "        title - string\n",
    "        address - string \n",
    "        price - \n",
    "        lat - \n",
    "        long - \n",
    "        descriptions - \n",
    "        neighbourhood_indicators -\n",
    "        population - \n",
    "        demographics - \n",
    "        \"\"\"\n",
    "        new_data = pd.DataFrame({\\\n",
    "                        'title': title,\\\n",
    "                        'address': address,\\\n",
    "                        'price': price,\\\n",
    "                        'lat': lat,\\\n",
    "                        'long': long\\\n",
    "                    }, index=[0])\n",
    "        \n",
    "        # DESCRIPTIONS\n",
    "        description_table = pd.DataFrame()\n",
    "#         headers_of_interest = [\\\n",
    "#                 \"rooms\", \"bedrooms\", \"powder room\", \"Number of units\", \"Building style\",\\\n",
    "#                 \"Condominium type\", \"Year built\", \"Building area\", \"Lot area\", \"walk_score\",\\\n",
    "#                 \"Net area\", \"Parking (total)\", \"Main unit\", \"Potential gross revenue\", \"Pool\"\\\n",
    "#                               ]\n",
    "#         # Ensures consistency accross listings\n",
    "#         for header in headers_of_interest:\n",
    "#             if header in descriptions.keys():\n",
    "#                 value = descriptions[header]\n",
    "#             else:\n",
    "#                 value = np.nan\n",
    "#             description_table[header] = pd.Series(value)\n",
    "\n",
    "        for key in descriptions.keys():\n",
    "            header = key\n",
    "            value = descriptions[header]\n",
    "            description_table[header] = pd.Series(value)\n",
    "        \n",
    "        # POPULATION AND DEMOGRAPHICS\n",
    "        new_data = pd.concat([new_data, neighbourhood_indicators, description_table,\\\n",
    "                             population, demographics], axis=1)\n",
    "        # LOGGING --------------------------     \n",
    "        #print(new_data)\n",
    "        \n",
    "        self.data = self.data.append(new_data, sort=False,\\\n",
    "                                     ignore_index=True)\n",
    "        \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "        \n",
    "    def start_driver(self):\n",
    "        \"\"\"\n",
    "        Starts and returns Crome webdriver. \n",
    "        The page link in the url attribute \n",
    "        is opened in headless mode.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Activate headless mode for fastest response\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-infobars\"); # disabling infobars\n",
    "        options.add_argument(\"--disable-extensions\"); # disabling extensions\n",
    "        options.add_argument(\"--disable-gpu\"); # applicable to windows os only\n",
    "        options.add_argument(\"--disable-dev-shm-usage\"); # overcome limited resource problems\n",
    "        options.add_argument(\"--no-sandbox\"); # Bypass OS security model\n",
    "        options.add_argument('--start-maximized') # open Browser in maximized mode\n",
    "        options.add_argument('--incognito')\n",
    "\n",
    "        # Start driver with url\n",
    "        self.driver = webdriver.Chrome(executable_path=self.DRIVER_PATH)\n",
    "        self.driver.get(self.url)\n",
    "\n",
    "    def sort_listings(self):\n",
    "        \"\"\"Sorts listings in webdriver from newest to oldest.\"\"\"\n",
    "        \n",
    "        # Click drop down menu\n",
    "        drop_down = self.driver.find_element_by_xpath(\\\n",
    "                                    \"//button[@id='dropdownSort']\")\n",
    "        drop_down.click()\n",
    "        \n",
    "        # Sort by most recent listings\n",
    "        sort_by = self.driver.find_element_by_xpath(\"//a[@data-option-value='3']\")\n",
    "        sort_by.click()\n",
    "    \n",
    "    def goto_first_page(self):\n",
    "        try:\n",
    "            next_page = self.driver.find_element_by_xpath(\\\n",
    "                                        \"//li[@class='goFirst']\")\n",
    "            next_page.click()\n",
    "        except:\n",
    "            print(\"goFirst button not available\")\n",
    "    \n",
    "    def next_page(self):\n",
    "        try:\n",
    "            next_page = self.driver.find_element_by_xpath(\\\n",
    "                                        \"//li[@class='next']\")\n",
    "            next_page.click()\n",
    "            pass\n",
    "        except:\n",
    "            time.sleep(0.5)\n",
    "            # Try again after waiting 0.5 sec.\n",
    "            try:\n",
    "                next_page = self.driver.find_element_by_xpath(\\\n",
    "                                            \"//li[@class='next']\")\n",
    "                next_page.click()\n",
    "                pass\n",
    "            except:\n",
    "                print(\"Next-page button not found!\")\n",
    "                \n",
    "    def get_page_position(self):\n",
    "        '''Returns the first and last page of the current search.\n",
    "        \n",
    "        Returns\n",
    "        tuple - (current_page, last_page), '''\n",
    "        \n",
    "        pages = self.driver.find_element_by_xpath(\\\n",
    "                                    \"//li[@class='pager-current']\").text.\\\n",
    "                                    split(\" / \")\n",
    "        \n",
    "        current_page, last_page = (int(page.replace(\",\",\"\")) for page in pages)\n",
    "        \n",
    "        return (current_page, last_page)\n",
    "    \n",
    "    def refresh_page(self):\n",
    "        \"Refreshes current webdriver page.\"\n",
    "        self.driver.refresh()\n",
    "        print(\"Page is being refreshed.\")\n",
    "        # Wait until page fully loaded\n",
    "        time.sleep(2)\n",
    "        \n",
    "    def distance(origin, destination):\n",
    "        \"\"\"Calculates distances from latitudinal/longitudinal data using\n",
    "        the haversine formula\"\"\"\n",
    "        lat1, lon1 = origin\n",
    "        lat2, lon2 = destination\n",
    "        radius = 6371 # km\n",
    "        \n",
    "        #Convert from degrees to radians\n",
    "        dlat = math.radians(lat2-lat1)\n",
    "        dlon = math.radians(lon2-lon1)\n",
    "        \n",
    "        # Haversine formula\n",
    "        a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "            * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "        d = radius * c\n",
    "\n",
    "        return d\n",
    "                \n",
    "                                                 \n",
    "# Instantiate class object\n",
    "centris = Centris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions need to be outside of the Class. wait_for_xpath() determined the approptiate time to call get_data(). Initially, both fuctions were part of the class object. It seems that after the get_data() call, the driver does not get updated within the class. This leads in some cases to old DOM's being accessed after the browser has already switched to the next page. To circumvent this issue, elements are called outside the class and tried until accessible. This allows the entire new DOM to be loaded before get_data() is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def wait_for_xpath(xpath: str, old_element):\n",
    "        \"\"\"\n",
    "        Wait until elements in new DOM are accessible.\n",
    "        \n",
    "        Arg.\n",
    "        xpath - xpath to new element \n",
    "        old_element - element at xpath from previous DOM (found in centris.old_DOM)\n",
    "        \n",
    "        Returns:\n",
    "        current_element - the element found in the new DOM at xpath\n",
    "        \"\"\"\n",
    "        \n",
    "        centris_driver = centris.driver\n",
    "        element_at_xpath = []\n",
    "        \n",
    "        # Ensure that the NEW rather than the previous or no DOM is active\n",
    "        # Maximum wait time 10 sec.\n",
    "        time_passed = 0\n",
    "        while (\\\n",
    "            (element_at_xpath == old_element or  element_at_xpath == [])\\\n",
    "            and (time_passed <= 10)\\\n",
    "              ):\n",
    "            # Wait for DOM to load\n",
    "            time.sleep(0.2)\n",
    "            time_passed += 0.2\n",
    "            \n",
    "            # Print every 2 seconds\n",
    "            if time_passed%2 == 0:\n",
    "                print(\"Waiting for new DOM...\")\n",
    "            \n",
    "            # Attempt to load new DOM\n",
    "            try: \n",
    "                element_at_xpath = centris_driver.find_elements_by_xpath(xpath)\n",
    "            except: pass\n",
    "        \n",
    "        # After 10 seconds unlikely to load at all -> restart entire process\n",
    "        if time_passed > 10:\n",
    "            print(\"RuntimeError: element not found.\")\n",
    "            centris.refresh_page()\n",
    "            get_data_from_centris()\n",
    "            wait_for_xpath(xpath, old_element)\n",
    "            \n",
    "        return element_at_xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_description(old_DOM):\n",
    "    \"\"\" Requires instantiated centris object. Scrapes and returns\n",
    "    description data: Year build, price, Net area, etc.\"\"\"\n",
    "    \n",
    "    descriptions = wait_for_xpath(\"//div[@class='col-lg-12 description']\",\\\n",
    "                                 old_DOM)\n",
    "    #First three elements not relevant\n",
    "    descriptions_list = descriptions[0].text.split(\"\\n\")[3:]\n",
    "    \n",
    "    #LOGGING------------------------\n",
    "    #print(\"DESCRIPTION:\", descriptions_list)\n",
    "    \n",
    "    # Update old_DOM dictionary with new element for next verification\n",
    "    centris.old_DOM['descriptions'] = descriptions\n",
    "    \n",
    "    return extract_descriptions(descriptions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptions(descriptions_list):\n",
    "    \"\"\"Takes in data from scrape_description() and returns it \n",
    "    as a dictionary\"\"\"\n",
    "    \n",
    "    # The data_dict found on this part of the page is inconsistent across listings\n",
    "    # The first row may contain the number of rooms, bedrooms and bathrooms without headers or may be missing\n",
    "    # Following rows have heathers with associated values after a line break\n",
    "    # The very last element may be a walking score without header\n",
    "    # Listings without first row may supply first row information in subsequent rows with headers\n",
    "    # Because of these inconsistencies, two seperate extractions need to be implemented: one for\n",
    "    # first row lements (if they exist) and another for subsequent rows\n",
    "    \n",
    "    # Transformed data\n",
    "    data_dict = {}\n",
    "    # Distinguish between elements from first and subsequent rows if first row exists\n",
    "    first_row = True\n",
    "    # Starting point for second part of transformation\n",
    "    second_row_index = 0\n",
    "    \n",
    "    # First Part\n",
    "    while first_row == True:\n",
    "        for description in descriptions_list:\n",
    "            numeric = re.findall(\"\\(*[0-9]+\\)*\", description) # numbers\n",
    "            text = re.findall(\"[A-Za-z]+[A-Za-z\\s\\-]*\", description) # text after/inbetween numbers \n",
    "\n",
    "            # Initial elements with numeric values correspond to first row\n",
    "            if (numeric != []):\n",
    "                # For each value there must be one text description\n",
    "                if (len(numeric) == len(text)):\n",
    "                    for description,value in zip(text, numeric):\n",
    "                        # Save as column in data_dict\n",
    "                        description_clean = description.replace(\"and\", \"\").strip()\n",
    "                        data_dict[description_clean] = value\n",
    "                    second_row_index += 1 \n",
    "                else:\n",
    "                    print(\"Unequal number of first row keys and values!\")\n",
    "                    print(\"Numbers:\", numeric)\n",
    "                    print(\"Text:\", text)\n",
    "                    break\n",
    "            else:\n",
    "                first_row = False # No numeric information implies header\n",
    "                break\n",
    "    \n",
    "    # Index range of second extraction\n",
    "    # Headers are found at every second index (0,2,4,...)\n",
    "    # Values are one index apart from their corresponding header (1,3,5,...)\n",
    "    list_length = len(descriptions_list)\n",
    "    if (list_length - second_row_index)%2 == 1: # Implies presence of element without header -> Walk Score\n",
    "        walk_score_listed = True\n",
    "        end_point = list_length -1\n",
    "    else:\n",
    "        walk_score_listed = False\n",
    "        end_point = list_length\n",
    "    # Indices corresponding to headers\n",
    "    extraction_range = range(second_row_index, end_point, 2)\n",
    "    \n",
    "    #LOGGING----------------------\n",
    "#     print(\"Second row index:\", second_row_index)\n",
    "#     print(\"Extraction range:\", extraction_range)\n",
    "#     print(\"List:\", descriptions_list)\n",
    "    \n",
    "    # Second Part\n",
    "    for header_index in extraction_range:\n",
    "        # Headers as column names\n",
    "        header = descriptions_list[header_index]\n",
    "        # Values corresponding to headers are found at subsequent indices\n",
    "        information = descriptions_list[header_index + 1] \n",
    "        data_dict[header] = information\n",
    "    \n",
    "    if walk_score_listed:\n",
    "        data_dict[\"walk_score\"] = descriptions_list[-1]\n",
    "        #LOGGING----------------------\n",
    "        #print(\"Walk Score:\", descriptions_list[-1])\n",
    "        \n",
    "    #LOGGING--------------------------\n",
    "#     print(\"Descriptions:\", data_dict)\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_neighbourhood(old_DOM_top, old_DOM_mid, old_DOM_buttom):\n",
    "    \"\"\" Scrapes and returns a list of ratings \n",
    "    between 0-10 for a set of neighborhood indicators\n",
    "    such as groceries, parks, noise, etc.)\n",
    "    \"\"\"\n",
    "    driver = centris.driver\n",
    "    \n",
    "    # Extract elements from top section of scrollable list\n",
    "    neighbourhood_top = wait_for_xpath(\\\n",
    "                            \"//div[@class='ll-list ps ps--active-y']\",\\\n",
    "                            old_DOM_top)\n",
    "    # Split into indicators and ranking values\n",
    "    top = [x.text for x in neighbourhood_top][0].split(\"\\n\")\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "#     print(\"Top neighbourhood:\", top)\n",
    "    \n",
    "    # Extract middle section - only one element\n",
    "    # Scroll and activate scrollable bar container\n",
    "    scrollable_bar = driver.find_element_by_xpath(\\\n",
    "                                            \"//div[@class='ps__thumb-y']\")\n",
    "    ActionChains(driver).\\\n",
    "        move_to_element(scrollable_bar).\\\n",
    "        send_keys(Keys.PAGE_DOWN).\\\n",
    "        click(scrollable_bar).perform()\n",
    "\n",
    "    # Elements from buttom of scrollable list\n",
    "    neighbourhood_mid = wait_for_xpath(\\\n",
    "                            \"//div[@class='ll-list ps ps--active-y']\",\\\n",
    "                            old_DOM_mid)\n",
    "    # Split into indicators and ranking values\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "    #print(\"Neighbourhoud mid section:\", neighbourhood_mid)\n",
    "    \n",
    "    middle = [x.text for x in neighbourhood_mid][0].split(\"\\n\")\n",
    "    \n",
    "    # Extract buttom section\n",
    "    # Scroll and load remaining elements\n",
    "#     scrollable_bar = driver.find_element_by_xpath(\\\n",
    "#                                             \"//div[@class='ps__thumb-y']\")\n",
    "    ActionChains(driver).\\\n",
    "        move_to_element(scrollable_bar).\\\n",
    "        send_keys(Keys.PAGE_DOWN).\\\n",
    "        click(scrollable_bar).perform()\n",
    "    \n",
    "    # Elements from buttom of scrollable list\n",
    "    neighbourhood_buttom = wait_for_xpath(\\\n",
    "                            \"//div[@class='ll-list ps ps--active-y']\",\\\n",
    "                            old_DOM_buttom)\n",
    "    # Split into indicators and ranking values\n",
    "    buttom = [x.text for x in neighbourhood_buttom][0].split(\"\\n\")\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "#     print(\"Buttom neighbourhood:\", buttom)\n",
    "    \n",
    "    # Unite all three sections by storing tuples of indicator names and corresponding values\n",
    "    united_list = []\n",
    "    list_length = len(top)\n",
    "    for i in range(0, list_length, 2):\n",
    "        united_list.append((top[i], top[i+1]))\n",
    "        united_list.append((middle[i], middle[i+1]))\n",
    "        united_list.append((buttom[i], buttom[i+1]))\n",
    "    \n",
    "    # Create set of unique tuples\n",
    "    neighbourhood_indicators = set(united_list)\n",
    "    \n",
    "    # LOGGING----------------------\n",
    "#     print(\"Number of neighborhood indicators: \", len(neighbourhood_indicators))\n",
    "#     print(\"UNITED:\", united_list)\n",
    "#     print(\"SET:\", neighbourhood_indicators)\n",
    "    \n",
    "    # Verify size and extract information as list\n",
    "    # If size unexpected, refresh page and restart process\n",
    "    if len(neighbourhood_indicators) < 8:\n",
    "            centris.refresh_page()\n",
    "            scrape_neighbourhood(old_DOM_top, old_DOM_buttom)\n",
    "\n",
    "    # Update old_DOM dictionary with new elements for next verification\n",
    "    centris.old_DOM['neighbourhood_top'] = neighbourhood_top\n",
    "    centris.old_DOM['neighbourhood_mid'] = neighbourhood_mid\n",
    "    centris.old_DOM['neighbourhood_buttom'] = neighbourhood_buttom\n",
    "\n",
    "    return extract_neighbourhood_indicators(neighbourhood_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_neighbourhood_indicators(indicators):\n",
    "    \"\"\"Takes in neighbourhood data from scrape_neighbourhood() and returns it \n",
    "    in tabular form as a DataFrame object\"\"\"\n",
    "    data = pd.DataFrame()\n",
    "    for indicator in indicators:\n",
    "        header = indicator[0]\n",
    "        value = indicator[1]\n",
    "        data[header] = pd.Series(value)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_population():\n",
    "    \"\"\"Scrapes and returns population summary data (density, variation etc.)\"\"\"\n",
    "    population_summaries =  centris.driver.find_element_by_id('info')\n",
    "    population_summaries_list = population_summaries\\\n",
    "                        .text.split(\"\\n\")\n",
    "    \n",
    "    # LOGGING-----------------------\n",
    "    #print(\"Population:\", population_summaries_list)\n",
    "    \n",
    "    return extract_population(population_summaries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_population(population):\n",
    "    \"\"\"Takes in population data from scrape_population() and returns it \n",
    "    in tabular form as a DataFrame object\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    for info in population:\n",
    "        units_removed =  info.replace(\"hab/km2\", \"\").strip()\n",
    "        # Numeric data\n",
    "        numeric = re.findall(\"[0-9]+[0-9,]*\", units_removed)\n",
    "        numeric_clean = numeric[-1].replace(\",\",\"\")\n",
    "\n",
    "        # Text data for column names\n",
    "        header = re.findall(\"[a-zA-Z\\s]+\", units_removed)\n",
    "        header_clean = header[0]\n",
    "        # Add numeric data to header excluding the value at index -1\n",
    "        for numeric_head_data in numeric[:-1]:\n",
    "            header_clean = header_clean + str(numeric_head_data) + \" \"\n",
    "\n",
    "        data[header_clean] = pd.Series(numeric_clean).astype(\"int\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_demographics(old_DOM):\n",
    "    \"\"\"Scrapes and return demographic data found in a clickable list\"\"\"\n",
    "    \n",
    "    driver = centris.driver\n",
    "    # Clickable list containing demographic data\n",
    "    menu = driver.find_element_by_id(\"menu\")\n",
    "    # Load menu by moving browser to it\n",
    "    ActionChains(driver).\\\n",
    "    move_to_element(menu).perform()\n",
    "    \n",
    "    #Buttons to access demographics data (education, incomes, etc.)\n",
    "    demographics_buttons = wait_for_xpath(\\\n",
    "                        \"//div[@class='centrisSocioDemobutton']\",\\\n",
    "                                                 old_DOM)\n",
    "\n",
    "    # LOGGING------------------------\n",
    "    # print(\"DEMO. BUTTONS:\", demographics_buttons)\n",
    "\n",
    "    # First entry on clickable demographics list (pre-selected)\n",
    "    demographics = []\n",
    "\n",
    "    # Click buttons to access next demogrpahics elements\n",
    "    for button in demographics_buttons:\n",
    "        try: \n",
    "            button.click()\n",
    "        except: \n",
    "            print(\"Demographics button missing!\")\n",
    "            # Reattempt loading buttons\n",
    "            centris.refresh_page()\n",
    "            ActionChains(driver).\\\n",
    "            move_to_element(menu).\\\n",
    "            perform()\n",
    "            time.sleep(2) # extra time to load\n",
    "            demographics_buttons = wait_for_xpath(\\\n",
    "                        \"//div[@class='centrisSocioDemobutton']\",\\\n",
    "                                                 old_DOM)\n",
    "            \n",
    "        # Get and append data after button click\n",
    "        demographic_data = driver.find_element_by_class_name(\\\n",
    "                         \"socioDemoLabel\")\n",
    "        demographics.append(demographic_data.text)\n",
    "    \n",
    "    # Split each demographic component into separate list\n",
    "    # Example: splits \"Occupation\" data into -> [\"Owners\", \"35%\", \"Renters\", \"65%\"]\n",
    "    demographics = [demo.split(\"\\n\") for demo in demographics]\n",
    "    \n",
    "    #LOGGING------------------------\n",
    "#     print(\"DEMO. DATA:\", demographics)\n",
    "#     print(\"-\"*50)\n",
    "    \n",
    "    # Update old_DOM dictionary with new elements for next verification\n",
    "    centris.old_DOM['demographics_buttons'] = demographics_buttons\n",
    "    \n",
    "    return extract_demographics(demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_demographics(demographics):\n",
    "    \"\"\"Takes in demographic data from extract_demographics() and returns it \n",
    "    in tabular form as a DataFrame object\"\"\"\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for demographic in demographics:\n",
    "        # Remove empty stings from splitting double line breaks \\n\\n\n",
    "        removed_empty_strings = [x for x in demographic if x != \"\"]\n",
    "        # Format of demographic: [header, value, header, value, ...]\n",
    "        header_index = range(0, len(demographic), 2)\n",
    "        for i in header_index:\n",
    "            header = demographic[i] + \" (%)\" # add units to column names\n",
    "            value = demographic[i+1].replace(\"%\", \"\") # remove units from values \n",
    "            data[header] = pd.Series(value).astype(\"int\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_centris():\n",
    "        \"\"\"\n",
    "        Requires instantiate Centris object. Scrapes information from the\n",
    "        webdriver and appends it to the Centris object.\n",
    "        \"\"\"\n",
    "        driver = centris.driver\n",
    "        old_DOM = centris.old_DOM\n",
    "        \n",
    "        # Data from headers\n",
    "        print(\"Start scraping new page...\")\n",
    "        title = wait_for_xpath(\"//span[@data-id='PageTitle']\", old_DOM['title'])\n",
    "        address = wait_for_xpath(\"//h2[@itemprop='address']\", old_DOM['address'])\n",
    "        price = wait_for_xpath(\"//span[@itemprop='price']\", old_DOM['price'])\n",
    "        lat = wait_for_xpath(\"//meta[@itemprop='latitude']\", old_DOM['lat'])\n",
    "        long = wait_for_xpath(\"//meta[@itemprop='longitude']\", old_DOM['long'])\n",
    "        \n",
    "        # Save elements as old DOM\n",
    "        centris.old_DOM['title'] = title\n",
    "        centris.old_DOM['address'] = address\n",
    "        centris.old_DOM['lat'] = lat\n",
    "        centris.old_DOM['long'] = long\n",
    "        \n",
    "        # Scrape remaining elements and store in dataframe\n",
    "        descriptions = scrape_description(old_DOM['descriptions'])\n",
    "        neighbourhood_indicators = scrape_neighbourhood(old_DOM['neighbourhood_top'],\\\n",
    "                                                            old_DOM['neighbourhood_mid'],\\\n",
    "                                                            old_DOM['neighbourhood_buttom'])\n",
    "        population = scrape_population()\n",
    "        demographics = scrape_demographics(old_DOM['demographics_buttons'])\n",
    "                \n",
    "        # Unify data in single dataframe and append to results table\n",
    "        centris.append_data(\n",
    "            title[0].text,\\\n",
    "            address[0].text,\\\n",
    "            price[0].text,\\\n",
    "            lat[0].get_attribute(\"content\"),\\\n",
    "            long[0].get_attribute(\"content\"),\\\n",
    "            descriptions,\\\n",
    "            neighbourhood_indicators,\\\n",
    "            population,\\\n",
    "            demographics\\\n",
    "        )\n",
    "        \n",
    "        # LOGGING--------------------------\n",
    "        #print(\"GET DATA: DESCRIPTIONS:\", descriptions)\n",
    "        \n",
    "        # Return to top of page, to access next-page button\n",
    "        body = driver.find_element_by_tag_name(\"body\")\n",
    "        body.send_keys(Keys.HOME)\n",
    "#         for i in range(7):\n",
    "#             body.send_keys(Keys.PAGE_UP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "centris = Centris()\n",
    "start = time.time()\n",
    "centris.start_driver()\n",
    "centris.sort_listings()\n",
    "print(\"Execution time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next cell, search for the region(s) you want to scrape in the webdriver window.\n",
    "This is not required but will narrow results and reduce runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "current_page, last_page = centris.get_page_position() \n",
    "pages_to_scrape = last_page - current_page + 1 # in case scraping is interupted\n",
    "one_to_100 = range(1,100) # to print message each 1% completion\n",
    "\n",
    "print(\"Scraping initiated.\")\n",
    "print(\"Total number of pages to scrape:\", pages_to_scrape)\n",
    "print(\"Estimated runtime:\", round(pages_to_scrape*((9.6)/(60*60)), 2), \"hours\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i in range(pages_to_scrape):\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"Page:\", i+1)\n",
    "    time_passed = 0 # to exit while loop after 10 seconds\n",
    "    \n",
    "    #Refresh every 20 pages to clear memory build-up\n",
    "    if (i+1)%20 == 0:\n",
    "        print(\"Clearing memory\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Each refresh frees some memory. Four seem to work best.\n",
    "        for i in range(4):\n",
    "            centris.refresh_page()\n",
    "            # Extra time for last refresh\n",
    "            # Ensures that DOM is fully loaded\n",
    "            if i == 3:\n",
    "                time.sleep(2)\n",
    "            \n",
    "    #Retrieve data    \n",
    "    get_data_from_centris()\n",
    "    \n",
    "    # Short delay for chrome to respond to PAGE_UP command\n",
    "    time.sleep(0.5)\n",
    "    centris.next_page()\n",
    "    \n",
    "    # Percent completed of scraping \n",
    "    percent_complete = round(100*((i)/total_pages),2)      \n",
    "    # Print after every 1% mark\n",
    "    if percent_complete in one_to_100:\n",
    "        execution_time = (time.time() - start)/(i+1) # seconds per page\n",
    "        print(percent_complete, \"%\", \"completed\")\n",
    "        print(\"Average execution time per page:\", round(execution_time, 2), \"sec.\")\n",
    "        print(\"Estimated remaining runtime:\", round(\\\n",
    "                                (total_pages - (i+1))\\\n",
    "                                *(execution_time\\\n",
    "                                /(60*60)), 1\\\n",
    "                                                   ), \"hours <\", \"-\"*50)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "\n",
    "print(\"Total runtime:\", execution_time/(60*60), \"hours\")\n",
    "centris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centris.data.to_csv(\"centris_montreal_complete.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"centris_montreal_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info(max_cols=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first remove the last 5 columns since they have virtually no content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,:\"bedroom in basement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will change the data type of the price column to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_clean = df.price.str.replace(\"[$,]\", \"\")\n",
    "df.price = price_clean.astype(\"int\")\n",
    "df.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate data\n",
    "\n",
    "We will use the `address` column to check for duplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.address.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[df.address.duplicated() == False]\n",
    "df.reset_index(inplace=True)\n",
    "df.info(max_cols=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates leaves holes in the index. Due to `reset_index()`, the range now coincides with the number of entries. \n",
    "The first two columns, corresponding to copies of the old and new index, can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, \"title\":]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Columns\n",
    "\n",
    "There are two columns called `rooms` and `room`. We will inspect both to find an explenation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.room.notnull()].loc[:,[\"address\", \"room\", \"rooms\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The room column corresponds to listings with with only a single room. This has been verified by visting the website and searching for some of the addresses. We will therefore merge the two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Non-null values in 'rooms':\")\n",
    "print(\"Before merge -\", df.rooms.notnull().sum())\n",
    "# Merging \"rooms\" and \"room\"\n",
    "rooms_new = pd.Series(\\\n",
    "    [room if pd.notna(room) else rooms for room, rooms in zip(df.room, df.rooms)]\\\n",
    "        )\n",
    "# Remove old columns\n",
    "df.drop([\"rooms\", \"room\"], axis=1, inplace=True)\n",
    "# Add new column\n",
    "df[\"rooms\"] = rooms_new\n",
    "print(\"After merge -\", df.rooms.notnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will now incpect other columns that contain the string \"room\" and replace missing values by 0 to facilitate mathematical operations on room data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rooms = df.columns[[\"room\" in col for col in df.columns]]\n",
    "df[rooms] = df[rooms].fillna(0)\n",
    "print(df[rooms].info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping similar columns\n",
    "bedroom_cols = ['bedrooms','bedroom','bedrooms in basement', 'bedroom in basement']\n",
    "bathroom_cols = ['bathroom', 'bathrooms']\n",
    "powederroom_cols = ['powder room', 'powder rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[bedroom_cols].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we merge `bedrooms` and `bedroom` since they clearly belong together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with 0 for element-wise addition\n",
    "bedrooms = df.bedrooms.fillna(0) + df.bedroom.fillna(0)\n",
    "# Drop old columns from df\n",
    "df.drop([\"bedrooms\", \"bedroom\"], axis=1)\n",
    "# Add new column\n",
    "df.bedrooms = bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At least 1 bedroom in basement\n",
    "df[df[\"bedrooms in basement\"] > 0].\\\n",
    "    loc[:, [\"address\", \"bedrooms\", \"bedrooms in basement\", \"bedroom in basement\", \"price\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bedrooms in the basement may be worth less, merging them with regular bedrooms may therefore not be optimal. However, the listings with bedrooms in the basement don't have regular bedrooms. This is true for for the entire list as well as for listings with a single bedroom in the basement. Merging all columns, therefore, appears to be the best solution. To keep a record of basement bedrooms, we will include a boolean column that can be used to either remove or alter corresponding entries at a later point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_zero(col):\n",
    "    \"\"\"Returns boolean list indicating records with counts of at least 1\"\"\"\n",
    "\n",
    "    not_zero = [count > 0 for count in col]\n",
    "    return not_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge of basement bedroom columns\n",
    "basement_bed = df[\"bedrooms in basement\"]\\\n",
    "         + df[\"bedroom in basement\"])\n",
    "\n",
    "# Records with basement bedrooms\n",
    "basement_bed_bool = not_zero(basement_bed)\n",
    "print(\"Apartments with basement bedrooms:\",\\\n",
    "    sum(basement_bed_bool))\n",
    "\n",
    "# Merge of basement with regular bedrooms\n",
    "all_bedrooms = basement_bed + df.bedrooms\n",
    "print(\"Apartments with any kind of bedroom\",\\\n",
    "    sum(not_zero(all_bedrooms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old columns \n",
    "df.drop(bedroom_cols, axis=1, inplace=True)\n",
    "\n",
    "# Append new columns\n",
    "df[\"bedrooms\"] = all_bedrooms\n",
    "df[\"basement_bedroom\"] = basement_bed_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "bathroom     2465\nbathrooms    1967\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 379
    }
   ],
   "source": [
    "# Non-zero records\n",
    "df[bathroom_cols].apply(lambda col: sum(not_zero(col)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "bathrooms = df.bathroom + df.bathrooms\n",
    "print(\"Non-zero records of merged column:\", sum(not_zero(bathrooms)))\n",
    "\n",
    "# Remove old columns \n",
    "df.drop(bathroom_cols, axis=1, inplace=True)\n",
    "\n",
    "# Append new column\n",
    "df[\"bathrooms\"] = bathrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5424 entries, 0 to 5423\nData columns (total 4 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   rooms             5424 non-null   float64\n 1   bedrooms          5424 non-null   float64\n 2   basement_bedroom  5424 non-null   bool   \n 3   bathrooms         5424 non-null   float64\ndtypes: bool(1), float64(3)\nmemory usage: 132.5 KB\n"
    }
   ],
   "source": [
    "df.iloc[:, -4:].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Powder rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "powder room     1086\npowder rooms      81\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 392
    }
   ],
   "source": [
    "# Non-zero records\n",
    "df[powederroom_cols].apply(lambda col: sum(not_zero(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "powder_rooms = df['powder room'] + df['powder rooms']\n",
    "\n",
    "# Remove old \n",
    "df.drop(powederroom_cols, axis=1, inplace=True)\n",
    "\n",
    "# Append new\n",
    "df['powder_rooms'] = powder_rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Non-zero records:\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "rooms           4451\nbedrooms        4402\nbathrooms       4432\npowder_rooms    1167\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 396
    }
   ],
   "source": [
    "# New room data\n",
    "room_data = df.iloc[:, [-5,-4,-2,-1]]\n",
    "\n",
    "print(\"Non-zero records:\")\n",
    "room_data.apply(lambda col: sum(not_zero(col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Room Data\n",
    "\n",
    "Next we will inspect records with missing data for all 4 room columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
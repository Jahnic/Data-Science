{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News - Analysis of posts\n",
    "\n",
    "Hacker news is a technology oriented forum where useres can post questions and discuss topics. To evaluate popularity and weigh the importance of indevidual submissions, posts can be up or downvoted similar to reddit. In this project, we will analyse \"Ask HN\" submissions which ask specific questions. The time zone for the data is USA Eastern and therefore equal to Montreal time.\n",
    "\n",
    "|Column name (index)| Description|\n",
    "|---|---|\n",
    "|id (0)| Unique identifier of user|\n",
    "|title (1)| Title of post|\n",
    "|url (2)| URL that the post links to|\n",
    "|num_points (3)| Total points (upvotes - downvotes)|\n",
    "|num_comments (4)| Total number of comments|\n",
    "|author (5)| Username |\n",
    "|created_at (6)| Date at post submission|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader # to parse .csv file\n",
    "\n",
    "open_file = open('C:/Users/User/Documents/data_sets/hacker_news.csv', encoding = 'utf-8') # utf-8 encoding required to read data\n",
    "read = reader(open_file) # parse\n",
    "hn = list(read) # transform into list of lists\n",
    "headers = hn[0] # headers\n",
    "hn = hn[1:] # data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"Ask hn\" posts:  9139\n",
      "Number of \"Show hn\" posts:  10158\n",
      "Number of other posts:  273822\n"
     ]
    }
   ],
   "source": [
    "# Filtering Ask HN and Show HN post\n",
    "ask_posts = [] # posts asking a question\n",
    "show_posts = [] # posts answering a question\n",
    "other_posts = [] \n",
    "\n",
    "for row in hn:\n",
    "    title = row[1].lower() # lower case title to facilitate usage of startswith() method\n",
    "    if title.startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else: other_posts.append(row)\n",
    "\n",
    "print('Number of \"Ask hn\" posts: ', len(ask_posts))  \n",
    "print('Number of \"Show hn\" posts: ', len(show_posts))  \n",
    "print('Number of other posts: ', len(other_posts))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_comments(data):\n",
    "    '''Returns the average number of comments in data'''\n",
    "    total_comments = 0 # number of comments in data\n",
    "    total_posts = len(data) # number of posts in data\n",
    "    for row in data:\n",
    "        comments = int(row[4])\n",
    "        total_comments += comments\n",
    "    return total_comments/total_posts # average number of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averge number comments for ask posts:  10.393478498741656\n",
      "Averge number comments for show posts:  4.886099625910612\n",
      "Averge number comments for other posts:  6.4572678601427205\n"
     ]
    }
   ],
   "source": [
    "avg_ask_comments = avg_comments(ask_posts)\n",
    "print('Averge number comments for ask posts: ', avg_ask_comments)\n",
    "avg_show_comments = avg_comments(show_posts)\n",
    "print('Averge number comments for show posts: ', avg_show_comments)\n",
    "avg_other_comments = avg_comments(other_posts)\n",
    "print('Averge number comments for other posts: ', avg_other_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average comments for each post type indicate a substantial difference between ask and show posts. This may be explained by questions stimulating discussions more easily (for example due to their concise nature), being less technically challenging as a consequence of being asked by mostly inexperienced users, or by a greater proclivity of users to help rather than discuss. Since questions generate the most comments, the following analysis will focus on \"Ask HN\" posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At what times do ask-posts aquire the most comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/26/2016 2:53\n",
      "9/26/2016 1:17\n",
      "9/25/2016 22:57\n"
     ]
    }
   ],
   "source": [
    "# identify date format\n",
    "for row in ask_posts[0:3]:\n",
    "    print(row[6])\n",
    "    \n",
    "date_format = \"%m/%d/%Y %H:%M\" # format of post dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts at  00  pm:  301\n",
      "Number of comments at  00  pm:  2277\n",
      "Number of posts at  01  pm:  282\n",
      "Number of comments at  01  pm:  2089\n",
      "Number of posts at  02  pm:  269\n",
      "Number of comments at  02  pm:  2996\n",
      "Number of posts at  03  pm:  271\n",
      "Number of comments at  03  pm:  2154\n",
      "Number of posts at  04  pm:  243\n",
      "Number of comments at  04  pm:  2360\n",
      "Number of posts at  05  pm:  209\n",
      "Number of comments at  05  pm:  1838\n",
      "Number of posts at  06  pm:  234\n",
      "Number of comments at  06  pm:  1587\n",
      "Number of posts at  07  pm:  226\n",
      "Number of comments at  07  pm:  1585\n",
      "Number of posts at  08  pm:  257\n",
      "Number of comments at  08  pm:  2362\n",
      "Number of posts at  09  pm:  222\n",
      "Number of comments at  09  pm:  1477\n",
      "Number of posts at  10  pm:  282\n",
      "Number of comments at  10  pm:  3013\n",
      "Number of posts at  11  pm:  312\n",
      "Number of comments at  11  pm:  2797\n",
      "Number of posts at  12  pm:  342\n",
      "Number of comments at  12  pm:  4234\n",
      "Number of posts at  13  pm:  444\n",
      "Number of comments at  13  pm:  7245\n",
      "Number of posts at  14  pm:  513\n",
      "Number of comments at  14  pm:  4972\n",
      "Number of posts at  15  pm:  646\n",
      "Number of comments at  15  pm:  18525\n",
      "Number of posts at  16  pm:  579\n",
      "Number of comments at  16  pm:  4466\n",
      "Number of posts at  17  pm:  587\n",
      "Number of comments at  17  pm:  5547\n",
      "Number of posts at  18  pm:  614\n",
      "Number of comments at  18  pm:  4877\n",
      "Number of posts at  19  pm:  552\n",
      "Number of comments at  19  pm:  3954\n",
      "Number of posts at  20  pm:  510\n",
      "Number of comments at  20  pm:  4462\n",
      "Number of posts at  21  pm:  518\n",
      "Number of comments at  21  pm:  4500\n",
      "Number of posts at  22  pm:  383\n",
      "Number of comments at  22  pm:  3372\n",
      "Number of posts at  23  pm:  343\n",
      "Number of comments at  23  pm:  2297\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt # time analysis\n",
    "\n",
    "result_list = [] # list of tuples containing post dates and number of comments\n",
    "for row in ask_posts:\n",
    "    result_list.append((row[6], row[4])) # list of tuples (date created, number of comments)\n",
    "\n",
    "posts_by_hour = {} # number of posts for each hour\n",
    "comments_by_hour = {} # number of comments for each hour\n",
    "\n",
    "# Extract hours and implement lists of post and comment frequency counts by hour\n",
    "for tup in result_list:\n",
    "    date = tup[0] # date of post\n",
    "    comments = int(tup[1]) # convert to integer to summarize comments\n",
    "    time = dt.datetime.strptime(date, date_format) # parse dates according to given format\n",
    "    hour = time.strftime(\"%H\") # extract hour from post\n",
    "    if hour not in posts_by_hour: # frequency counts\n",
    "        posts_by_hour[hour] = 1\n",
    "        comments_by_hour[hour] = comments\n",
    "    else:\n",
    "        posts_by_hour[hour] += 1\n",
    "        comments_by_hour[hour] += comments\n",
    "\n",
    "# Post comment counts from 5 - 11 pm\n",
    "table\n",
    "for hour in sorted(posts_by_hour):\n",
    "    print(\"Number of posts at \", hour, \" pm: \", posts_by_hour[hour])\n",
    "    print(\"Number of comments at \", hour, \" pm: \", comments_by_hour[hour])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average number comments per post at each hour\n",
    "avg_by_hour = [] # list of lists containting hours and corresponding average comments per post\n",
    "for hour in posts_by_hour:\n",
    "    posts = posts_by_hour[hour]\n",
    "    comments = comments_by_hour[hour]\n",
    "    avg = round(comments/posts, 1) # average number of comments per post \n",
    "    avg_by_hour.append([avg, hour])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.70 average comments per \"Ask HN\" post at 15:00\n",
      "16.30 average comments per \"Ask HN\" post at 13:00\n",
      "12.40 average comments per \"Ask HN\" post at 12:00\n",
      "11.10 average comments per \"Ask HN\" post at 02:00\n",
      "10.70 average comments per \"Ask HN\" post at 10:00\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "sorted_avg = sorted(avg_by_hour, reverse=True)[:5] # top 5 hours with highest avgerage comments per \"Ask HN\" post\n",
    "for avg in sorted_avg:\n",
    "    average = avg[0]\n",
    "    hour = dt.datetime.strptime(avg[1], '%H') # Initialize hour as datetime object\n",
    "    hour = dt.datetime.strftime(hour, \"%H:%M\") # Formate time \n",
    "    print('{average:.2f} average comments per \"Ask HN\" post at {hour}'.format(average = average, hour = hour))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that at 15:00 the largest number of average comments are registers. This indicates, that for US eastern time, which includes Montreal, 15:00 is the best time to ask questions and generate the most comments on HN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which URLs are most frequently discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15931, 'medium.com'), (14439, 'github.com'), (6069, 'nytimes.com'), (5280, 'youtube.com'), (4116, 'techcrunch.com'), (3432, 'theguardian.com'), (2979, 'arstechnica.com'), (2745, 'bloomberg.com'), (2321, 'en.wikipedia.org'), (1992, 'bbc.com')]\n"
     ]
    }
   ],
   "source": [
    "# URL root analysis\n",
    "url_count = {} # dictionary of URL root frequencies\n",
    "for row in hn:\n",
    "    trimmed = row[2].replace('http://', '').replace('https://', '').replace('www.', '') # remove superfluous prefix\n",
    "    root = trimmed.split('/')[0] # split after backslash and keep only root\n",
    "    if root not in url_count and root != '': # exclude posts without URLs\n",
    "        url_count[root] = 1\n",
    "    elif root != '': url_count[root] += 1\n",
    "\n",
    "url_list = [] # list of frequencies to allow sorting\n",
    "for element in url_count:\n",
    "    url_list.append((url_count[element], element))\n",
    "print(sorted(url_list, reverse=True)[:10]) # top 10 urls and their corresponding counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, medium and github appear as the most cited URLs by a large margin. Both websites, offer information on tech related subjects that provide ample material for stimulating discussions; medium in the form of blogs and github mainly as a repository. Remaining websites are news related with exceptoin of youtube and wikipedia. Next we will analyze, whether several posts linked to the same full-length URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 'aioptify.com/topmldmbooks.php?utm_source=hackernews&utm_medium=cpm&utm_campaign=topmlbooks')\n",
      "(17, 'technologyreview.com/view/541276/deep-learning-machine-teaches-itself-chess-in-72-hours-plays-at-international-master/')\n",
      "(16, 'systemmeasure.com')\n",
      "(15, 'waitbutwhy.com/2015/11/the-cook-and-the-chef-musks-secret-sauce.html')\n",
      "(15, 'journal.sjdm.org/15/15923a/jdm15923a.pdf')\n",
      "(15, 'businessofsoftware.org/free-software-pricing-guide/')\n",
      "(14, 'technologyreview.com/view/542626/why-self-driving-cars-must-be-programmed-to-kill/')\n",
      "(14, 'speerty.com')\n",
      "(14, 'moralmachine.mit.edu/')\n",
      "(14, 'github.com/joowani/dtags')\n",
      "\n",
      "Count of frequencies: \n",
      "1 :  221750\n",
      "2 :  18862\n",
      "5 :  385\n",
      "3 :  3774\n",
      "4 :  1046\n",
      "10 :  12\n",
      "11 :  6\n",
      "6 :  146\n",
      "7 :  80\n",
      "9 :  22\n",
      "8 :  39\n",
      "14 :  5\n",
      "13 :  1\n",
      "12 :  3\n",
      "22 :  1\n",
      "15 :  3\n",
      "16 :  1\n",
      "17 :  1\n"
     ]
    }
   ],
   "source": [
    "url_count = {} # dictionary of full-length URL frequencies\n",
    "for row in hn:\n",
    "    trimmed = row[2].replace('http://', '').replace('https://', '').replace('www.', '') # remove superfluous prefix\n",
    "    if trimmed not in url_count and trimmed != '': # exclude posts without URLs\n",
    "        url_count[trimmed] = 1\n",
    "    elif trimmed != '': url_count[trimmed] += 1\n",
    "\n",
    "url_list = [] # list of URL frequencies to allow sorting\n",
    "for element in url_count:\n",
    "    url_list.append((url_count[element], element))\n",
    "\n",
    "# Count of frequencies\n",
    "frequency_count = {}\n",
    "for freq in url_list:\n",
    "    count = freq[0] # URL count\n",
    "    if count not in frequency_count:\n",
    "        frequency_count[count] = 1\n",
    "    else: frequency_count[count] += 1\n",
    "    \n",
    "# Print top 10 full-length URLs and their corresponding counts\n",
    "for url in sorted(url_list, reverse=True)[:10]:  \n",
    "    print(url)\n",
    "print('\\nCount of frequencies: ')\n",
    "for count in frequency_count:\n",
    "    print(count, ': ', frequency_count[count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top cited URLs include a review of several data science books, an article on Elon Musk and a scientific paper on judgment and decision making. The latter probably rose to prominence as a result of the repeated use of the word **bullshit** (www.journal.sjdm.org/15/15923a/jdm15923a.pdf). Notably, an interesting website among the top 10 outlines 13 distinct moral dilemmas (www.moralmachine.mit.edu). In each scenario, an autonomous vehicle lost brake function and you are presented with two decisions leading to two distinct outcomes. The cases in reality are even more complex since the outcomes would not be easily known beforehand but it does raise some interesting questions on the ethical programming of self-driving cars. Lastly, the frequency list shows a sharp drop-off of frequencies at 4 posts linking to the same URL. This information will be later used as a cut-off in an attempt to find outstanding URL links.\n",
    "\n",
    "Next we will associate full-length URLs with their corresponding number of comments. This may give some more insight on how popular the URLs are. It is possible that multiple users linked to the same URLs but significantly less users were interested in them. Evaluating, in addition, the number of comments and total points will give us a better understanding of the hype surrounding these links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upvotes:\n",
      "[5771, 1, 'apple.com/customer-letter/']\n",
      "[3125, 1, 'bbc.co.uk/news/uk-politics-36615028']\n",
      "[2553, 1, 'pardonsnowden.org/']\n",
      "[2049, 1, 'news.microsoft.com/2016/06/13/microsoft-to-acquire-linkedin/#sm.0000pigrxf7dne36z7o2gp4nrghse']\n",
      "[2049, 1, 'blog.dustinkirkland.com/2016/03/ubuntu-on-windows.html?m=1']\n",
      "[2011, 1, 'nytimes.com/2016/02/12/science/ligo-gravitational-waves-black-holes-einstein.html']\n",
      "[1952, 1, 'techcrunch.com/2015/09/16/14-year-old-boy-arrested-for-bringing-homemade-clock-to-school/']\n",
      "[1876, 1, 'blog.ycombinator.com/basic-income']\n",
      "[1855, 1, 'bunniestudios.com/blog/?p=4782']\n",
      "[1851, 1, 'tesla.com/blog/master-plan-part-deux']\n",
      "\n",
      "Comments: \n",
      "[2531, 1, 'bbc.co.uk/news/uk-politics-36615028']\n",
      "[1733, 1, 'apple.com/iPhone7']\n",
      "[1448, 1, 'blog.ycombinator.com/moving-forward-on-basic-income']\n",
      "[1120, 1, 'blog.ycombinator.com/basic-income']\n",
      "[973, 1, 'businessinsider.com/github-the-full-inside-story-2016-2']\n",
      "[967, 1, 'apple.com/customer-letter/']\n",
      "[870, 1, 'techcrunch.com/2015/09/16/14-year-old-boy-arrested-for-bringing-homemade-clock-to-school/']\n",
      "[854, 1, 'haneycodes.net/npm-left-pad-have-we-forgotten-how-to-program/']\n",
      "[842, 1, 'techcrunch.com/2015/09/09/apple-unveils-the-ipad-pro/']\n",
      "[831, 1, 'news.microsoft.com/2016/06/13/microsoft-to-acquire-linkedin/#sm.0000pigrxf7dne36z7o2gp4nrghse']\n"
     ]
    }
   ],
   "source": [
    "url_count = {} # dictionary of full-length URL frequencies\n",
    "# URL as keys; total points, comments and number of posts linking to same URL as values\n",
    "for row in hn:\n",
    "    trimmed = row[2].replace('http://', '').replace('https://', '').replace('www.', '') # remove superfluous prefix\n",
    "    if trimmed not in url_count and trimmed != '': # exclude posts without URLs\n",
    "        url_count[trimmed] = [int(row[3]), int(row[4]), 1] # tuple: (number of points, number of comments, number of links)\n",
    "    elif trimmed != '':\n",
    "        old = url_count[trimmed] # current counts in dictionary\n",
    "        new = [int(row[3]), int(row[4]), 1] # new counts\n",
    "        url_count[trimmed] = [sum(x) for x in zip(old, new)] # element-wise list addition of old and new counts\n",
    "\n",
    "upvotes = [] # list of frequencies to allow sorting\n",
    "for element in url_count:\n",
    "    upvotes.append([url_count[element][0], url_count[element][2], element]) # upvotes, number of liks to URL, URL\n",
    "    \n",
    "comments = [] # list of frequencies to allow sorting\n",
    "for element in url_count:\n",
    "    comments.append([url_count[element][1], url_count[element][2], element]) # comments, number of liks to URL, URL\n",
    "\n",
    "print('Upvotes:')\n",
    "sorted_upvotes = sorted(upvotes, reverse=True) # descending order\n",
    "for url in sorted_upvotes[:10]:  # top 10 full-length URLs and their corresponding upvotes\n",
    "    print(url)\n",
    "print('\\nComments: ')\n",
    "sorted_comments = sorted(comments, reverse=True) # descending order\n",
    "for url in sorted_comments[:10]:  # top 10 full-length URLs and their corresponding comments\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, non of the top 10 upvoted posts nor those with the most comments had more than one article linking to the same URL. Further, with one exception, all posts had 99 upvotes/comments. We will therefore filter posts out that link to only one URL to reduce some of the noise. The hope here is to find some unique articles that prompted multiple users to post, comment and upvote the informatoin found in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upvotes:\n",
      "[1193, 4, 'github.com/HannahMitt/HomeMirror']\n",
      "[1002, 4, 'youtube.com/watch?v=0nbkaYsR94c']\n",
      "[981, 5, 'martinfowler.com/articles/serverless.html']\n",
      "[923, 5, 'code.facebook.com/posts/1189117404435352/']\n",
      "[857, 4, 'sci-hub.io/']\n",
      "[778, 4, 'bouk.co/blog/hacking-developers/']\n",
      "[763, 5, 'threadbase.com/unravelled']\n",
      "[676, 5, 'brave.com/']\n",
      "[644, 5, 'github.com/ptmt/react-native-desktop']\n",
      "[643, 8, 'udacity.com/course/deep-learning--ud730']\n",
      "\n",
      "Comments: \n",
      "[551, 5, 'brave.com/']\n",
      "[428, 4, 'theguardian.com/business/2016/jun/08/mcdonalds-community-centers-us-physical-social-networks']\n",
      "[420, 4, 'youtube.com/watch?v=0nbkaYsR94c']\n",
      "[358, 5, 'martinfowler.com/articles/serverless.html']\n",
      "[302, 4, 'theguardian.com/us-news/2016/mar/21/death-by-gentrification-the-killing-that-shamed-san-francisco']\n",
      "[281, 4, 'blog.dustinkirkland.com/2016/02/zfs-is-fs-for-containers-in-ubuntu-1604.html']\n",
      "[256, 5, 'github.com/ptmt/react-native-desktop']\n",
      "[254, 4, 'thenation.com/article/universities-are-becoming-billion-dollar-hedge-funds-with-schools-attached/']\n",
      "[239, 5, 'krebsonsecurity.com/2016/02/this-is-why-people-fear-the-internet-of-things/']\n",
      "[239, 5, 'code.facebook.com/posts/1189117404435352/']\n"
     ]
    }
   ],
   "source": [
    "upvotes_filtered = []\n",
    "for upvotes in sorted_upvotes:\n",
    "    if upvotes[1] > 3: # append only those posts that link to the same URL more than 4x\n",
    "        upvotes_filtered.append(upvotes)\n",
    "comments_filtered = []\n",
    "for comments in sorted_comments:\n",
    "    if comments[1] > 3: # append only those posts that link to the same URL more than 4x\n",
    "        comments_filtered.append(comments)\n",
    "        \n",
    "print('Upvotes:')\n",
    "for url in upvotes_filtered[:10]:  # top 10 full-length URLs and their corresponding upvotes\n",
    "    print(url)\n",
    "print('\\nComments: ')\n",
    "for url in comments_filtered[:10]:  # top 10 full-length URLs and their corresponding comments\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To come up with a single top 10 list will search the top 100 of both and register entries with commone URLs. A new metric will be calulated to evaluate hype by multiplying the number of upvotes by the number of comments. The results will be divided by the maximum computed metric for easier interpretation. Multiplication is chosen since it favors number that are more similar in scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 4, 'youtube.com/watch?v=0nbkaYsR94c']\n",
      "[0.8850774641193803, 5, 'brave.com/']\n",
      "[0.8345166809238665, 5, 'martinfowler.com/articles/serverless.html']\n",
      "[0.5241825872065393, 5, 'code.facebook.com/posts/1189117404435352/']\n",
      "[0.43986313088109497, 4, 'sci-hub.io/']\n",
      "[0.4251972245984222, 4, 'bouk.co/blog/hacking-developers/']\n",
      "[0.3940381142476951, 4, 'github.com/HannahMitt/HomeMirror']\n",
      "[0.39174983366600136, 5, 'github.com/ptmt/react-native-desktop']\n",
      "[0.32544434939644523, 4, 'theguardian.com/business/2016/jun/08/mcdonalds-community-centers-us-physical-social-networks']\n",
      "[0.2701430472388556, 5, 'threadbase.com/unravelled']\n"
     ]
    }
   ],
   "source": [
    "results = [] # results with new metric from both comments and upvotes\n",
    "upvotes_comments = [] # list of upvotes and comments associated to the same URL. Used to calculate maximal metric \n",
    "\n",
    "# Find URLs that are represented in both the top 100 number upvotes and top 100 number of comments\n",
    "for top_upvote in upvotes_filtered[:100]:\n",
    "    for top_comment in comments_filtered[:100]: # Not efficient but lists are small in size\n",
    "        if top_upvote[2] == top_comment[2]: # if same URL\n",
    "            new_metric = top_upvote[0]*top_comment[0] # number of comments * number of upvotes\n",
    "            results.append([new_metric, top_upvote[1], top_upvote[2]])\n",
    "            upvotes_comments.append((top_upvote[0],top_comment[0]))\n",
    "\n",
    "max_metric = max([x*y for x,y in upvotes_comments]) # largest metric; used to compute ratios\n",
    "final_results = []\n",
    "\n",
    "# Transform metrics into ratios (0,1]\n",
    "for result in results:\n",
    "    ratio = result[0]/max_metric\n",
    "    final_results.append([ratio, result[1], result[2]])\n",
    "\n",
    "final_results = sorted(final_results, reverse=True) # sort according to metric\n",
    "# print top 10 of final results\n",
    "for result in final_results[:10]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
